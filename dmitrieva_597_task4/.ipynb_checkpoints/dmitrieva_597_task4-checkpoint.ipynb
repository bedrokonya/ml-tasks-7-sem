{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import load_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.1\n",
    "        self.max_gamma = 0.975\n",
    "        self.epsilon = 0.1\n",
    "        self.gamma_decay = 1.0125\n",
    "        self.expirience_size = 18\n",
    "        self.max_memory_size = 25\n",
    "        self.mini_batch_size = 5\n",
    "        self.experiences = np.empty([0, self.expirience_size], dtype=object)\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(1,) + self.state_size))\n",
    "        model.add(Dense(40))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(40))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.add(Activation('linear'))\n",
    "        model.compile(loss='mean_squared_error',  optimizer=Adam(lr=0.002, decay=2.25e-05))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def remember(self, experience):\n",
    "        self.experiences = np.insert(self.experiences, 0,\n",
    "                                     experience, axis=0)\n",
    "        if len(self.experiences) > self.max_memory_size:\n",
    "            self.experiences = np.delete(self.experiences,\n",
    "                                         self.max_memory_size, axis=0)\n",
    "\n",
    "    def sample_experiences(self, mini_batch_size):\n",
    "        if(mini_batch_size > len(self.experiences)):\n",
    "            rep_needed = True\n",
    "        else:\n",
    "            rep_needed = False\n",
    "        s = self.experiences[np.random.choice(\n",
    "                self.experiences.shape[0],\n",
    "                mini_batch_size, replace=rep_needed)]\n",
    "        return s\n",
    "    \n",
    "    def forward_pass(self, state):\n",
    "        input = np.empty([1, 1, self.state_size[0]])\n",
    "        input[0][0] = state\n",
    "        return self.model.predict(input)[0]\n",
    "\n",
    "    def get_targets(self, state, action, reward, next_state):\n",
    "        current_state_q_values = self.forward_pass(state)\n",
    "        next_state_q_values = self.forward_pass(next_state)\n",
    "        max_q_next_state = np.max(next_state_q_values)\n",
    "        targets = np.empty([1, self.action_size])\n",
    "\n",
    "        for i in range(self.action_size):\n",
    "            if i == action:\n",
    "                targets[0][i] = reward + (self.gamma * max_q_next_state)\n",
    "            else:\n",
    "                targets[0][i] = current_state_q_values[i]\n",
    "        return targets\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        r = np.random.uniform()\n",
    "        if r < self.epsilon:\n",
    "            action = np.floor(np.random.randint(self.action_size))\n",
    "        else:\n",
    "            state_q_values = self.forward_pass(state)\n",
    "            action = np.argmax(state_q_values)\n",
    "        return int(action)\n",
    "    \n",
    "    def replay(self):\n",
    "        sample_batch = self.sample_experiences(self.mini_batch_size)\n",
    "        for e in sample_batch:\n",
    "            state, action, reward, new_state = e[0:8], e[8], e[9], e[10:18]\n",
    "            targets = self.get_targets(state, action, reward, new_state)\n",
    "            x = np.empty([1, 1, 8])\n",
    "            x[0][0] = state\n",
    "            self.model.train_on_batch(x, targets)\n",
    "            \n",
    "    def increase_gamma(self):\n",
    "        if self.gamma < self.max_gamma:\n",
    "            self.gamma = self.gamma * self.gamma_decay\n",
    "            print(\"\\n======== new gamma: {:.8} ========\".format(self.gamma))\n",
    "        \n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "seed = 2\n",
    "random.seed(seed)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                360       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 164       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 2,164\n",
      "Trainable params: 2,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "max_steps_per_episode = 1000\n",
    "max_episodes = 4000\n",
    "total_reward = np.zeros(max_episodes)\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Episode: 0/5000, num_steps: 113, total_reward: -403.4\n",
      "Episode: 1/5000, num_steps: 80, total_reward: -237.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2/5000, num_steps: 88, total_reward: -450.7\n",
      "Episode: 3/5000, num_steps: 80, total_reward: -246.8\n",
      "Episode: 4/5000, num_steps: 84, total_reward: -367.2\n",
      "Episode: 5/5000, num_steps: 62, total_reward: -120.2\n",
      "Episode: 6/5000, num_steps: 67, total_reward: -148.1\n",
      "Episode: 7/5000, num_steps: 201, total_reward: -421.3\n",
      "Episode: 8/5000, num_steps: 120, total_reward: -226.4\n",
      "Episode: 9/5000, num_steps: 79, total_reward: -396.9\n",
      "Episode: 10/5000, num_steps: 104, total_reward: -55.17\n",
      "\n",
      "======== new gamma: 0.10125 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -301.816\n",
      "---------------------------\n",
      "Episode: 11/5000, num_steps: 114, total_reward: -287.4\n",
      "Episode: 12/5000, num_steps: 128, total_reward: -169.7\n",
      "Episode: 13/5000, num_steps: 103, total_reward: -316.0\n",
      "Episode: 14/5000, num_steps: 92, total_reward: 0.3239\n",
      "Episode: 15/5000, num_steps: 132, total_reward: -325.2\n",
      "Episode: 16/5000, num_steps: 328, total_reward: -127.5\n",
      "Episode: 17/5000, num_steps: 113, total_reward: -479.9\n",
      "Episode: 18/5000, num_steps: 279, total_reward: -325.0\n",
      "Episode: 19/5000, num_steps: 188, total_reward: -237.3\n",
      "Episode: 20/5000, num_steps: 547, total_reward: 262.5\n",
      "\n",
      "======== new gamma: 0.10251562 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -232.29\n",
      "---------------------------\n",
      "Episode: 21/5000, num_steps: 94, total_reward: -9.293\n",
      "Episode: 22/5000, num_steps: 169, total_reward: 30.19\n",
      "Episode: 23/5000, num_steps: 106, total_reward: -101.0\n",
      "Episode: 24/5000, num_steps: 152, total_reward: -244.0\n",
      "Episode: 25/5000, num_steps: 249, total_reward: -64.97\n",
      "Episode: 26/5000, num_steps: 177, total_reward: -48.34\n",
      "Episode: 27/5000, num_steps: 167, total_reward: -324.8\n",
      "Episode: 28/5000, num_steps: 158, total_reward: -252.4\n",
      "Episode: 29/5000, num_steps: 118, total_reward: -24.92\n",
      "Episode: 30/5000, num_steps: 97, total_reward: -182.3\n",
      "\n",
      "======== new gamma: 0.10379707 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -77.7067\n",
      "---------------------------\n",
      "Episode: 31/5000, num_steps: 320, total_reward: -72.81\n",
      "Episode: 32/5000, num_steps: 487, total_reward: -365.9\n",
      "Episode: 33/5000, num_steps: 449, total_reward: -130.0\n",
      "Episode: 34/5000, num_steps: 151, total_reward: -293.1\n",
      "Episode: 35/5000, num_steps: 76, total_reward: -67.68\n",
      "Episode: 36/5000, num_steps: 196, total_reward: -172.2\n",
      "Episode: 37/5000, num_steps: 467, total_reward: -240.5\n",
      "Episode: 38/5000, num_steps: 136, total_reward: -228.7\n",
      "Episode: 39/5000, num_steps: 248, total_reward: -304.4\n",
      "Episode: 40/5000, num_steps: 253, total_reward: -308.7\n",
      "\n",
      "======== new gamma: 0.10509453 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -205.753\n",
      "---------------------------\n",
      "Episode: 41/5000, num_steps: 188, total_reward: 37.76\n",
      "Episode: 42/5000, num_steps: 233, total_reward: -287.8\n",
      "Episode: 43/5000, num_steps: 163, total_reward: -221.9\n",
      "Episode: 44/5000, num_steps: 179, total_reward: -154.3\n",
      "Episode: 45/5000, num_steps: 103, total_reward: -386.2\n",
      "Episode: 46/5000, num_steps: 125, total_reward: -395.3\n",
      "Episode: 47/5000, num_steps: 120, total_reward: -244.4\n",
      "Episode: 48/5000, num_steps: 291, total_reward: -242.5\n",
      "Episode: 49/5000, num_steps: 311, total_reward: -246.4\n",
      "Episode: 50/5000, num_steps: 277, total_reward: -258.7\n",
      "\n",
      "======== new gamma: 0.10640822 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -244.983\n",
      "---------------------------\n",
      "Episode: 51/5000, num_steps: 164, total_reward: -96.01\n",
      "Episode: 52/5000, num_steps: 111, total_reward: -762.5\n",
      "Episode: 53/5000, num_steps: 240, total_reward: -156.9\n",
      "Episode: 54/5000, num_steps: 169, total_reward: -387.4\n",
      "Episode: 55/5000, num_steps: 129, total_reward: -351.6\n",
      "Episode: 56/5000, num_steps: 202, total_reward: -212.4\n",
      "Episode: 57/5000, num_steps: 114, total_reward: -262.6\n",
      "Episode: 58/5000, num_steps: 485, total_reward: -287.3\n",
      "Episode: 59/5000, num_steps: 550, total_reward: -236.4\n",
      "Episode: 60/5000, num_steps: 321, total_reward: -207.2\n",
      "\n",
      "======== new gamma: 0.10773832 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -301.184\n",
      "---------------------------\n",
      "Episode: 61/5000, num_steps: 303, total_reward: -226.3\n",
      "Episode: 62/5000, num_steps: 192, total_reward: -127.1\n",
      "Episode: 63/5000, num_steps: 141, total_reward: -207.0\n",
      "Episode: 64/5000, num_steps: 231, total_reward: -47.61\n",
      "Episode: 65/5000, num_steps: 155, total_reward: -209.6\n",
      "Episode: 66/5000, num_steps: 225, total_reward: -235.1\n",
      "Episode: 67/5000, num_steps: 195, total_reward: -40.36\n",
      "Episode: 68/5000, num_steps: 203, total_reward: -369.0\n",
      "Episode: 69/5000, num_steps: 152, total_reward: -186.4\n",
      "Episode: 70/5000, num_steps: 142, total_reward: -239.8\n",
      "\n",
      "======== new gamma: 0.10908505 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -185.566\n",
      "---------------------------\n",
      "Episode: 71/5000, num_steps: 329, total_reward: -281.3\n",
      "Episode: 72/5000, num_steps: 210, total_reward: -161.9\n",
      "Episode: 73/5000, num_steps: 320, total_reward: -216.6\n",
      "Episode: 74/5000, num_steps: 176, total_reward: -69.23\n",
      "Episode: 75/5000, num_steps: 109, total_reward: -205.4\n",
      "Episode: 76/5000, num_steps: 156, total_reward: -341.2\n",
      "Episode: 77/5000, num_steps: 85, total_reward: -280.6\n",
      "Episode: 78/5000, num_steps: 69, total_reward: -257.9\n",
      "Episode: 79/5000, num_steps: 286, total_reward: -216.7\n",
      "Episode: 80/5000, num_steps: 173, total_reward: -303.4\n",
      "\n",
      "======== new gamma: 0.11044861 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -227.071\n",
      "---------------------------\n",
      "Episode: 81/5000, num_steps: 220, total_reward: -8.796\n",
      "Episode: 82/5000, num_steps: 239, total_reward: -107.5\n",
      "Episode: 83/5000, num_steps: 141, total_reward: 66.17\n",
      "Episode: 84/5000, num_steps: 205, total_reward: -146.2\n",
      "Episode: 85/5000, num_steps: 179, total_reward: -9.396\n",
      "Episode: 86/5000, num_steps: 178, total_reward: -290.0\n",
      "Episode: 87/5000, num_steps: 159, total_reward: -166.6\n",
      "Episode: 88/5000, num_steps: 274, total_reward: -44.67\n",
      "Episode: 89/5000, num_steps: 327, total_reward: -158.4\n",
      "Episode: 90/5000, num_steps: 247, total_reward: -277.8\n",
      "\n",
      "======== new gamma: 0.11182922 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -116.888\n",
      "---------------------------\n",
      "Episode: 91/5000, num_steps: 221, total_reward: -161.4\n",
      "Episode: 92/5000, num_steps: 269, total_reward: -208.9\n",
      "Episode: 93/5000, num_steps: 157, total_reward: -47.62\n",
      "Episode: 94/5000, num_steps: 284, total_reward: -112.4\n",
      "Episode: 95/5000, num_steps: 258, total_reward: -192.6\n",
      "Episode: 96/5000, num_steps: 191, total_reward: -249.4\n",
      "Episode: 97/5000, num_steps: 178, total_reward: -177.2\n",
      "Episode: 98/5000, num_steps: 248, total_reward: -197.4\n",
      "Episode: 99/5000, num_steps: 179, total_reward: -274.4\n",
      "Episode: 100/5000, num_steps: 216, total_reward: -82.4\n",
      "\n",
      "======== new gamma: 0.11322708 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -189.91\n",
      "---------------------------\n",
      "Episode: 101/5000, num_steps: 142, total_reward: -16.62\n",
      "Episode: 102/5000, num_steps: 165, total_reward: -172.8\n",
      "Episode: 103/5000, num_steps: 145, total_reward: -173.3\n",
      "Episode: 104/5000, num_steps: 79, total_reward: -152.7\n",
      "Episode: 105/5000, num_steps: 182, total_reward: -490.2\n",
      "Episode: 106/5000, num_steps: 212, total_reward: -87.48\n",
      "Episode: 107/5000, num_steps: 166, total_reward: -56.19\n",
      "Episode: 108/5000, num_steps: 226, total_reward: -206.7\n",
      "Episode: 109/5000, num_steps: 149, total_reward: -254.8\n",
      "Episode: 110/5000, num_steps: 128, total_reward: -135.0\n",
      "\n",
      "======== new gamma: 0.11464242 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -169.311\n",
      "---------------------------\n",
      "Episode: 111/5000, num_steps: 281, total_reward: -255.2\n",
      "Episode: 112/5000, num_steps: 225, total_reward: -109.7\n",
      "Episode: 113/5000, num_steps: 166, total_reward: 5.264\n",
      "Episode: 114/5000, num_steps: 253, total_reward: -493.6\n",
      "Episode: 115/5000, num_steps: 255, total_reward: -70.28\n",
      "Episode: 116/5000, num_steps: 273, total_reward: 30.41\n",
      "Episode: 117/5000, num_steps: 160, total_reward: -412.9\n",
      "Episode: 118/5000, num_steps: 112, total_reward: -439.6\n",
      "Episode: 119/5000, num_steps: 157, total_reward: -344.2\n",
      "Episode: 120/5000, num_steps: 137, total_reward: -136.6\n",
      "\n",
      "======== new gamma: 0.11607545 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -222.487\n",
      "---------------------------\n",
      "Episode: 121/5000, num_steps: 176, total_reward: -288.9\n",
      "Episode: 122/5000, num_steps: 151, total_reward: -161.5\n",
      "Episode: 123/5000, num_steps: 108, total_reward: -105.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 124/5000, num_steps: 252, total_reward: -46.64\n",
      "Episode: 125/5000, num_steps: 187, total_reward: -116.5\n",
      "Episode: 126/5000, num_steps: 369, total_reward: -303.8\n",
      "Episode: 127/5000, num_steps: 259, total_reward: -174.5\n",
      "Episode: 128/5000, num_steps: 130, total_reward: -178.3\n",
      "Episode: 129/5000, num_steps: 336, total_reward: -250.0\n",
      "Episode: 130/5000, num_steps: 168, total_reward: -61.61\n",
      "\n",
      "======== new gamma: 0.11752639 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -176.183\n",
      "---------------------------\n",
      "Episode: 131/5000, num_steps: 447, total_reward: -96.14\n",
      "Episode: 132/5000, num_steps: 223, total_reward: -200.3\n",
      "Episode: 133/5000, num_steps: 295, total_reward: -252.0\n",
      "Episode: 134/5000, num_steps: 210, total_reward: -24.61\n",
      "Episode: 135/5000, num_steps: 226, total_reward: -289.4\n",
      "Episode: 136/5000, num_steps: 123, total_reward: -62.82\n",
      "Episode: 137/5000, num_steps: 203, total_reward: -74.54\n",
      "Episode: 138/5000, num_steps: 61, total_reward: -415.0\n",
      "Episode: 139/5000, num_steps: 138, total_reward: -776.9\n",
      "Episode: 140/5000, num_steps: 286, total_reward: -477.9\n",
      "\n",
      "======== new gamma: 0.11899547 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -225.327\n",
      "---------------------------\n",
      "Episode: 141/5000, num_steps: 243, total_reward: -186.6\n",
      "Episode: 142/5000, num_steps: 278, total_reward: -49.18\n",
      "Episode: 143/5000, num_steps: 277, total_reward: -22.82\n",
      "Episode: 144/5000, num_steps: 147, total_reward: -204.8\n",
      "Episode: 145/5000, num_steps: 127, total_reward: -410.2\n",
      "Episode: 146/5000, num_steps: 164, total_reward: -562.7\n",
      "Episode: 147/5000, num_steps: 179, total_reward: -261.7\n",
      "Episode: 148/5000, num_steps: 238, total_reward: -253.8\n",
      "Episode: 149/5000, num_steps: 278, total_reward: -227.9\n",
      "Episode: 150/5000, num_steps: 159, total_reward: -172.9\n",
      "\n",
      "======== new gamma: 0.12048292 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -265.748\n",
      "---------------------------\n",
      "Episode: 151/5000, num_steps: 143, total_reward: -103.1\n",
      "Episode: 152/5000, num_steps: 380, total_reward: -429.0\n",
      "Episode: 153/5000, num_steps: 176, total_reward: 51.17\n",
      "Episode: 154/5000, num_steps: 213, total_reward: -240.9\n",
      "Episode: 155/5000, num_steps: 149, total_reward: -178.8\n",
      "Episode: 156/5000, num_steps: 253, total_reward: 1.34\n",
      "Episode: 157/5000, num_steps: 223, total_reward: -102.0\n",
      "Episode: 158/5000, num_steps: 138, total_reward: -44.59\n",
      "Episode: 159/5000, num_steps: 176, total_reward: -178.0\n",
      "Episode: 160/5000, num_steps: 246, total_reward: -114.3\n",
      "\n",
      "======== new gamma: 0.12198895 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -139.687\n",
      "---------------------------\n",
      "Episode: 161/5000, num_steps: 152, total_reward: -181.5\n",
      "Episode: 162/5000, num_steps: 170, total_reward: 45.67\n",
      "Episode: 163/5000, num_steps: 256, total_reward: -43.71\n",
      "Episode: 164/5000, num_steps: 1000, total_reward: 34.99\n",
      "Episode: 165/5000, num_steps: 1000, total_reward: -55.61\n",
      "Episode: 166/5000, num_steps: 239, total_reward: -302.6\n",
      "Episode: 167/5000, num_steps: 98, total_reward: -451.4\n",
      "Episode: 168/5000, num_steps: 177, total_reward: -123.5\n",
      "Episode: 169/5000, num_steps: 105, total_reward: -339.4\n",
      "Episode: 170/5000, num_steps: 145, total_reward: -212.4\n",
      "\n",
      "======== new gamma: 0.12351382 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -153.142\n",
      "---------------------------\n",
      "Episode: 171/5000, num_steps: 139, total_reward: -203.9\n",
      "Episode: 172/5000, num_steps: 137, total_reward: -258.5\n",
      "Episode: 173/5000, num_steps: 136, total_reward: -289.9\n",
      "Episode: 174/5000, num_steps: 100, total_reward: -221.6\n",
      "Episode: 175/5000, num_steps: 73, total_reward: -152.5\n",
      "Episode: 176/5000, num_steps: 218, total_reward: -92.24\n",
      "Episode: 177/5000, num_steps: 235, total_reward: 48.99\n",
      "Episode: 178/5000, num_steps: 95, total_reward: -298.0\n",
      "Episode: 179/5000, num_steps: 320, total_reward: -112.2\n",
      "Episode: 180/5000, num_steps: 251, total_reward: -70.21\n",
      "\n",
      "======== new gamma: 0.12505774 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -179.22\n",
      "---------------------------\n",
      "Episode: 181/5000, num_steps: 215, total_reward: -232.5\n",
      "Episode: 182/5000, num_steps: 161, total_reward: -159.3\n",
      "Episode: 183/5000, num_steps: 108, total_reward: -349.5\n",
      "Episode: 184/5000, num_steps: 168, total_reward: -223.2\n",
      "Episode: 185/5000, num_steps: 189, total_reward: -67.45\n",
      "Episode: 186/5000, num_steps: 104, total_reward: -409.4\n",
      "Episode: 187/5000, num_steps: 204, total_reward: -323.5\n",
      "Episode: 188/5000, num_steps: 428, total_reward: 200.7\n",
      "Episode: 189/5000, num_steps: 913, total_reward: -205.8\n",
      "Episode: 190/5000, num_steps: 140, total_reward: -476.8\n",
      "\n",
      "======== new gamma: 0.12662096 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -183.989\n",
      "---------------------------\n",
      "Episode: 191/5000, num_steps: 146, total_reward: -406.0\n",
      "Episode: 192/5000, num_steps: 241, total_reward: -73.85\n",
      "Episode: 193/5000, num_steps: 276, total_reward: -295.9\n",
      "Episode: 194/5000, num_steps: 246, total_reward: -242.8\n",
      "Episode: 195/5000, num_steps: 92, total_reward: -164.7\n",
      "Episode: 196/5000, num_steps: 228, total_reward: -27.34\n",
      "Episode: 197/5000, num_steps: 154, total_reward: -348.0\n",
      "Episode: 198/5000, num_steps: 220, total_reward: -211.4\n",
      "Episode: 199/5000, num_steps: 151, total_reward: -153.4\n",
      "Episode: 200/5000, num_steps: 214, total_reward: -214.5\n",
      "\n",
      "======== new gamma: 0.12820372 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -240.022\n",
      "---------------------------\n",
      "Episode: 201/5000, num_steps: 155, total_reward: -64.09\n",
      "Episode: 202/5000, num_steps: 314, total_reward: -76.63\n",
      "Episode: 203/5000, num_steps: 227, total_reward: -197.4\n",
      "Episode: 204/5000, num_steps: 268, total_reward: -75.52\n",
      "Episode: 205/5000, num_steps: 155, total_reward: -184.1\n",
      "Episode: 206/5000, num_steps: 349, total_reward: -91.32\n",
      "Episode: 207/5000, num_steps: 108, total_reward: -486.5\n",
      "Episode: 208/5000, num_steps: 186, total_reward: -290.4\n",
      "Episode: 209/5000, num_steps: 184, total_reward: -164.4\n",
      "Episode: 210/5000, num_steps: 275, total_reward: -199.6\n",
      "\n",
      "======== new gamma: 0.12980627 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -184.487\n",
      "---------------------------\n",
      "Episode: 211/5000, num_steps: 122, total_reward: -105.4\n",
      "Episode: 212/5000, num_steps: 156, total_reward: -478.9\n",
      "Episode: 213/5000, num_steps: 260, total_reward: -504.6\n",
      "Episode: 214/5000, num_steps: 91, total_reward: -737.7\n",
      "Episode: 215/5000, num_steps: 185, total_reward: -62.03\n",
      "Episode: 216/5000, num_steps: 170, total_reward: -71.13\n",
      "Episode: 217/5000, num_steps: 462, total_reward: 266.3\n",
      "Episode: 218/5000, num_steps: 258, total_reward: -255.9\n",
      "Episode: 219/5000, num_steps: 148, total_reward: -29.04\n",
      "Episode: 220/5000, num_steps: 175, total_reward: -9.296\n",
      "\n",
      "======== new gamma: 0.13142885 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -217.805\n",
      "---------------------------\n",
      "Episode: 221/5000, num_steps: 102, total_reward: -254.3\n",
      "Episode: 222/5000, num_steps: 157, total_reward: -533.6\n",
      "Episode: 223/5000, num_steps: 121, total_reward: -363.0\n",
      "Episode: 224/5000, num_steps: 422, total_reward: -464.5\n",
      "Episode: 225/5000, num_steps: 238, total_reward: -530.7\n",
      "Episode: 226/5000, num_steps: 388, total_reward: -81.81\n",
      "Episode: 227/5000, num_steps: 192, total_reward: -25.98\n",
      "Episode: 228/5000, num_steps: 323, total_reward: -63.36\n",
      "Episode: 229/5000, num_steps: 249, total_reward: -212.2\n",
      "Episode: 230/5000, num_steps: 860, total_reward: 260.4\n",
      "\n",
      "======== new gamma: 0.13307171 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -253.872\n",
      "---------------------------\n",
      "Episode: 231/5000, num_steps: 276, total_reward: -87.9\n",
      "Episode: 232/5000, num_steps: 143, total_reward: -178.8\n",
      "Episode: 233/5000, num_steps: 165, total_reward: 34.25\n",
      "Episode: 234/5000, num_steps: 296, total_reward: -258.3\n",
      "Episode: 235/5000, num_steps: 132, total_reward: -200.7\n",
      "Episode: 236/5000, num_steps: 118, total_reward: -352.8\n",
      "Episode: 237/5000, num_steps: 197, total_reward: -175.8\n",
      "Episode: 238/5000, num_steps: 275, total_reward: -87.35\n",
      "Episode: 239/5000, num_steps: 222, total_reward: -192.5\n",
      "Episode: 240/5000, num_steps: 355, total_reward: -44.21\n",
      "\n",
      "======== new gamma: 0.13473511 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -123.94\n",
      "---------------------------\n",
      "Episode: 241/5000, num_steps: 117, total_reward: -312.0\n",
      "Episode: 242/5000, num_steps: 134, total_reward: -276.4\n",
      "Episode: 243/5000, num_steps: 92, total_reward: -31.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 244/5000, num_steps: 232, total_reward: -64.27\n",
      "Episode: 245/5000, num_steps: 133, total_reward: -229.9\n",
      "Episode: 246/5000, num_steps: 181, total_reward: -312.6\n",
      "Episode: 247/5000, num_steps: 239, total_reward: -31.95\n",
      "Episode: 248/5000, num_steps: 828, total_reward: 113.6\n",
      "Episode: 249/5000, num_steps: 265, total_reward: -73.66\n",
      "Episode: 250/5000, num_steps: 298, total_reward: -56.93\n",
      "\n",
      "======== new gamma: 0.13641929 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -126.298\n",
      "---------------------------\n",
      "Episode: 251/5000, num_steps: 170, total_reward: -211.7\n",
      "Episode: 252/5000, num_steps: 280, total_reward: -201.7\n",
      "Episode: 253/5000, num_steps: 182, total_reward: -44.95\n",
      "Episode: 254/5000, num_steps: 439, total_reward: -234.9\n",
      "Episode: 255/5000, num_steps: 182, total_reward: -69.95\n",
      "Episode: 256/5000, num_steps: 232, total_reward: -186.0\n",
      "Episode: 257/5000, num_steps: 231, total_reward: -290.5\n",
      "Episode: 258/5000, num_steps: 361, total_reward: -266.2\n",
      "Episode: 259/5000, num_steps: 304, total_reward: -98.54\n",
      "Episode: 260/5000, num_steps: 338, total_reward: -61.62\n",
      "\n",
      "======== new gamma: 0.13812454 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -166.14\n",
      "---------------------------\n",
      "Episode: 261/5000, num_steps: 235, total_reward: -92.32\n",
      "Episode: 262/5000, num_steps: 135, total_reward: -316.5\n",
      "Episode: 263/5000, num_steps: 174, total_reward: -342.5\n",
      "Episode: 264/5000, num_steps: 172, total_reward: -406.1\n",
      "Episode: 265/5000, num_steps: 102, total_reward: -352.9\n",
      "Episode: 266/5000, num_steps: 160, total_reward: -652.0\n",
      "Episode: 267/5000, num_steps: 151, total_reward: -822.1\n",
      "Episode: 268/5000, num_steps: 120, total_reward: -271.3\n",
      "Episode: 269/5000, num_steps: 160, total_reward: -462.5\n",
      "Episode: 270/5000, num_steps: 122, total_reward: -223.7\n",
      "\n",
      "======== new gamma: 0.13985109 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -377.986\n",
      "---------------------------\n",
      "Episode: 271/5000, num_steps: 169, total_reward: -367.7\n",
      "Episode: 272/5000, num_steps: 198, total_reward: -355.1\n",
      "Episode: 273/5000, num_steps: 222, total_reward: -334.5\n",
      "Episode: 274/5000, num_steps: 162, total_reward: -333.4\n",
      "Episode: 275/5000, num_steps: 231, total_reward: -280.9\n",
      "Episode: 276/5000, num_steps: 251, total_reward: -47.93\n",
      "Episode: 277/5000, num_steps: 231, total_reward: -46.69\n",
      "Episode: 278/5000, num_steps: 226, total_reward: -30.99\n",
      "Episode: 279/5000, num_steps: 293, total_reward: -267.4\n",
      "Episode: 280/5000, num_steps: 184, total_reward: -358.5\n",
      "\n",
      "======== new gamma: 0.14159923 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -228.822\n",
      "---------------------------\n",
      "Episode: 281/5000, num_steps: 168, total_reward: -395.8\n",
      "Episode: 282/5000, num_steps: 344, total_reward: -294.2\n",
      "Episode: 283/5000, num_steps: 341, total_reward: -354.5\n",
      "Episode: 284/5000, num_steps: 354, total_reward: -98.06\n",
      "Episode: 285/5000, num_steps: 347, total_reward: -148.2\n",
      "Episode: 286/5000, num_steps: 246, total_reward: -343.8\n",
      "Episode: 287/5000, num_steps: 197, total_reward: -400.4\n",
      "Episode: 288/5000, num_steps: 246, total_reward: -78.88\n",
      "Episode: 289/5000, num_steps: 215, total_reward: -224.9\n",
      "Episode: 290/5000, num_steps: 100, total_reward: -296.2\n",
      "\n",
      "======== new gamma: 0.14336922 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -269.739\n",
      "---------------------------\n",
      "Episode: 291/5000, num_steps: 203, total_reward: -174.6\n",
      "Episode: 292/5000, num_steps: 231, total_reward: -288.8\n",
      "Episode: 293/5000, num_steps: 406, total_reward: -254.5\n",
      "Episode: 294/5000, num_steps: 172, total_reward: -27.72\n",
      "Episode: 295/5000, num_steps: 223, total_reward: -217.8\n",
      "Episode: 296/5000, num_steps: 188, total_reward: 73.43\n",
      "Episode: 297/5000, num_steps: 311, total_reward: -65.47\n",
      "Episode: 298/5000, num_steps: 345, total_reward: -212.4\n",
      "Episode: 299/5000, num_steps: 324, total_reward: -195.9\n",
      "Episode: 300/5000, num_steps: 114, total_reward: -198.9\n",
      "\n",
      "======== new gamma: 0.14516134 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -165.988\n",
      "---------------------------\n",
      "Episode: 301/5000, num_steps: 295, total_reward: -218.3\n",
      "Episode: 302/5000, num_steps: 208, total_reward: -338.3\n",
      "Episode: 303/5000, num_steps: 206, total_reward: -171.8\n",
      "Episode: 304/5000, num_steps: 144, total_reward: -255.7\n",
      "Episode: 305/5000, num_steps: 160, total_reward: -160.8\n",
      "Episode: 306/5000, num_steps: 227, total_reward: -191.8\n",
      "Episode: 307/5000, num_steps: 319, total_reward: -74.03\n",
      "Episode: 308/5000, num_steps: 245, total_reward: -314.1\n",
      "Episode: 309/5000, num_steps: 247, total_reward: -255.9\n",
      "Episode: 310/5000, num_steps: 361, total_reward: -192.1\n",
      "\n",
      "======== new gamma: 0.14697585 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -217.97\n",
      "---------------------------\n",
      "Episode: 311/5000, num_steps: 234, total_reward: -198.6\n",
      "Episode: 312/5000, num_steps: 285, total_reward: -56.53\n",
      "Episode: 313/5000, num_steps: 256, total_reward: -240.1\n",
      "Episode: 314/5000, num_steps: 307, total_reward: -214.4\n",
      "Episode: 315/5000, num_steps: 214, total_reward: -201.4\n",
      "Episode: 316/5000, num_steps: 166, total_reward: -305.3\n",
      "Episode: 317/5000, num_steps: 249, total_reward: -61.29\n",
      "Episode: 318/5000, num_steps: 215, total_reward: -349.9\n",
      "Episode: 319/5000, num_steps: 236, total_reward: -244.1\n",
      "Episode: 320/5000, num_steps: 227, total_reward: -218.8\n",
      "\n",
      "======== new gamma: 0.14881305 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -206.367\n",
      "---------------------------\n",
      "Episode: 321/5000, num_steps: 241, total_reward: -257.9\n",
      "Episode: 322/5000, num_steps: 265, total_reward: -213.8\n",
      "Episode: 323/5000, num_steps: 162, total_reward: -143.3\n",
      "Episode: 324/5000, num_steps: 256, total_reward: -259.2\n",
      "Episode: 325/5000, num_steps: 288, total_reward: -200.9\n",
      "Episode: 326/5000, num_steps: 181, total_reward: 14.66\n",
      "Episode: 327/5000, num_steps: 243, total_reward: -201.7\n",
      "Episode: 328/5000, num_steps: 355, total_reward: -255.5\n",
      "Episode: 329/5000, num_steps: 194, total_reward: 59.76\n",
      "Episode: 330/5000, num_steps: 159, total_reward: -165.0\n",
      "\n",
      "======== new gamma: 0.15067321 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -167.666\n",
      "---------------------------\n",
      "Episode: 331/5000, num_steps: 138, total_reward: -43.15\n",
      "Episode: 332/5000, num_steps: 262, total_reward: -205.2\n",
      "Episode: 333/5000, num_steps: 125, total_reward: -102.3\n",
      "Episode: 334/5000, num_steps: 175, total_reward: -41.73\n",
      "Episode: 335/5000, num_steps: 308, total_reward: -192.1\n",
      "Episode: 336/5000, num_steps: 248, total_reward: -17.27\n",
      "Episode: 337/5000, num_steps: 186, total_reward: -46.82\n",
      "Episode: 338/5000, num_steps: 262, total_reward: -248.8\n",
      "Episode: 339/5000, num_steps: 240, total_reward: -87.08\n",
      "Episode: 340/5000, num_steps: 282, total_reward: -155.3\n",
      "\n",
      "======== new gamma: 0.15255663 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -114.953\n",
      "---------------------------\n",
      "Episode: 341/5000, num_steps: 254, total_reward: -157.0\n",
      "Episode: 342/5000, num_steps: 310, total_reward: -64.48\n",
      "Episode: 343/5000, num_steps: 232, total_reward: -206.8\n",
      "Episode: 344/5000, num_steps: 467, total_reward: 192.9\n",
      "Episode: 345/5000, num_steps: 114, total_reward: -187.7\n",
      "Episode: 346/5000, num_steps: 328, total_reward: -225.3\n",
      "Episode: 347/5000, num_steps: 135, total_reward: -279.9\n",
      "Episode: 348/5000, num_steps: 314, total_reward: -238.4\n",
      "Episode: 349/5000, num_steps: 293, total_reward: -103.1\n",
      "Episode: 350/5000, num_steps: 195, total_reward: -146.0\n",
      "\n",
      "======== new gamma: 0.15446359 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -142.501\n",
      "---------------------------\n",
      "Episode: 351/5000, num_steps: 174, total_reward: -109.4\n",
      "Episode: 352/5000, num_steps: 551, total_reward: 221.4\n",
      "Episode: 353/5000, num_steps: 253, total_reward: -229.7\n",
      "Episode: 354/5000, num_steps: 185, total_reward: -321.5\n",
      "Episode: 355/5000, num_steps: 239, total_reward: -245.1\n",
      "Episode: 356/5000, num_steps: 180, total_reward: -341.3\n",
      "Episode: 357/5000, num_steps: 235, total_reward: -199.6\n",
      "Episode: 358/5000, num_steps: 223, total_reward: -104.5\n",
      "Episode: 359/5000, num_steps: 188, total_reward: 15.62\n",
      "Episode: 360/5000, num_steps: 290, total_reward: -223.5\n",
      "\n",
      "======== new gamma: 0.15639438 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -146.02\n",
      "---------------------------\n",
      "Episode: 361/5000, num_steps: 170, total_reward: -202.0\n",
      "Episode: 362/5000, num_steps: 250, total_reward: -253.0\n",
      "Episode: 363/5000, num_steps: 319, total_reward: -57.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 364/5000, num_steps: 251, total_reward: -192.3\n",
      "Episode: 365/5000, num_steps: 221, total_reward: -211.2\n",
      "Episode: 366/5000, num_steps: 163, total_reward: -60.9\n",
      "Episode: 367/5000, num_steps: 204, total_reward: -17.8\n",
      "Episode: 368/5000, num_steps: 313, total_reward: -352.9\n",
      "Episode: 369/5000, num_steps: 244, total_reward: -79.88\n",
      "Episode: 370/5000, num_steps: 254, total_reward: -216.6\n",
      "\n",
      "======== new gamma: 0.15834931 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -165.086\n",
      "---------------------------\n",
      "Episode: 371/5000, num_steps: 141, total_reward: -9.007\n",
      "Episode: 372/5000, num_steps: 321, total_reward: -305.9\n",
      "Episode: 373/5000, num_steps: 243, total_reward: -218.1\n",
      "Episode: 374/5000, num_steps: 159, total_reward: -142.6\n",
      "Episode: 375/5000, num_steps: 370, total_reward: -169.3\n",
      "Episode: 376/5000, num_steps: 123, total_reward: -240.4\n",
      "Episode: 377/5000, num_steps: 84, total_reward: -149.6\n",
      "Episode: 378/5000, num_steps: 160, total_reward: -26.44\n",
      "Episode: 379/5000, num_steps: 276, total_reward: -159.5\n",
      "Episode: 380/5000, num_steps: 199, total_reward: -57.42\n",
      "\n",
      "======== new gamma: 0.16032868 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -163.744\n",
      "---------------------------\n",
      "Episode: 381/5000, num_steps: 238, total_reward: -180.4\n",
      "Episode: 382/5000, num_steps: 200, total_reward: -300.7\n",
      "Episode: 383/5000, num_steps: 167, total_reward: -146.3\n",
      "Episode: 384/5000, num_steps: 225, total_reward: -125.3\n",
      "Episode: 385/5000, num_steps: 219, total_reward: -329.2\n",
      "Episode: 386/5000, num_steps: 151, total_reward: -202.1\n",
      "Episode: 387/5000, num_steps: 292, total_reward: -212.7\n",
      "Episode: 388/5000, num_steps: 200, total_reward: -371.4\n",
      "Episode: 389/5000, num_steps: 91, total_reward: -108.0\n",
      "Episode: 390/5000, num_steps: 213, total_reward: -348.8\n",
      "\n",
      "======== new gamma: 0.16233279 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -203.353\n",
      "---------------------------\n",
      "Episode: 391/5000, num_steps: 100, total_reward: -190.9\n",
      "Episode: 392/5000, num_steps: 111, total_reward: -214.8\n",
      "Episode: 393/5000, num_steps: 216, total_reward: -320.9\n",
      "Episode: 394/5000, num_steps: 97, total_reward: -195.3\n",
      "Episode: 395/5000, num_steps: 115, total_reward: -266.2\n",
      "Episode: 396/5000, num_steps: 157, total_reward: -112.2\n",
      "Episode: 397/5000, num_steps: 170, total_reward: -216.7\n",
      "Episode: 398/5000, num_steps: 168, total_reward: -265.5\n",
      "Episode: 399/5000, num_steps: 180, total_reward: -38.06\n",
      "Episode: 400/5000, num_steps: 169, total_reward: -191.6\n",
      "\n",
      "======== new gamma: 0.16436195 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -216.959\n",
      "---------------------------\n",
      "Episode: 401/5000, num_steps: 295, total_reward: -229.6\n",
      "Episode: 402/5000, num_steps: 592, total_reward: 175.3\n",
      "Episode: 403/5000, num_steps: 195, total_reward: -91.48\n",
      "Episode: 404/5000, num_steps: 155, total_reward: -165.5\n",
      "Episode: 405/5000, num_steps: 196, total_reward: -206.4\n",
      "Episode: 406/5000, num_steps: 162, total_reward: -146.2\n",
      "Episode: 407/5000, num_steps: 253, total_reward: -195.7\n",
      "Episode: 408/5000, num_steps: 232, total_reward: -322.5\n",
      "Episode: 409/5000, num_steps: 252, total_reward: -64.54\n",
      "Episode: 410/5000, num_steps: 236, total_reward: -282.1\n",
      "\n",
      "======== new gamma: 0.16641647 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -143.83\n",
      "---------------------------\n",
      "Episode: 411/5000, num_steps: 276, total_reward: 181.9\n",
      "Episode: 412/5000, num_steps: 169, total_reward: -92.72\n",
      "Episode: 413/5000, num_steps: 138, total_reward: -207.9\n",
      "Episode: 414/5000, num_steps: 249, total_reward: -98.72\n",
      "Episode: 415/5000, num_steps: 367, total_reward: -147.8\n",
      "Episode: 416/5000, num_steps: 258, total_reward: -209.5\n",
      "Episode: 417/5000, num_steps: 348, total_reward: -244.7\n",
      "Episode: 418/5000, num_steps: 620, total_reward: 176.7\n",
      "Episode: 419/5000, num_steps: 249, total_reward: -245.7\n",
      "Episode: 420/5000, num_steps: 426, total_reward: -258.8\n",
      "\n",
      "======== new gamma: 0.16849668 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -117.052\n",
      "---------------------------\n",
      "Episode: 421/5000, num_steps: 234, total_reward: -268.7\n",
      "Episode: 422/5000, num_steps: 191, total_reward: -57.17\n",
      "Episode: 423/5000, num_steps: 289, total_reward: -179.9\n",
      "Episode: 424/5000, num_steps: 508, total_reward: 215.1\n",
      "Episode: 425/5000, num_steps: 282, total_reward: -105.4\n",
      "Episode: 426/5000, num_steps: 389, total_reward: -2.598\n",
      "Episode: 427/5000, num_steps: 299, total_reward: -52.28\n",
      "Episode: 428/5000, num_steps: 274, total_reward: -237.7\n",
      "Episode: 429/5000, num_steps: 243, total_reward: -94.67\n",
      "Episode: 430/5000, num_steps: 278, total_reward: 8.683\n",
      "\n",
      "======== new gamma: 0.17060289 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -104.205\n",
      "---------------------------\n",
      "Episode: 431/5000, num_steps: 467, total_reward: 222.9\n",
      "Episode: 432/5000, num_steps: 192, total_reward: -26.91\n",
      "Episode: 433/5000, num_steps: 162, total_reward: -146.9\n",
      "Episode: 434/5000, num_steps: 244, total_reward: 8.111\n",
      "Episode: 435/5000, num_steps: 307, total_reward: -286.3\n",
      "Episode: 436/5000, num_steps: 279, total_reward: -257.1\n",
      "Episode: 437/5000, num_steps: 225, total_reward: -204.7\n",
      "Episode: 438/5000, num_steps: 241, total_reward: -61.04\n",
      "Episode: 439/5000, num_steps: 207, total_reward: -67.83\n",
      "Episode: 440/5000, num_steps: 272, total_reward: -211.7\n",
      "\n",
      "======== new gamma: 0.17273542 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -81.1183\n",
      "---------------------------\n",
      "Episode: 441/5000, num_steps: 662, total_reward: 224.2\n",
      "Episode: 442/5000, num_steps: 214, total_reward: -232.7\n",
      "Episode: 443/5000, num_steps: 180, total_reward: -16.81\n",
      "Episode: 444/5000, num_steps: 563, total_reward: -124.7\n",
      "Episode: 445/5000, num_steps: 205, total_reward: 12.81\n",
      "Episode: 446/5000, num_steps: 186, total_reward: -56.6\n",
      "Episode: 447/5000, num_steps: 207, total_reward: -195.2\n",
      "Episode: 448/5000, num_steps: 238, total_reward: -77.45\n",
      "Episode: 449/5000, num_steps: 374, total_reward: -229.1\n",
      "Episode: 450/5000, num_steps: 399, total_reward: -235.3\n",
      "\n",
      "======== new gamma: 0.17489461 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -90.7321\n",
      "---------------------------\n",
      "Episode: 451/5000, num_steps: 237, total_reward: -81.54\n",
      "Episode: 452/5000, num_steps: 209, total_reward: -182.6\n",
      "Episode: 453/5000, num_steps: 185, total_reward: -57.12\n",
      "Episode: 454/5000, num_steps: 309, total_reward: -7.689\n",
      "Episode: 455/5000, num_steps: 332, total_reward: 10.18\n",
      "Episode: 456/5000, num_steps: 266, total_reward: -19.05\n",
      "Episode: 457/5000, num_steps: 143, total_reward: -59.48\n",
      "Episode: 458/5000, num_steps: 219, total_reward: -41.94\n",
      "Episode: 459/5000, num_steps: 153, total_reward: -169.6\n",
      "Episode: 460/5000, num_steps: 468, total_reward: 242.3\n",
      "\n",
      "======== new gamma: 0.1770808 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -84.4206\n",
      "---------------------------\n",
      "Episode: 461/5000, num_steps: 196, total_reward: -128.2\n",
      "Episode: 462/5000, num_steps: 232, total_reward: -270.2\n",
      "Episode: 463/5000, num_steps: 247, total_reward: -227.7\n",
      "Episode: 464/5000, num_steps: 111, total_reward: -337.3\n",
      "Episode: 465/5000, num_steps: 165, total_reward: -87.97\n",
      "Episode: 466/5000, num_steps: 107, total_reward: -327.2\n",
      "Episode: 467/5000, num_steps: 193, total_reward: -275.1\n",
      "Episode: 468/5000, num_steps: 266, total_reward: -276.3\n",
      "Episode: 469/5000, num_steps: 422, total_reward: -221.2\n",
      "Episode: 470/5000, num_steps: 175, total_reward: -34.87\n",
      "\n",
      "======== new gamma: 0.17929431 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -190.89\n",
      "---------------------------\n",
      "Episode: 471/5000, num_steps: 416, total_reward: -47.42\n",
      "Episode: 472/5000, num_steps: 197, total_reward: -321.0\n",
      "Episode: 473/5000, num_steps: 149, total_reward: -339.6\n",
      "Episode: 474/5000, num_steps: 140, total_reward: -239.4\n",
      "Episode: 475/5000, num_steps: 146, total_reward: -68.02\n",
      "Episode: 476/5000, num_steps: 223, total_reward: -392.4\n",
      "Episode: 477/5000, num_steps: 195, total_reward: -404.0\n",
      "Episode: 478/5000, num_steps: 217, total_reward: 12.53\n",
      "Episode: 479/5000, num_steps: 191, total_reward: -318.0\n",
      "Episode: 480/5000, num_steps: 211, total_reward: -289.5\n",
      "\n",
      "======== new gamma: 0.18153549 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -215.21\n",
      "---------------------------\n",
      "Episode: 481/5000, num_steps: 570, total_reward: 189.7\n",
      "Episode: 482/5000, num_steps: 154, total_reward: -174.6\n",
      "Episode: 483/5000, num_steps: 189, total_reward: -45.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 484/5000, num_steps: 281, total_reward: -73.97\n",
      "Episode: 485/5000, num_steps: 152, total_reward: -176.5\n",
      "Episode: 486/5000, num_steps: 277, total_reward: -74.52\n",
      "Episode: 487/5000, num_steps: 299, total_reward: -62.88\n",
      "Episode: 488/5000, num_steps: 173, total_reward: -129.6\n",
      "Episode: 489/5000, num_steps: 247, total_reward: -68.2\n",
      "Episode: 490/5000, num_steps: 254, total_reward: -70.14\n",
      "\n",
      "======== new gamma: 0.18380468 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -90.5132\n",
      "---------------------------\n",
      "Episode: 491/5000, num_steps: 434, total_reward: -46.86\n",
      "Episode: 492/5000, num_steps: 325, total_reward: -142.2\n",
      "Episode: 493/5000, num_steps: 200, total_reward: -140.9\n",
      "Episode: 494/5000, num_steps: 1000, total_reward: 66.82\n",
      "Episode: 495/5000, num_steps: 391, total_reward: -231.6\n",
      "Episode: 496/5000, num_steps: 260, total_reward: 0.1149\n",
      "Episode: 497/5000, num_steps: 162, total_reward: -179.4\n",
      "Episode: 498/5000, num_steps: 175, total_reward: -38.64\n",
      "Episode: 499/5000, num_steps: 153, total_reward: -142.9\n",
      "Episode: 500/5000, num_steps: 245, total_reward: -225.4\n",
      "\n",
      "======== new gamma: 0.18610224 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -92.5684\n",
      "---------------------------\n",
      "Episode: 501/5000, num_steps: 284, total_reward: -2.386\n",
      "Episode: 502/5000, num_steps: 552, total_reward: 230.8\n",
      "Episode: 503/5000, num_steps: 198, total_reward: -85.74\n",
      "Episode: 504/5000, num_steps: 100, total_reward: -211.4\n",
      "Episode: 505/5000, num_steps: 94, total_reward: -242.0\n",
      "Episode: 506/5000, num_steps: 123, total_reward: -257.2\n",
      "Episode: 507/5000, num_steps: 138, total_reward: -323.3\n",
      "Episode: 508/5000, num_steps: 108, total_reward: -311.4\n",
      "Episode: 509/5000, num_steps: 255, total_reward: -230.3\n",
      "Episode: 510/5000, num_steps: 195, total_reward: -214.3\n",
      "\n",
      "======== new gamma: 0.18842852 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -165.826\n",
      "---------------------------\n",
      "Episode: 511/5000, num_steps: 198, total_reward: -203.1\n",
      "Episode: 512/5000, num_steps: 202, total_reward: -52.13\n",
      "Episode: 513/5000, num_steps: 179, total_reward: -8.709\n",
      "Episode: 514/5000, num_steps: 151, total_reward: -149.5\n",
      "Episode: 515/5000, num_steps: 193, total_reward: -167.5\n",
      "Episode: 516/5000, num_steps: 244, total_reward: -59.79\n",
      "Episode: 517/5000, num_steps: 333, total_reward: -89.18\n",
      "Episode: 518/5000, num_steps: 354, total_reward: -68.2\n",
      "Episode: 519/5000, num_steps: 256, total_reward: -223.4\n",
      "Episode: 520/5000, num_steps: 647, total_reward: 247.7\n",
      "\n",
      "======== new gamma: 0.19078387 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -123.574\n",
      "---------------------------\n",
      "Episode: 521/5000, num_steps: 170, total_reward: -223.1\n",
      "Episode: 522/5000, num_steps: 153, total_reward: -40.34\n",
      "Episode: 523/5000, num_steps: 230, total_reward: -30.53\n",
      "Episode: 524/5000, num_steps: 201, total_reward: -205.0\n",
      "Episode: 525/5000, num_steps: 276, total_reward: -201.1\n",
      "Episode: 526/5000, num_steps: 363, total_reward: -238.7\n",
      "Episode: 527/5000, num_steps: 262, total_reward: -40.23\n",
      "Episode: 528/5000, num_steps: 193, total_reward: -68.7\n",
      "Episode: 529/5000, num_steps: 225, total_reward: -239.3\n",
      "Episode: 530/5000, num_steps: 277, total_reward: -253.7\n",
      "\n",
      "======== new gamma: 0.19316867 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -103.927\n",
      "---------------------------\n",
      "Episode: 531/5000, num_steps: 279, total_reward: -197.4\n",
      "Episode: 532/5000, num_steps: 161, total_reward: -165.6\n",
      "Episode: 533/5000, num_steps: 335, total_reward: -116.9\n",
      "Episode: 534/5000, num_steps: 219, total_reward: -183.1\n",
      "Episode: 535/5000, num_steps: 204, total_reward: -227.1\n",
      "Episode: 536/5000, num_steps: 303, total_reward: -268.5\n",
      "Episode: 537/5000, num_steps: 189, total_reward: 62.96\n",
      "Episode: 538/5000, num_steps: 216, total_reward: -57.39\n",
      "Episode: 539/5000, num_steps: 261, total_reward: -66.07\n",
      "Episode: 540/5000, num_steps: 249, total_reward: -53.96\n",
      "\n",
      "======== new gamma: 0.19558328 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -147.272\n",
      "---------------------------\n",
      "Episode: 541/5000, num_steps: 185, total_reward: -84.02\n",
      "Episode: 542/5000, num_steps: 264, total_reward: -234.8\n",
      "Episode: 543/5000, num_steps: 386, total_reward: -248.3\n",
      "Episode: 544/5000, num_steps: 267, total_reward: -240.5\n",
      "Episode: 545/5000, num_steps: 165, total_reward: -267.1\n",
      "Episode: 546/5000, num_steps: 311, total_reward: -105.4\n",
      "Episode: 547/5000, num_steps: 404, total_reward: 231.4\n",
      "Episode: 548/5000, num_steps: 183, total_reward: -74.85\n",
      "Episode: 549/5000, num_steps: 422, total_reward: -59.58\n",
      "Episode: 550/5000, num_steps: 262, total_reward: -47.5\n",
      "\n",
      "======== new gamma: 0.19802807 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -113.701\n",
      "---------------------------\n",
      "Episode: 551/5000, num_steps: 181, total_reward: -18.35\n",
      "Episode: 552/5000, num_steps: 233, total_reward: -35.43\n",
      "Episode: 553/5000, num_steps: 182, total_reward: -47.28\n",
      "Episode: 554/5000, num_steps: 916, total_reward: 144.5\n",
      "Episode: 555/5000, num_steps: 312, total_reward: -236.5\n",
      "Episode: 556/5000, num_steps: 266, total_reward: 38.95\n",
      "Episode: 557/5000, num_steps: 321, total_reward: -196.9\n",
      "Episode: 558/5000, num_steps: 162, total_reward: -138.8\n",
      "Episode: 559/5000, num_steps: 231, total_reward: -78.04\n",
      "Episode: 560/5000, num_steps: 302, total_reward: -122.3\n",
      "\n",
      "======== new gamma: 0.20050342 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -61.5351\n",
      "---------------------------\n",
      "Episode: 561/5000, num_steps: 263, total_reward: -1.882\n",
      "Episode: 562/5000, num_steps: 164, total_reward: -354.4\n",
      "Episode: 563/5000, num_steps: 199, total_reward: -6.189\n",
      "Episode: 564/5000, num_steps: 325, total_reward: -108.9\n",
      "Episode: 565/5000, num_steps: 173, total_reward: 26.54\n",
      "Episode: 566/5000, num_steps: 200, total_reward: -44.24\n",
      "Episode: 567/5000, num_steps: 281, total_reward: 6.414\n",
      "Episode: 568/5000, num_steps: 353, total_reward: -108.9\n",
      "Episode: 569/5000, num_steps: 241, total_reward: -34.42\n",
      "Episode: 570/5000, num_steps: 286, total_reward: -5.887\n",
      "\n",
      "======== new gamma: 0.20300971 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -74.8267\n",
      "---------------------------\n",
      "Episode: 571/5000, num_steps: 284, total_reward: -131.7\n",
      "Episode: 572/5000, num_steps: 256, total_reward: -221.9\n",
      "Episode: 573/5000, num_steps: 141, total_reward: -292.1\n",
      "Episode: 574/5000, num_steps: 231, total_reward: 4.468\n",
      "Episode: 575/5000, num_steps: 178, total_reward: 27.5\n",
      "Episode: 576/5000, num_steps: 1000, total_reward: 71.29\n",
      "Episode: 577/5000, num_steps: 251, total_reward: -172.9\n",
      "Episode: 578/5000, num_steps: 333, total_reward: -15.14\n",
      "Episode: 579/5000, num_steps: 392, total_reward: -265.1\n",
      "Episode: 580/5000, num_steps: 314, total_reward: -8.974\n",
      "\n",
      "======== new gamma: 0.20554733 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -100.146\n",
      "---------------------------\n",
      "Episode: 581/5000, num_steps: 337, total_reward: -245.1\n",
      "Episode: 582/5000, num_steps: 126, total_reward: -143.5\n",
      "Episode: 583/5000, num_steps: 222, total_reward: -42.65\n",
      "Episode: 584/5000, num_steps: 123, total_reward: -242.0\n",
      "Episode: 585/5000, num_steps: 120, total_reward: -269.0\n",
      "Episode: 586/5000, num_steps: 233, total_reward: -342.0\n",
      "Episode: 587/5000, num_steps: 628, total_reward: 216.4\n",
      "Episode: 588/5000, num_steps: 162, total_reward: -268.0\n",
      "Episode: 589/5000, num_steps: 147, total_reward: -326.9\n",
      "Episode: 590/5000, num_steps: 107, total_reward: -333.2\n",
      "\n",
      "======== new gamma: 0.20811668 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -167.171\n",
      "---------------------------\n",
      "Episode: 591/5000, num_steps: 210, total_reward: -383.7\n",
      "Episode: 592/5000, num_steps: 171, total_reward: -281.9\n",
      "Episode: 593/5000, num_steps: 167, total_reward: -421.0\n",
      "Episode: 594/5000, num_steps: 194, total_reward: -313.8\n",
      "Episode: 595/5000, num_steps: 305, total_reward: -363.4\n",
      "Episode: 596/5000, num_steps: 314, total_reward: -11.16\n",
      "Episode: 597/5000, num_steps: 248, total_reward: -24.01\n",
      "Episode: 598/5000, num_steps: 426, total_reward: -225.3\n",
      "Episode: 599/5000, num_steps: 154, total_reward: -230.9\n",
      "Episode: 600/5000, num_steps: 216, total_reward: -185.6\n",
      "\n",
      "======== new gamma: 0.21071813 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -258.84\n",
      "---------------------------\n",
      "Episode: 601/5000, num_steps: 278, total_reward: -221.1\n",
      "Episode: 602/5000, num_steps: 360, total_reward: -284.0\n",
      "Episode: 603/5000, num_steps: 492, total_reward: 200.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 604/5000, num_steps: 184, total_reward: -159.2\n",
      "Episode: 605/5000, num_steps: 243, total_reward: -41.12\n",
      "Episode: 606/5000, num_steps: 184, total_reward: 6.116\n",
      "Episode: 607/5000, num_steps: 211, total_reward: -282.1\n",
      "Episode: 608/5000, num_steps: 189, total_reward: 63.2\n",
      "Episode: 609/5000, num_steps: 237, total_reward: -216.6\n",
      "Episode: 610/5000, num_steps: 234, total_reward: -243.9\n",
      "\n",
      "======== new gamma: 0.21335211 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -111.963\n",
      "---------------------------\n",
      "Episode: 611/5000, num_steps: 720, total_reward: 230.8\n",
      "Episode: 612/5000, num_steps: 230, total_reward: -309.8\n",
      "Episode: 613/5000, num_steps: 417, total_reward: -271.3\n",
      "Episode: 614/5000, num_steps: 242, total_reward: -186.5\n",
      "Episode: 615/5000, num_steps: 151, total_reward: -230.1\n",
      "Episode: 616/5000, num_steps: 136, total_reward: -165.9\n",
      "Episode: 617/5000, num_steps: 216, total_reward: -205.7\n",
      "Episode: 618/5000, num_steps: 289, total_reward: -201.6\n",
      "Episode: 619/5000, num_steps: 280, total_reward: 9.549\n",
      "Episode: 620/5000, num_steps: 268, total_reward: -57.22\n",
      "\n",
      "======== new gamma: 0.21601901 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -157.45\n",
      "---------------------------\n",
      "Episode: 621/5000, num_steps: 315, total_reward: -9.111\n",
      "Episode: 622/5000, num_steps: 240, total_reward: -232.6\n",
      "Episode: 623/5000, num_steps: 571, total_reward: -294.8\n",
      "Episode: 624/5000, num_steps: 250, total_reward: -54.28\n",
      "Episode: 625/5000, num_steps: 248, total_reward: -45.4\n",
      "Episode: 626/5000, num_steps: 150, total_reward: -163.6\n",
      "Episode: 627/5000, num_steps: 330, total_reward: 263.9\n",
      "Episode: 628/5000, num_steps: 278, total_reward: -2.467\n",
      "Episode: 629/5000, num_steps: 372, total_reward: -70.84\n",
      "Episode: 630/5000, num_steps: 194, total_reward: -26.39\n",
      "\n",
      "======== new gamma: 0.21871925 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -66.6439\n",
      "---------------------------\n",
      "Episode: 631/5000, num_steps: 300, total_reward: -262.0\n",
      "Episode: 632/5000, num_steps: 105, total_reward: -599.8\n",
      "Episode: 633/5000, num_steps: 295, total_reward: -160.1\n",
      "Episode: 634/5000, num_steps: 366, total_reward: -215.8\n",
      "Episode: 635/5000, num_steps: 428, total_reward: -93.31\n",
      "Episode: 636/5000, num_steps: 604, total_reward: 149.4\n",
      "Episode: 637/5000, num_steps: 256, total_reward: -249.6\n",
      "Episode: 638/5000, num_steps: 245, total_reward: -179.8\n",
      "Episode: 639/5000, num_steps: 323, total_reward: -120.4\n",
      "Episode: 640/5000, num_steps: 284, total_reward: -199.2\n",
      "\n",
      "======== new gamma: 0.22145324 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -175.769\n",
      "---------------------------\n",
      "Episode: 641/5000, num_steps: 1000, total_reward: 65.34\n",
      "Episode: 642/5000, num_steps: 273, total_reward: -53.93\n",
      "Episode: 643/5000, num_steps: 327, total_reward: -91.8\n",
      "Episode: 644/5000, num_steps: 275, total_reward: -285.9\n",
      "Episode: 645/5000, num_steps: 272, total_reward: -41.36\n",
      "Episode: 646/5000, num_steps: 350, total_reward: 186.6\n",
      "Episode: 647/5000, num_steps: 463, total_reward: 202.1\n",
      "Episode: 648/5000, num_steps: 253, total_reward: -64.78\n",
      "Episode: 649/5000, num_steps: 346, total_reward: -308.9\n",
      "Episode: 650/5000, num_steps: 297, total_reward: -43.28\n",
      "\n",
      "======== new gamma: 0.22422141 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -59.189\n",
      "---------------------------\n",
      "Episode: 651/5000, num_steps: 217, total_reward: -237.6\n",
      "Episode: 652/5000, num_steps: 169, total_reward: -334.0\n",
      "Episode: 653/5000, num_steps: 314, total_reward: 200.8\n",
      "Episode: 654/5000, num_steps: 242, total_reward: -209.3\n",
      "Episode: 655/5000, num_steps: 229, total_reward: -16.05\n",
      "Episode: 656/5000, num_steps: 111, total_reward: -364.2\n",
      "Episode: 657/5000, num_steps: 238, total_reward: 39.91\n",
      "Episode: 658/5000, num_steps: 254, total_reward: -250.6\n",
      "Episode: 659/5000, num_steps: 193, total_reward: -35.49\n",
      "Episode: 660/5000, num_steps: 210, total_reward: 5.876\n",
      "\n",
      "======== new gamma: 0.22702417 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -124.981\n",
      "---------------------------\n",
      "Episode: 661/5000, num_steps: 238, total_reward: -52.51\n",
      "Episode: 662/5000, num_steps: 244, total_reward: -187.5\n",
      "Episode: 663/5000, num_steps: 237, total_reward: -292.8\n",
      "Episode: 664/5000, num_steps: 301, total_reward: -249.2\n",
      "Episode: 665/5000, num_steps: 308, total_reward: -267.3\n",
      "Episode: 666/5000, num_steps: 166, total_reward: -306.2\n",
      "Episode: 667/5000, num_steps: 290, total_reward: -90.12\n",
      "Episode: 668/5000, num_steps: 182, total_reward: -33.75\n",
      "Episode: 669/5000, num_steps: 287, total_reward: -200.3\n",
      "Episode: 670/5000, num_steps: 133, total_reward: -189.6\n",
      "\n",
      "======== new gamma: 0.22986198 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -167.376\n",
      "---------------------------\n",
      "Episode: 671/5000, num_steps: 215, total_reward: -302.3\n",
      "Episode: 672/5000, num_steps: 657, total_reward: 130.4\n",
      "Episode: 673/5000, num_steps: 224, total_reward: -226.0\n",
      "Episode: 674/5000, num_steps: 230, total_reward: -166.1\n",
      "Episode: 675/5000, num_steps: 251, total_reward: -214.8\n",
      "Episode: 676/5000, num_steps: 192, total_reward: -1.112\n",
      "Episode: 677/5000, num_steps: 387, total_reward: -97.62\n",
      "Episode: 678/5000, num_steps: 259, total_reward: -205.6\n",
      "Episode: 679/5000, num_steps: 255, total_reward: -240.0\n",
      "Episode: 680/5000, num_steps: 148, total_reward: -136.6\n",
      "\n",
      "======== new gamma: 0.23273525 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -151.281\n",
      "---------------------------\n",
      "Episode: 681/5000, num_steps: 290, total_reward: -64.89\n",
      "Episode: 682/5000, num_steps: 292, total_reward: -206.0\n",
      "Episode: 683/5000, num_steps: 145, total_reward: -240.9\n",
      "Episode: 684/5000, num_steps: 232, total_reward: -73.32\n",
      "Episode: 685/5000, num_steps: 426, total_reward: -286.4\n",
      "Episode: 686/5000, num_steps: 230, total_reward: 15.73\n",
      "Episode: 687/5000, num_steps: 156, total_reward: -163.9\n",
      "Episode: 688/5000, num_steps: 308, total_reward: -82.86\n",
      "Episode: 689/5000, num_steps: 170, total_reward: -40.23\n",
      "Episode: 690/5000, num_steps: 233, total_reward: 2.644\n",
      "\n",
      "======== new gamma: 0.23564444 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -127.947\n",
      "---------------------------\n",
      "Episode: 691/5000, num_steps: 248, total_reward: -234.3\n",
      "Episode: 692/5000, num_steps: 284, total_reward: -87.15\n",
      "Episode: 693/5000, num_steps: 229, total_reward: -187.9\n",
      "Episode: 694/5000, num_steps: 356, total_reward: -84.63\n",
      "Episode: 695/5000, num_steps: 174, total_reward: -30.75\n",
      "Episode: 696/5000, num_steps: 218, total_reward: -22.0\n",
      "Episode: 697/5000, num_steps: 313, total_reward: -71.52\n",
      "Episode: 698/5000, num_steps: 281, total_reward: -246.1\n",
      "Episode: 699/5000, num_steps: 260, total_reward: -410.6\n",
      "Episode: 700/5000, num_steps: 233, total_reward: -304.1\n",
      "\n",
      "======== new gamma: 0.23859 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -137.217\n",
      "---------------------------\n",
      "Episode: 701/5000, num_steps: 263, total_reward: 47.26\n",
      "Episode: 702/5000, num_steps: 232, total_reward: -71.39\n",
      "Episode: 703/5000, num_steps: 281, total_reward: -179.7\n",
      "Episode: 704/5000, num_steps: 169, total_reward: -70.59\n",
      "Episode: 705/5000, num_steps: 223, total_reward: -180.4\n",
      "Episode: 706/5000, num_steps: 196, total_reward: -8.903\n",
      "Episode: 707/5000, num_steps: 322, total_reward: -57.13\n",
      "Episode: 708/5000, num_steps: 555, total_reward: 130.6\n",
      "Episode: 709/5000, num_steps: 228, total_reward: -286.4\n",
      "Episode: 710/5000, num_steps: 374, total_reward: -188.8\n",
      "\n",
      "======== new gamma: 0.24157237 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -98.0772\n",
      "---------------------------\n",
      "Episode: 711/5000, num_steps: 170, total_reward: -0.6207\n",
      "Episode: 712/5000, num_steps: 276, total_reward: -44.04\n",
      "Episode: 713/5000, num_steps: 328, total_reward: -298.7\n",
      "Episode: 714/5000, num_steps: 150, total_reward: -185.4\n",
      "Episode: 715/5000, num_steps: 236, total_reward: -50.46\n",
      "Episode: 716/5000, num_steps: 353, total_reward: -112.7\n",
      "Episode: 717/5000, num_steps: 162, total_reward: -109.7\n",
      "Episode: 718/5000, num_steps: 307, total_reward: -73.5\n",
      "Episode: 719/5000, num_steps: 286, total_reward: -224.7\n",
      "Episode: 720/5000, num_steps: 195, total_reward: -145.3\n",
      "\n",
      "======== new gamma: 0.24459203 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -128.86\n",
      "---------------------------\n",
      "Episode: 721/5000, num_steps: 165, total_reward: -27.19\n",
      "Episode: 722/5000, num_steps: 280, total_reward: -236.4\n",
      "Episode: 723/5000, num_steps: 230, total_reward: -281.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 724/5000, num_steps: 185, total_reward: 52.57\n",
      "Episode: 725/5000, num_steps: 423, total_reward: 260.6\n",
      "Episode: 726/5000, num_steps: 591, total_reward: 106.2\n",
      "Episode: 727/5000, num_steps: 151, total_reward: -186.2\n",
      "Episode: 728/5000, num_steps: 352, total_reward: -300.3\n",
      "Episode: 729/5000, num_steps: 498, total_reward: -280.6\n",
      "Episode: 730/5000, num_steps: 426, total_reward: -228.4\n",
      "\n",
      "======== new gamma: 0.24764943 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -103.857\n",
      "---------------------------\n",
      "Episode: 731/5000, num_steps: 329, total_reward: -110.6\n",
      "Episode: 732/5000, num_steps: 341, total_reward: -19.54\n",
      "Episode: 733/5000, num_steps: 242, total_reward: -146.8\n",
      "Episode: 734/5000, num_steps: 228, total_reward: -177.3\n",
      "Episode: 735/5000, num_steps: 411, total_reward: 207.2\n",
      "Episode: 736/5000, num_steps: 456, total_reward: 158.4\n",
      "Episode: 737/5000, num_steps: 165, total_reward: -493.5\n",
      "Episode: 738/5000, num_steps: 282, total_reward: -694.4\n",
      "Episode: 739/5000, num_steps: 187, total_reward: -452.1\n",
      "Episode: 740/5000, num_steps: 100, total_reward: -768.7\n",
      "\n",
      "======== new gamma: 0.25074504 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -195.703\n",
      "---------------------------\n",
      "Episode: 741/5000, num_steps: 182, total_reward: -190.6\n",
      "Episode: 742/5000, num_steps: 172, total_reward: -424.8\n",
      "Episode: 743/5000, num_steps: 216, total_reward: -494.8\n",
      "Episode: 744/5000, num_steps: 164, total_reward: -610.1\n",
      "Episode: 745/5000, num_steps: 225, total_reward: -170.8\n",
      "Episode: 746/5000, num_steps: 303, total_reward: -34.58\n",
      "Episode: 747/5000, num_steps: 400, total_reward: 184.9\n",
      "Episode: 748/5000, num_steps: 241, total_reward: -0.5056\n",
      "Episode: 749/5000, num_steps: 175, total_reward: -33.95\n",
      "Episode: 750/5000, num_steps: 278, total_reward: -115.8\n",
      "\n",
      "======== new gamma: 0.25387936 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -254.385\n",
      "---------------------------\n",
      "Episode: 751/5000, num_steps: 227, total_reward: -150.8\n",
      "Episode: 752/5000, num_steps: 268, total_reward: -224.2\n",
      "Episode: 753/5000, num_steps: 291, total_reward: 29.1\n",
      "Episode: 754/5000, num_steps: 290, total_reward: -101.8\n",
      "Episode: 755/5000, num_steps: 236, total_reward: -102.4\n",
      "Episode: 756/5000, num_steps: 443, total_reward: 240.3\n",
      "Episode: 757/5000, num_steps: 303, total_reward: -212.2\n",
      "Episode: 758/5000, num_steps: 250, total_reward: -190.9\n",
      "Episode: 759/5000, num_steps: 242, total_reward: -55.17\n",
      "Episode: 760/5000, num_steps: 291, total_reward: -250.1\n",
      "\n",
      "======== new gamma: 0.25705285 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -88.4007\n",
      "---------------------------\n",
      "Episode: 761/5000, num_steps: 231, total_reward: -212.7\n",
      "Episode: 762/5000, num_steps: 684, total_reward: 160.3\n",
      "Episode: 763/5000, num_steps: 277, total_reward: -268.2\n",
      "Episode: 764/5000, num_steps: 115, total_reward: -26.8\n",
      "Episode: 765/5000, num_steps: 301, total_reward: -216.4\n",
      "Episode: 766/5000, num_steps: 708, total_reward: 152.5\n",
      "Episode: 767/5000, num_steps: 290, total_reward: -185.2\n",
      "Episode: 768/5000, num_steps: 254, total_reward: -11.16\n",
      "Episode: 769/5000, num_steps: 320, total_reward: -221.5\n",
      "Episode: 770/5000, num_steps: 495, total_reward: -245.4\n",
      "\n",
      "======== new gamma: 0.26026601 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -107.95\n",
      "---------------------------\n",
      "Episode: 771/5000, num_steps: 275, total_reward: -338.7\n",
      "Episode: 772/5000, num_steps: 358, total_reward: -206.7\n",
      "Episode: 773/5000, num_steps: 240, total_reward: -153.9\n",
      "Episode: 774/5000, num_steps: 459, total_reward: 232.8\n",
      "Episode: 775/5000, num_steps: 1000, total_reward: 64.86\n",
      "Episode: 776/5000, num_steps: 226, total_reward: 29.96\n",
      "Episode: 777/5000, num_steps: 156, total_reward: -157.8\n",
      "Episode: 778/5000, num_steps: 214, total_reward: -93.72\n",
      "Episode: 779/5000, num_steps: 250, total_reward: -29.23\n",
      "Episode: 780/5000, num_steps: 172, total_reward: 39.08\n",
      "\n",
      "======== new gamma: 0.26351934 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -89.7663\n",
      "---------------------------\n",
      "Episode: 781/5000, num_steps: 267, total_reward: -181.8\n",
      "Episode: 782/5000, num_steps: 363, total_reward: -253.9\n",
      "Episode: 783/5000, num_steps: 516, total_reward: 196.8\n",
      "Episode: 784/5000, num_steps: 267, total_reward: -22.26\n",
      "Episode: 785/5000, num_steps: 329, total_reward: -270.8\n",
      "Episode: 786/5000, num_steps: 250, total_reward: -56.77\n",
      "Episode: 787/5000, num_steps: 452, total_reward: 229.9\n",
      "Episode: 788/5000, num_steps: 396, total_reward: 242.9\n",
      "Episode: 789/5000, num_steps: 458, total_reward: -231.5\n",
      "Episode: 790/5000, num_steps: 179, total_reward: 16.38\n",
      "\n",
      "======== new gamma: 0.26681333 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -30.8399\n",
      "---------------------------\n",
      "Episode: 791/5000, num_steps: 262, total_reward: -242.5\n",
      "Episode: 792/5000, num_steps: 977, total_reward: 217.9\n",
      "Episode: 793/5000, num_steps: 407, total_reward: 231.9\n",
      "Episode: 794/5000, num_steps: 355, total_reward: -56.72\n",
      "Episode: 795/5000, num_steps: 175, total_reward: -5.726\n",
      "Episode: 796/5000, num_steps: 204, total_reward: -210.9\n",
      "Episode: 797/5000, num_steps: 278, total_reward: -60.2\n",
      "Episode: 798/5000, num_steps: 564, total_reward: 153.0\n",
      "Episode: 799/5000, num_steps: 234, total_reward: -196.2\n",
      "Episode: 800/5000, num_steps: 164, total_reward: -78.28\n",
      "\n",
      "======== new gamma: 0.27014849 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -15.3063\n",
      "---------------------------\n",
      "Episode: 801/5000, num_steps: 429, total_reward: -274.9\n",
      "Episode: 802/5000, num_steps: 308, total_reward: 42.86\n",
      "Episode: 803/5000, num_steps: 170, total_reward: -94.97\n",
      "Episode: 804/5000, num_steps: 208, total_reward: -128.4\n",
      "Episode: 805/5000, num_steps: 264, total_reward: -60.11\n",
      "Episode: 806/5000, num_steps: 286, total_reward: -259.0\n",
      "Episode: 807/5000, num_steps: 380, total_reward: -227.1\n",
      "Episode: 808/5000, num_steps: 265, total_reward: -64.45\n",
      "Episode: 809/5000, num_steps: 244, total_reward: 3.784\n",
      "Episode: 810/5000, num_steps: 377, total_reward: -101.5\n",
      "\n",
      "======== new gamma: 0.27352535 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -114.059\n",
      "---------------------------\n",
      "Episode: 811/5000, num_steps: 298, total_reward: -44.8\n",
      "Episode: 812/5000, num_steps: 256, total_reward: -213.7\n",
      "Episode: 813/5000, num_steps: 324, total_reward: -210.8\n",
      "Episode: 814/5000, num_steps: 280, total_reward: -201.9\n",
      "Episode: 815/5000, num_steps: 348, total_reward: -106.2\n",
      "Episode: 816/5000, num_steps: 1000, total_reward: 83.15\n",
      "Episode: 817/5000, num_steps: 234, total_reward: -179.7\n",
      "Episode: 818/5000, num_steps: 266, total_reward: -242.1\n",
      "Episode: 819/5000, num_steps: 206, total_reward: -195.1\n",
      "Episode: 820/5000, num_steps: 256, total_reward: -33.41\n",
      "\n",
      "======== new gamma: 0.27694442 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -141.257\n",
      "---------------------------\n",
      "Episode: 821/5000, num_steps: 164, total_reward: 20.6\n",
      "Episode: 822/5000, num_steps: 273, total_reward: -250.3\n",
      "Episode: 823/5000, num_steps: 352, total_reward: -237.4\n",
      "Episode: 824/5000, num_steps: 226, total_reward: -12.03\n",
      "Episode: 825/5000, num_steps: 384, total_reward: -26.05\n",
      "Episode: 826/5000, num_steps: 221, total_reward: -215.5\n",
      "Episode: 827/5000, num_steps: 334, total_reward: -213.3\n",
      "Episode: 828/5000, num_steps: 288, total_reward: -222.7\n",
      "Episode: 829/5000, num_steps: 508, total_reward: 230.3\n",
      "Episode: 830/5000, num_steps: 208, total_reward: -211.8\n",
      "\n",
      "======== new gamma: 0.28040622 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -95.9806\n",
      "---------------------------\n",
      "Episode: 831/5000, num_steps: 222, total_reward: -57.54\n",
      "Episode: 832/5000, num_steps: 267, total_reward: -246.8\n",
      "Episode: 833/5000, num_steps: 231, total_reward: -199.2\n",
      "Episode: 834/5000, num_steps: 284, total_reward: -21.47\n",
      "Episode: 835/5000, num_steps: 210, total_reward: -181.3\n",
      "Episode: 836/5000, num_steps: 176, total_reward: 1.572\n",
      "Episode: 837/5000, num_steps: 240, total_reward: -57.13\n",
      "Episode: 838/5000, num_steps: 238, total_reward: -31.86\n",
      "Episode: 839/5000, num_steps: 342, total_reward: -40.13\n",
      "Episode: 840/5000, num_steps: 202, total_reward: -181.9\n",
      "\n",
      "======== new gamma: 0.2839113 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -104.562\n",
      "---------------------------\n",
      "Episode: 841/5000, num_steps: 229, total_reward: -226.6\n",
      "Episode: 842/5000, num_steps: 273, total_reward: -2.727\n",
      "Episode: 843/5000, num_steps: 277, total_reward: -240.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 844/5000, num_steps: 229, total_reward: -204.7\n",
      "Episode: 845/5000, num_steps: 289, total_reward: 21.44\n",
      "Episode: 846/5000, num_steps: 646, total_reward: 216.8\n",
      "Episode: 847/5000, num_steps: 584, total_reward: 184.6\n",
      "Episode: 848/5000, num_steps: 241, total_reward: -24.69\n",
      "Episode: 849/5000, num_steps: 262, total_reward: -22.39\n",
      "Episode: 850/5000, num_steps: 292, total_reward: -28.63\n",
      "\n",
      "======== new gamma: 0.28746019 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -48.0776\n",
      "---------------------------\n",
      "Episode: 851/5000, num_steps: 233, total_reward: -14.38\n",
      "Episode: 852/5000, num_steps: 309, total_reward: -216.7\n",
      "Episode: 853/5000, num_steps: 305, total_reward: 25.06\n",
      "Episode: 854/5000, num_steps: 400, total_reward: -112.5\n",
      "Episode: 855/5000, num_steps: 227, total_reward: -62.82\n",
      "Episode: 856/5000, num_steps: 363, total_reward: -248.1\n",
      "Episode: 857/5000, num_steps: 186, total_reward: 79.89\n",
      "Episode: 858/5000, num_steps: 229, total_reward: 25.39\n",
      "Episode: 859/5000, num_steps: 322, total_reward: -232.0\n",
      "Episode: 860/5000, num_steps: 304, total_reward: -242.0\n",
      "\n",
      "======== new gamma: 0.29105344 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -78.4789\n",
      "---------------------------\n",
      "Episode: 861/5000, num_steps: 266, total_reward: -16.83\n",
      "Episode: 862/5000, num_steps: 259, total_reward: -18.96\n",
      "Episode: 863/5000, num_steps: 240, total_reward: -52.86\n",
      "Episode: 864/5000, num_steps: 306, total_reward: -34.2\n",
      "Episode: 865/5000, num_steps: 249, total_reward: -47.51\n",
      "Episode: 866/5000, num_steps: 226, total_reward: -376.9\n",
      "Episode: 867/5000, num_steps: 181, total_reward: -44.7\n",
      "Episode: 868/5000, num_steps: 288, total_reward: 2.484\n",
      "Episode: 869/5000, num_steps: 297, total_reward: -334.1\n",
      "Episode: 870/5000, num_steps: 257, total_reward: -170.8\n",
      "\n",
      "======== new gamma: 0.29469161 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -116.558\n",
      "---------------------------\n",
      "Episode: 871/5000, num_steps: 151, total_reward: -124.6\n",
      "Episode: 872/5000, num_steps: 240, total_reward: -122.5\n",
      "Episode: 873/5000, num_steps: 279, total_reward: -22.89\n",
      "Episode: 874/5000, num_steps: 232, total_reward: -61.97\n",
      "Episode: 875/5000, num_steps: 247, total_reward: -50.4\n",
      "Episode: 876/5000, num_steps: 309, total_reward: -245.6\n",
      "Episode: 877/5000, num_steps: 219, total_reward: -214.3\n",
      "Episode: 878/5000, num_steps: 184, total_reward: -178.7\n",
      "Episode: 879/5000, num_steps: 243, total_reward: -122.6\n",
      "Episode: 880/5000, num_steps: 373, total_reward: -298.3\n",
      "\n",
      "======== new gamma: 0.29837526 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -131.443\n",
      "---------------------------\n",
      "Episode: 881/5000, num_steps: 246, total_reward: -178.4\n",
      "Episode: 882/5000, num_steps: 265, total_reward: -11.14\n",
      "Episode: 883/5000, num_steps: 499, total_reward: -116.9\n",
      "Episode: 884/5000, num_steps: 192, total_reward: 35.71\n",
      "Episode: 885/5000, num_steps: 292, total_reward: -216.5\n",
      "Episode: 886/5000, num_steps: 254, total_reward: -181.0\n",
      "Episode: 887/5000, num_steps: 314, total_reward: -32.51\n",
      "Episode: 888/5000, num_steps: 353, total_reward: -254.6\n",
      "Episode: 889/5000, num_steps: 227, total_reward: -64.63\n",
      "Episode: 890/5000, num_steps: 197, total_reward: 23.65\n",
      "\n",
      "======== new gamma: 0.30210495 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -131.836\n",
      "---------------------------\n",
      "Episode: 891/5000, num_steps: 225, total_reward: -47.46\n",
      "Episode: 892/5000, num_steps: 231, total_reward: -253.3\n",
      "Episode: 893/5000, num_steps: 315, total_reward: -33.45\n",
      "Episode: 894/5000, num_steps: 430, total_reward: -133.5\n",
      "Episode: 895/5000, num_steps: 313, total_reward: 42.87\n",
      "Episode: 896/5000, num_steps: 326, total_reward: -59.7\n",
      "Episode: 897/5000, num_steps: 600, total_reward: 207.5\n",
      "Episode: 898/5000, num_steps: 298, total_reward: -94.99\n",
      "Episode: 899/5000, num_steps: 354, total_reward: -271.0\n",
      "Episode: 900/5000, num_steps: 240, total_reward: -64.18\n",
      "\n",
      "======== new gamma: 0.30588126 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -61.9358\n",
      "---------------------------\n",
      "Episode: 901/5000, num_steps: 302, total_reward: -229.0\n",
      "Episode: 902/5000, num_steps: 206, total_reward: -106.3\n",
      "Episode: 903/5000, num_steps: 221, total_reward: -29.74\n",
      "Episode: 904/5000, num_steps: 202, total_reward: -169.2\n",
      "Episode: 905/5000, num_steps: 274, total_reward: -68.45\n",
      "Episode: 906/5000, num_steps: 275, total_reward: -39.26\n",
      "Episode: 907/5000, num_steps: 293, total_reward: -103.8\n",
      "Episode: 908/5000, num_steps: 245, total_reward: -27.38\n",
      "Episode: 909/5000, num_steps: 220, total_reward: -193.2\n",
      "Episode: 910/5000, num_steps: 353, total_reward: -231.7\n",
      "\n",
      "======== new gamma: 0.30970478 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -103.059\n",
      "---------------------------\n",
      "Episode: 911/5000, num_steps: 334, total_reward: -70.93\n",
      "Episode: 912/5000, num_steps: 288, total_reward: -204.9\n",
      "Episode: 913/5000, num_steps: 166, total_reward: -184.5\n",
      "Episode: 914/5000, num_steps: 369, total_reward: -69.4\n",
      "Episode: 915/5000, num_steps: 363, total_reward: 248.2\n",
      "Episode: 916/5000, num_steps: 666, total_reward: 230.8\n",
      "Episode: 917/5000, num_steps: 251, total_reward: -37.27\n",
      "Episode: 918/5000, num_steps: 239, total_reward: -81.39\n",
      "Episode: 919/5000, num_steps: 314, total_reward: -215.3\n",
      "Episode: 920/5000, num_steps: 272, total_reward: -97.28\n",
      "\n",
      "======== new gamma: 0.31357608 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -61.6212\n",
      "---------------------------\n",
      "Episode: 921/5000, num_steps: 240, total_reward: -51.25\n",
      "Episode: 922/5000, num_steps: 315, total_reward: -48.68\n",
      "Episode: 923/5000, num_steps: 729, total_reward: 204.7\n",
      "Episode: 924/5000, num_steps: 307, total_reward: -38.3\n",
      "Episode: 925/5000, num_steps: 180, total_reward: -5.589\n",
      "Episode: 926/5000, num_steps: 223, total_reward: 14.21\n",
      "Episode: 927/5000, num_steps: 227, total_reward: -14.88\n",
      "Episode: 928/5000, num_steps: 241, total_reward: -215.4\n",
      "Episode: 929/5000, num_steps: 237, total_reward: -32.75\n",
      "Episode: 930/5000, num_steps: 303, total_reward: -22.84\n",
      "\n",
      "======== new gamma: 0.31749579 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -28.5208\n",
      "---------------------------\n",
      "Episode: 931/5000, num_steps: 338, total_reward: -286.1\n",
      "Episode: 932/5000, num_steps: 238, total_reward: -77.68\n",
      "Episode: 933/5000, num_steps: 255, total_reward: -40.79\n",
      "Episode: 934/5000, num_steps: 288, total_reward: -189.7\n",
      "Episode: 935/5000, num_steps: 260, total_reward: -28.0\n",
      "Episode: 936/5000, num_steps: 184, total_reward: -24.03\n",
      "Episode: 937/5000, num_steps: 288, total_reward: -216.3\n",
      "Episode: 938/5000, num_steps: 273, total_reward: 7.155\n",
      "Episode: 939/5000, num_steps: 396, total_reward: -70.29\n",
      "Episode: 940/5000, num_steps: 423, total_reward: 243.1\n",
      "\n",
      "======== new gamma: 0.32146448 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -94.8616\n",
      "---------------------------\n",
      "Episode: 941/5000, num_steps: 292, total_reward: -180.3\n",
      "Episode: 942/5000, num_steps: 229, total_reward: -63.25\n",
      "Episode: 943/5000, num_steps: 251, total_reward: -66.23\n",
      "Episode: 944/5000, num_steps: 320, total_reward: -249.5\n",
      "Episode: 945/5000, num_steps: 289, total_reward: -93.51\n",
      "Episode: 946/5000, num_steps: 311, total_reward: -226.4\n",
      "Episode: 947/5000, num_steps: 454, total_reward: -107.5\n",
      "Episode: 948/5000, num_steps: 299, total_reward: -196.1\n",
      "Episode: 949/5000, num_steps: 628, total_reward: 203.9\n",
      "Episode: 950/5000, num_steps: 236, total_reward: -37.12\n",
      "\n",
      "======== new gamma: 0.32548279 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -73.5691\n",
      "---------------------------\n",
      "Episode: 951/5000, num_steps: 147, total_reward: -173.8\n",
      "Episode: 952/5000, num_steps: 241, total_reward: -38.45\n",
      "Episode: 953/5000, num_steps: 879, total_reward: 202.5\n",
      "Episode: 954/5000, num_steps: 329, total_reward: -22.95\n",
      "Episode: 955/5000, num_steps: 220, total_reward: -184.4\n",
      "Episode: 956/5000, num_steps: 367, total_reward: -250.5\n",
      "Episode: 957/5000, num_steps: 259, total_reward: -39.62\n",
      "Episode: 958/5000, num_steps: 171, total_reward: -141.3\n",
      "Episode: 959/5000, num_steps: 206, total_reward: -202.0\n",
      "Episode: 960/5000, num_steps: 357, total_reward: -211.0\n",
      "\n",
      "======== new gamma: 0.32955132 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -88.7571\n",
      "---------------------------\n",
      "Episode: 961/5000, num_steps: 341, total_reward: -270.1\n",
      "Episode: 962/5000, num_steps: 365, total_reward: -18.46\n",
      "Episode: 963/5000, num_steps: 229, total_reward: -215.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 964/5000, num_steps: 510, total_reward: -159.6\n",
      "Episode: 965/5000, num_steps: 208, total_reward: -177.9\n",
      "Episode: 966/5000, num_steps: 264, total_reward: -11.95\n",
      "Episode: 967/5000, num_steps: 208, total_reward: -195.9\n",
      "Episode: 968/5000, num_steps: 429, total_reward: -44.72\n",
      "Episode: 969/5000, num_steps: 247, total_reward: -82.52\n",
      "Episode: 970/5000, num_steps: 218, total_reward: -91.43\n",
      "\n",
      "======== new gamma: 0.33367072 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -138.724\n",
      "---------------------------\n",
      "Episode: 971/5000, num_steps: 227, total_reward: -216.9\n",
      "Episode: 972/5000, num_steps: 387, total_reward: -240.9\n",
      "Episode: 973/5000, num_steps: 334, total_reward: -168.2\n",
      "Episode: 974/5000, num_steps: 356, total_reward: -66.3\n",
      "Episode: 975/5000, num_steps: 383, total_reward: -251.7\n",
      "Episode: 976/5000, num_steps: 247, total_reward: -67.51\n",
      "Episode: 977/5000, num_steps: 334, total_reward: -142.9\n",
      "Episode: 978/5000, num_steps: 255, total_reward: -100.4\n",
      "Episode: 979/5000, num_steps: 221, total_reward: -247.5\n",
      "Episode: 980/5000, num_steps: 293, total_reward: -69.44\n",
      "\n",
      "======== new gamma: 0.3378416 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -159.376\n",
      "---------------------------\n",
      "Episode: 981/5000, num_steps: 398, total_reward: -60.62\n",
      "Episode: 982/5000, num_steps: 425, total_reward: -51.14\n",
      "Episode: 983/5000, num_steps: 236, total_reward: -112.3\n",
      "Episode: 984/5000, num_steps: 239, total_reward: -34.41\n",
      "Episode: 985/5000, num_steps: 285, total_reward: -26.9\n",
      "Episode: 986/5000, num_steps: 421, total_reward: -106.1\n",
      "Episode: 987/5000, num_steps: 1000, total_reward: 124.8\n",
      "Episode: 988/5000, num_steps: 220, total_reward: -182.7\n",
      "Episode: 989/5000, num_steps: 287, total_reward: -193.0\n",
      "Episode: 990/5000, num_steps: 277, total_reward: -10.93\n",
      "\n",
      "======== new gamma: 0.34206462 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -71.1709\n",
      "---------------------------\n",
      "Episode: 991/5000, num_steps: 359, total_reward: 18.07\n",
      "Episode: 992/5000, num_steps: 289, total_reward: -92.83\n",
      "Episode: 993/5000, num_steps: 421, total_reward: -226.3\n",
      "Episode: 994/5000, num_steps: 293, total_reward: -209.9\n",
      "Episode: 995/5000, num_steps: 776, total_reward: 236.4\n",
      "Episode: 996/5000, num_steps: 217, total_reward: -238.6\n",
      "Episode: 997/5000, num_steps: 264, total_reward: -226.0\n",
      "Episode: 998/5000, num_steps: 211, total_reward: -62.41\n",
      "Episode: 999/5000, num_steps: 184, total_reward: 27.51\n",
      "Episode: 1000/5000, num_steps: 296, total_reward: -211.3\n",
      "\n",
      "======== new gamma: 0.34634043 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -78.4927\n",
      "---------------------------\n",
      "Episode: 1001/5000, num_steps: 208, total_reward: 37.44\n",
      "Episode: 1002/5000, num_steps: 392, total_reward: -90.1\n",
      "Episode: 1003/5000, num_steps: 218, total_reward: -188.9\n",
      "Episode: 1004/5000, num_steps: 477, total_reward: -139.8\n",
      "Episode: 1005/5000, num_steps: 370, total_reward: -83.03\n",
      "Episode: 1006/5000, num_steps: 282, total_reward: -154.7\n",
      "Episode: 1007/5000, num_steps: 310, total_reward: -100.9\n",
      "Episode: 1008/5000, num_steps: 330, total_reward: -23.06\n",
      "Episode: 1009/5000, num_steps: 259, total_reward: -73.49\n",
      "Episode: 1010/5000, num_steps: 190, total_reward: -50.14\n",
      "\n",
      "======== new gamma: 0.35066968 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -102.777\n",
      "---------------------------\n",
      "Episode: 1011/5000, num_steps: 247, total_reward: -23.87\n",
      "Episode: 1012/5000, num_steps: 292, total_reward: -47.61\n",
      "Episode: 1013/5000, num_steps: 384, total_reward: -97.72\n",
      "Episode: 1014/5000, num_steps: 191, total_reward: 33.71\n",
      "Episode: 1015/5000, num_steps: 250, total_reward: -49.28\n",
      "Episode: 1016/5000, num_steps: 277, total_reward: -281.3\n",
      "Episode: 1017/5000, num_steps: 289, total_reward: -317.1\n",
      "Episode: 1018/5000, num_steps: 523, total_reward: 240.5\n",
      "Episode: 1019/5000, num_steps: 182, total_reward: -516.0\n",
      "Episode: 1020/5000, num_steps: 275, total_reward: -1.274\n",
      "\n",
      "======== new gamma: 0.35505305 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -110.878\n",
      "---------------------------\n",
      "Episode: 1021/5000, num_steps: 167, total_reward: -45.63\n",
      "Episode: 1022/5000, num_steps: 782, total_reward: 238.2\n",
      "Episode: 1023/5000, num_steps: 303, total_reward: -35.02\n",
      "Episode: 1024/5000, num_steps: 339, total_reward: -260.0\n",
      "Episode: 1025/5000, num_steps: 170, total_reward: -155.2\n",
      "Episode: 1026/5000, num_steps: 373, total_reward: -266.4\n",
      "Episode: 1027/5000, num_steps: 316, total_reward: -44.93\n",
      "Episode: 1028/5000, num_steps: 520, total_reward: -50.27\n",
      "Episode: 1029/5000, num_steps: 261, total_reward: -177.1\n",
      "Episode: 1030/5000, num_steps: 336, total_reward: -30.99\n",
      "\n",
      "======== new gamma: 0.35949122 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -79.7727\n",
      "---------------------------\n",
      "Episode: 1031/5000, num_steps: 229, total_reward: -205.4\n",
      "Episode: 1032/5000, num_steps: 342, total_reward: -188.6\n",
      "Episode: 1033/5000, num_steps: 276, total_reward: -184.8\n",
      "Episode: 1034/5000, num_steps: 319, total_reward: -51.02\n",
      "Episode: 1035/5000, num_steps: 360, total_reward: -223.6\n",
      "Episode: 1036/5000, num_steps: 291, total_reward: 16.39\n",
      "Episode: 1037/5000, num_steps: 177, total_reward: -66.06\n",
      "Episode: 1038/5000, num_steps: 244, total_reward: -83.13\n",
      "Episode: 1039/5000, num_steps: 236, total_reward: -77.86\n",
      "Episode: 1040/5000, num_steps: 305, total_reward: -57.94\n",
      "\n",
      "======== new gamma: 0.36398486 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -109.505\n",
      "---------------------------\n",
      "Episode: 1041/5000, num_steps: 200, total_reward: 16.05\n",
      "Episode: 1042/5000, num_steps: 304, total_reward: -55.69\n",
      "Episode: 1043/5000, num_steps: 224, total_reward: -219.7\n",
      "Episode: 1044/5000, num_steps: 306, total_reward: -35.88\n",
      "Episode: 1045/5000, num_steps: 558, total_reward: 180.5\n",
      "Episode: 1046/5000, num_steps: 312, total_reward: -278.5\n",
      "Episode: 1047/5000, num_steps: 354, total_reward: -209.0\n",
      "Episode: 1048/5000, num_steps: 234, total_reward: -202.4\n",
      "Episode: 1049/5000, num_steps: 387, total_reward: -20.07\n",
      "Episode: 1050/5000, num_steps: 579, total_reward: -41.8\n",
      "\n",
      "======== new gamma: 0.36853467 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -88.2612\n",
      "---------------------------\n",
      "Episode: 1051/5000, num_steps: 310, total_reward: -30.09\n",
      "Episode: 1052/5000, num_steps: 329, total_reward: -211.4\n",
      "Episode: 1053/5000, num_steps: 350, total_reward: -235.8\n",
      "Episode: 1054/5000, num_steps: 286, total_reward: 25.73\n",
      "Episode: 1055/5000, num_steps: 431, total_reward: -106.2\n",
      "Episode: 1056/5000, num_steps: 614, total_reward: 149.5\n",
      "Episode: 1057/5000, num_steps: 283, total_reward: -235.4\n",
      "Episode: 1058/5000, num_steps: 297, total_reward: -240.6\n",
      "Episode: 1059/5000, num_steps: 318, total_reward: -28.29\n",
      "Episode: 1060/5000, num_steps: 216, total_reward: -215.9\n",
      "\n",
      "======== new gamma: 0.37314135 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -95.4335\n",
      "---------------------------\n",
      "Episode: 1061/5000, num_steps: 175, total_reward: -477.3\n",
      "Episode: 1062/5000, num_steps: 165, total_reward: -69.86\n",
      "Episode: 1063/5000, num_steps: 288, total_reward: -63.29\n",
      "Episode: 1064/5000, num_steps: 419, total_reward: -34.98\n",
      "Episode: 1065/5000, num_steps: 219, total_reward: -184.1\n",
      "Episode: 1066/5000, num_steps: 358, total_reward: -230.2\n",
      "Episode: 1067/5000, num_steps: 197, total_reward: -67.16\n",
      "Episode: 1068/5000, num_steps: 318, total_reward: -24.8\n",
      "Episode: 1069/5000, num_steps: 303, total_reward: -233.1\n",
      "Episode: 1070/5000, num_steps: 343, total_reward: -3.236\n",
      "\n",
      "======== new gamma: 0.37780562 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -160.064\n",
      "---------------------------\n",
      "Episode: 1071/5000, num_steps: 242, total_reward: -168.4\n",
      "Episode: 1072/5000, num_steps: 310, total_reward: -77.48\n",
      "Episode: 1073/5000, num_steps: 217, total_reward: -78.16\n",
      "Episode: 1074/5000, num_steps: 361, total_reward: -258.9\n",
      "Episode: 1075/5000, num_steps: 340, total_reward: 9.806\n",
      "Episode: 1076/5000, num_steps: 283, total_reward: -59.08\n",
      "Episode: 1077/5000, num_steps: 310, total_reward: -173.7\n",
      "Episode: 1078/5000, num_steps: 426, total_reward: -360.7\n",
      "Episode: 1079/5000, num_steps: 395, total_reward: -3.396\n",
      "Episode: 1080/5000, num_steps: 273, total_reward: 25.63\n",
      "\n",
      "======== new gamma: 0.38252819 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -117.334\n",
      "---------------------------\n",
      "Episode: 1081/5000, num_steps: 241, total_reward: -38.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1082/5000, num_steps: 428, total_reward: -72.36\n",
      "Episode: 1083/5000, num_steps: 310, total_reward: -2.027\n",
      "Episode: 1084/5000, num_steps: 248, total_reward: -177.9\n",
      "Episode: 1085/5000, num_steps: 239, total_reward: -166.0\n",
      "Episode: 1086/5000, num_steps: 308, total_reward: -189.2\n",
      "Episode: 1087/5000, num_steps: 237, total_reward: -45.6\n",
      "Episode: 1088/5000, num_steps: 235, total_reward: -3.866\n",
      "Episode: 1089/5000, num_steps: 178, total_reward: -177.5\n",
      "Episode: 1090/5000, num_steps: 294, total_reward: -47.77\n",
      "\n",
      "======== new gamma: 0.38730979 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -84.7665\n",
      "---------------------------\n",
      "Episode: 1091/5000, num_steps: 340, total_reward: -236.8\n",
      "Episode: 1092/5000, num_steps: 264, total_reward: -9.837\n",
      "Episode: 1093/5000, num_steps: 280, total_reward: 2.997\n",
      "Episode: 1094/5000, num_steps: 367, total_reward: -94.43\n",
      "Episode: 1095/5000, num_steps: 298, total_reward: -229.3\n",
      "Episode: 1096/5000, num_steps: 267, total_reward: -51.14\n",
      "Episode: 1097/5000, num_steps: 358, total_reward: -40.64\n",
      "Episode: 1098/5000, num_steps: 214, total_reward: -159.9\n",
      "Episode: 1099/5000, num_steps: 351, total_reward: 11.65\n",
      "Episode: 1100/5000, num_steps: 520, total_reward: 237.1\n",
      "\n",
      "======== new gamma: 0.39215116 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -85.5111\n",
      "---------------------------\n",
      "Episode: 1101/5000, num_steps: 350, total_reward: -12.18\n",
      "Episode: 1102/5000, num_steps: 371, total_reward: -53.52\n",
      "Episode: 1103/5000, num_steps: 318, total_reward: -84.3\n",
      "Episode: 1104/5000, num_steps: 345, total_reward: -239.0\n",
      "Episode: 1105/5000, num_steps: 483, total_reward: -219.4\n",
      "Episode: 1106/5000, num_steps: 174, total_reward: -32.48\n",
      "Episode: 1107/5000, num_steps: 118, total_reward: -16.62\n",
      "Episode: 1108/5000, num_steps: 179, total_reward: -9.009\n",
      "Episode: 1109/5000, num_steps: 393, total_reward: -207.6\n",
      "Episode: 1110/5000, num_steps: 296, total_reward: -78.58\n",
      "\n",
      "======== new gamma: 0.39705305 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -63.703\n",
      "---------------------------\n",
      "Episode: 1111/5000, num_steps: 257, total_reward: -147.2\n",
      "Episode: 1112/5000, num_steps: 379, total_reward: -3.708\n",
      "Episode: 1113/5000, num_steps: 281, total_reward: -199.9\n",
      "Episode: 1114/5000, num_steps: 356, total_reward: -18.24\n",
      "Episode: 1115/5000, num_steps: 296, total_reward: -324.0\n",
      "Episode: 1116/5000, num_steps: 325, total_reward: -3.264\n",
      "Episode: 1117/5000, num_steps: 472, total_reward: 220.8\n",
      "Episode: 1118/5000, num_steps: 246, total_reward: -51.49\n",
      "Episode: 1119/5000, num_steps: 250, total_reward: -96.83\n",
      "Episode: 1120/5000, num_steps: 160, total_reward: -86.96\n",
      "\n",
      "======== new gamma: 0.40201622 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -70.2434\n",
      "---------------------------\n",
      "Episode: 1121/5000, num_steps: 462, total_reward: -45.1\n",
      "Episode: 1122/5000, num_steps: 244, total_reward: -12.01\n",
      "Episode: 1123/5000, num_steps: 314, total_reward: -99.67\n",
      "Episode: 1124/5000, num_steps: 316, total_reward: -6.077\n",
      "Episode: 1125/5000, num_steps: 262, total_reward: -18.4\n",
      "Episode: 1126/5000, num_steps: 319, total_reward: -98.59\n",
      "Episode: 1127/5000, num_steps: 171, total_reward: -127.1\n",
      "Episode: 1128/5000, num_steps: 1000, total_reward: 93.69\n",
      "Episode: 1129/5000, num_steps: 286, total_reward: 4.312\n",
      "Episode: 1130/5000, num_steps: 332, total_reward: -178.3\n",
      "\n",
      "======== new gamma: 0.40704142 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -39.5917\n",
      "---------------------------\n",
      "Episode: 1131/5000, num_steps: 582, total_reward: 200.1\n",
      "Episode: 1132/5000, num_steps: 264, total_reward: -78.89\n",
      "Episode: 1133/5000, num_steps: 371, total_reward: -255.4\n",
      "Episode: 1134/5000, num_steps: 304, total_reward: -94.72\n",
      "Episode: 1135/5000, num_steps: 247, total_reward: 16.62\n",
      "Episode: 1136/5000, num_steps: 784, total_reward: 240.1\n",
      "Episode: 1137/5000, num_steps: 287, total_reward: 45.66\n",
      "Episode: 1138/5000, num_steps: 313, total_reward: 14.6\n",
      "Episode: 1139/5000, num_steps: 565, total_reward: -188.0\n",
      "Episode: 1140/5000, num_steps: 213, total_reward: -192.1\n",
      "\n",
      "======== new gamma: 0.41212944 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -27.8326\n",
      "---------------------------\n",
      "Episode: 1141/5000, num_steps: 260, total_reward: -86.9\n",
      "Episode: 1142/5000, num_steps: 296, total_reward: -169.1\n",
      "Episode: 1143/5000, num_steps: 279, total_reward: -228.3\n",
      "Episode: 1144/5000, num_steps: 329, total_reward: -215.0\n",
      "Episode: 1145/5000, num_steps: 184, total_reward: -22.92\n",
      "Episode: 1146/5000, num_steps: 342, total_reward: 214.7\n",
      "Episode: 1147/5000, num_steps: 653, total_reward: 232.9\n",
      "Episode: 1148/5000, num_steps: 399, total_reward: -28.9\n",
      "Episode: 1149/5000, num_steps: 583, total_reward: 208.6\n",
      "Episode: 1150/5000, num_steps: 247, total_reward: -13.18\n",
      "\n",
      "======== new gamma: 0.41728105 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -28.6953\n",
      "---------------------------\n",
      "Episode: 1151/5000, num_steps: 754, total_reward: 225.4\n",
      "Episode: 1152/5000, num_steps: 547, total_reward: 222.7\n",
      "Episode: 1153/5000, num_steps: 319, total_reward: -79.64\n",
      "Episode: 1154/5000, num_steps: 266, total_reward: 29.2\n",
      "Episode: 1155/5000, num_steps: 336, total_reward: -181.6\n",
      "Episode: 1156/5000, num_steps: 316, total_reward: -71.03\n",
      "Episode: 1157/5000, num_steps: 189, total_reward: 66.62\n",
      "Episode: 1158/5000, num_steps: 293, total_reward: -81.48\n",
      "Episode: 1159/5000, num_steps: 687, total_reward: 206.9\n",
      "Episode: 1160/5000, num_steps: 272, total_reward: -244.3\n",
      "\n",
      "======== new gamma: 0.42249707 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 32.3901\n",
      "---------------------------\n",
      "Episode: 1161/5000, num_steps: 339, total_reward: -191.9\n",
      "Episode: 1162/5000, num_steps: 302, total_reward: -216.1\n",
      "Episode: 1163/5000, num_steps: 258, total_reward: -239.1\n",
      "Episode: 1164/5000, num_steps: 265, total_reward: -47.46\n",
      "Episode: 1165/5000, num_steps: 289, total_reward: -28.99\n",
      "Episode: 1166/5000, num_steps: 308, total_reward: -123.7\n",
      "Episode: 1167/5000, num_steps: 169, total_reward: -172.8\n",
      "Episode: 1168/5000, num_steps: 315, total_reward: 6.661\n",
      "Episode: 1169/5000, num_steps: 430, total_reward: -227.9\n",
      "Episode: 1170/5000, num_steps: 243, total_reward: -79.25\n",
      "\n",
      "======== new gamma: 0.42777828 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -148.564\n",
      "---------------------------\n",
      "Episode: 1171/5000, num_steps: 257, total_reward: -222.7\n",
      "Episode: 1172/5000, num_steps: 94, total_reward: -24.31\n",
      "Episode: 1173/5000, num_steps: 358, total_reward: -131.5\n",
      "Episode: 1174/5000, num_steps: 314, total_reward: -197.1\n",
      "Episode: 1175/5000, num_steps: 1000, total_reward: 84.3\n",
      "Episode: 1176/5000, num_steps: 542, total_reward: -348.3\n",
      "Episode: 1177/5000, num_steps: 291, total_reward: -202.1\n",
      "Episode: 1178/5000, num_steps: 372, total_reward: -203.1\n",
      "Episode: 1179/5000, num_steps: 549, total_reward: 258.0\n",
      "Episode: 1180/5000, num_steps: 480, total_reward: 231.1\n",
      "\n",
      "======== new gamma: 0.43312551 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -106.608\n",
      "---------------------------\n",
      "Episode: 1181/5000, num_steps: 281, total_reward: -186.2\n",
      "Episode: 1182/5000, num_steps: 239, total_reward: -50.9\n",
      "Episode: 1183/5000, num_steps: 526, total_reward: 206.1\n",
      "Episode: 1184/5000, num_steps: 225, total_reward: 12.19\n",
      "Episode: 1185/5000, num_steps: 326, total_reward: -16.46\n",
      "Episode: 1186/5000, num_steps: 240, total_reward: -184.6\n",
      "Episode: 1187/5000, num_steps: 234, total_reward: -33.18\n",
      "Episode: 1188/5000, num_steps: 311, total_reward: -83.39\n",
      "Episode: 1189/5000, num_steps: 232, total_reward: -208.2\n",
      "Episode: 1190/5000, num_steps: 372, total_reward: -71.04\n",
      "\n",
      "======== new gamma: 0.43853958 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -31.3498\n",
      "---------------------------\n",
      "Episode: 1191/5000, num_steps: 346, total_reward: 294.1\n",
      "Episode: 1192/5000, num_steps: 534, total_reward: 215.4\n",
      "Episode: 1193/5000, num_steps: 459, total_reward: 245.0\n",
      "Episode: 1194/5000, num_steps: 246, total_reward: -6.307\n",
      "Episode: 1195/5000, num_steps: 277, total_reward: -231.1\n",
      "Episode: 1196/5000, num_steps: 291, total_reward: -52.3\n",
      "Episode: 1197/5000, num_steps: 263, total_reward: 299.3\n",
      "Episode: 1198/5000, num_steps: 297, total_reward: -214.1\n",
      "Episode: 1199/5000, num_steps: 441, total_reward: -189.0\n",
      "Episode: 1200/5000, num_steps: 301, total_reward: -7.419\n",
      "\n",
      "======== new gamma: 0.44402132 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 28.9896\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1201/5000, num_steps: 244, total_reward: -195.2\n",
      "Episode: 1202/5000, num_steps: 1000, total_reward: 167.8\n",
      "Episode: 1203/5000, num_steps: 224, total_reward: -198.2\n",
      "Episode: 1204/5000, num_steps: 195, total_reward: -166.1\n",
      "Episode: 1205/5000, num_steps: 479, total_reward: 265.1\n",
      "Episode: 1206/5000, num_steps: 120, total_reward: -109.0\n",
      "Episode: 1207/5000, num_steps: 1000, total_reward: 87.91\n",
      "Episode: 1208/5000, num_steps: 161, total_reward: -167.0\n",
      "Episode: 1209/5000, num_steps: 282, total_reward: -233.0\n",
      "Episode: 1210/5000, num_steps: 711, total_reward: 185.0\n",
      "\n",
      "======== new gamma: 0.44957159 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -55.4983\n",
      "---------------------------\n",
      "Episode: 1211/5000, num_steps: 394, total_reward: -230.3\n",
      "Episode: 1212/5000, num_steps: 177, total_reward: -215.0\n",
      "Episode: 1213/5000, num_steps: 161, total_reward: -575.7\n",
      "Episode: 1214/5000, num_steps: 281, total_reward: -328.6\n",
      "Episode: 1215/5000, num_steps: 312, total_reward: -294.7\n",
      "Episode: 1216/5000, num_steps: 369, total_reward: -309.0\n",
      "Episode: 1217/5000, num_steps: 239, total_reward: -32.49\n",
      "Episode: 1218/5000, num_steps: 379, total_reward: 235.0\n",
      "Episode: 1219/5000, num_steps: 241, total_reward: -27.45\n",
      "Episode: 1220/5000, num_steps: 454, total_reward: 215.1\n",
      "\n",
      "======== new gamma: 0.45519123 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -159.32\n",
      "---------------------------\n",
      "Episode: 1221/5000, num_steps: 323, total_reward: -271.1\n",
      "Episode: 1222/5000, num_steps: 371, total_reward: -261.5\n",
      "Episode: 1223/5000, num_steps: 383, total_reward: -45.21\n",
      "Episode: 1224/5000, num_steps: 932, total_reward: 103.2\n",
      "Episode: 1225/5000, num_steps: 177, total_reward: -507.1\n",
      "Episode: 1226/5000, num_steps: 381, total_reward: -201.9\n",
      "Episode: 1227/5000, num_steps: 246, total_reward: -177.3\n",
      "Episode: 1228/5000, num_steps: 222, total_reward: -182.1\n",
      "Episode: 1229/5000, num_steps: 376, total_reward: -74.95\n",
      "Episode: 1230/5000, num_steps: 634, total_reward: 248.4\n",
      "\n",
      "======== new gamma: 0.46088112 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -140.283\n",
      "---------------------------\n",
      "Episode: 1231/5000, num_steps: 215, total_reward: -178.8\n",
      "Episode: 1232/5000, num_steps: 383, total_reward: -35.88\n",
      "Episode: 1233/5000, num_steps: 368, total_reward: -218.6\n",
      "Episode: 1234/5000, num_steps: 256, total_reward: -31.4\n",
      "Episode: 1235/5000, num_steps: 486, total_reward: -74.37\n",
      "Episode: 1236/5000, num_steps: 1000, total_reward: 84.62\n",
      "Episode: 1237/5000, num_steps: 349, total_reward: -108.3\n",
      "Episode: 1238/5000, num_steps: 699, total_reward: 247.1\n",
      "Episode: 1239/5000, num_steps: 286, total_reward: -224.2\n",
      "Episode: 1240/5000, num_steps: 693, total_reward: -322.1\n",
      "\n",
      "======== new gamma: 0.46664214 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -29.1407\n",
      "---------------------------\n",
      "Episode: 1241/5000, num_steps: 333, total_reward: -47.22\n",
      "Episode: 1242/5000, num_steps: 348, total_reward: -25.9\n",
      "Episode: 1243/5000, num_steps: 367, total_reward: -31.42\n",
      "Episode: 1244/5000, num_steps: 355, total_reward: 293.5\n",
      "Episode: 1245/5000, num_steps: 289, total_reward: -15.25\n",
      "Episode: 1246/5000, num_steps: 177, total_reward: -7.552\n",
      "Episode: 1247/5000, num_steps: 302, total_reward: -258.7\n",
      "Episode: 1248/5000, num_steps: 740, total_reward: 220.5\n",
      "Episode: 1249/5000, num_steps: 373, total_reward: -256.9\n",
      "Episode: 1250/5000, num_steps: 1000, total_reward: 33.52\n",
      "\n",
      "======== new gamma: 0.47247517 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -45.1044\n",
      "---------------------------\n",
      "Episode: 1251/5000, num_steps: 368, total_reward: -201.2\n",
      "Episode: 1252/5000, num_steps: 239, total_reward: -183.8\n",
      "Episode: 1253/5000, num_steps: 1000, total_reward: 86.31\n",
      "Episode: 1254/5000, num_steps: 669, total_reward: 212.0\n",
      "Episode: 1255/5000, num_steps: 362, total_reward: -47.3\n",
      "Episode: 1256/5000, num_steps: 320, total_reward: -224.3\n",
      "Episode: 1257/5000, num_steps: 335, total_reward: -257.2\n",
      "Episode: 1258/5000, num_steps: 355, total_reward: 296.5\n",
      "Episode: 1259/5000, num_steps: 258, total_reward: -51.22\n",
      "Episode: 1260/5000, num_steps: 310, total_reward: -88.14\n",
      "\n",
      "======== new gamma: 0.47838111 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -33.6835\n",
      "---------------------------\n",
      "Episode: 1261/5000, num_steps: 292, total_reward: -34.57\n",
      "Episode: 1262/5000, num_steps: 604, total_reward: 252.9\n",
      "Episode: 1263/5000, num_steps: 271, total_reward: -252.6\n",
      "Episode: 1264/5000, num_steps: 274, total_reward: -199.1\n",
      "Episode: 1265/5000, num_steps: 435, total_reward: 267.8\n",
      "Episode: 1266/5000, num_steps: 382, total_reward: -236.7\n",
      "Episode: 1267/5000, num_steps: 180, total_reward: 64.14\n",
      "Episode: 1268/5000, num_steps: 211, total_reward: -191.0\n",
      "Episode: 1269/5000, num_steps: 148, total_reward: -181.8\n",
      "Episode: 1270/5000, num_steps: 1000, total_reward: 80.84\n",
      "\n",
      "======== new gamma: 0.48436087 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -59.9098\n",
      "---------------------------\n",
      "Episode: 1271/5000, num_steps: 342, total_reward: -264.8\n",
      "Episode: 1272/5000, num_steps: 308, total_reward: -225.8\n",
      "Episode: 1273/5000, num_steps: 342, total_reward: -226.9\n",
      "Episode: 1274/5000, num_steps: 742, total_reward: 204.2\n",
      "Episode: 1275/5000, num_steps: 487, total_reward: -252.4\n",
      "Episode: 1276/5000, num_steps: 300, total_reward: -65.25\n",
      "Episode: 1277/5000, num_steps: 451, total_reward: -108.8\n",
      "Episode: 1278/5000, num_steps: 173, total_reward: -51.07\n",
      "Episode: 1279/5000, num_steps: 293, total_reward: -60.32\n",
      "Episode: 1280/5000, num_steps: 270, total_reward: -18.67\n",
      "\n",
      "======== new gamma: 0.49041538 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -97.0281\n",
      "---------------------------\n",
      "Episode: 1281/5000, num_steps: 224, total_reward: -81.79\n",
      "Episode: 1282/5000, num_steps: 492, total_reward: 5.427\n",
      "Episode: 1283/5000, num_steps: 378, total_reward: 256.4\n",
      "Episode: 1284/5000, num_steps: 346, total_reward: -247.4\n",
      "Episode: 1285/5000, num_steps: 181, total_reward: 12.97\n",
      "Episode: 1286/5000, num_steps: 361, total_reward: -7.086\n",
      "Episode: 1287/5000, num_steps: 215, total_reward: -186.6\n",
      "Episode: 1288/5000, num_steps: 693, total_reward: 182.6\n",
      "Episode: 1289/5000, num_steps: 342, total_reward: -44.72\n",
      "Episode: 1290/5000, num_steps: 336, total_reward: -195.9\n",
      "\n",
      "======== new gamma: 0.49654557 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -12.8897\n",
      "---------------------------\n",
      "Episode: 1291/5000, num_steps: 230, total_reward: -189.4\n",
      "Episode: 1292/5000, num_steps: 696, total_reward: 171.5\n",
      "Episode: 1293/5000, num_steps: 322, total_reward: -20.42\n",
      "Episode: 1294/5000, num_steps: 206, total_reward: 23.12\n",
      "Episode: 1295/5000, num_steps: 459, total_reward: 259.5\n",
      "Episode: 1296/5000, num_steps: 631, total_reward: 217.3\n",
      "Episode: 1297/5000, num_steps: 299, total_reward: -223.4\n",
      "Episode: 1298/5000, num_steps: 309, total_reward: -27.7\n",
      "Episode: 1299/5000, num_steps: 559, total_reward: 209.9\n",
      "Episode: 1300/5000, num_steps: 201, total_reward: -36.8\n",
      "\n",
      "======== new gamma: 0.50275239 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 22.4623\n",
      "---------------------------\n",
      "Episode: 1301/5000, num_steps: 400, total_reward: 281.0\n",
      "Episode: 1302/5000, num_steps: 184, total_reward: 66.3\n",
      "Episode: 1303/5000, num_steps: 496, total_reward: -68.13\n",
      "Episode: 1304/5000, num_steps: 436, total_reward: -69.86\n",
      "Episode: 1305/5000, num_steps: 296, total_reward: 252.0\n",
      "Episode: 1306/5000, num_steps: 1000, total_reward: 60.59\n",
      "Episode: 1307/5000, num_steps: 177, total_reward: 34.23\n",
      "Episode: 1308/5000, num_steps: 294, total_reward: -32.88\n",
      "Episode: 1309/5000, num_steps: 218, total_reward: -212.4\n",
      "Episode: 1310/5000, num_steps: 93, total_reward: -49.09\n",
      "\n",
      "======== new gamma: 0.5090368 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 27.3999\n",
      "---------------------------\n",
      "Episode: 1311/5000, num_steps: 223, total_reward: -438.4\n",
      "Episode: 1312/5000, num_steps: 457, total_reward: -326.4\n",
      "Episode: 1313/5000, num_steps: 297, total_reward: -254.2\n",
      "Episode: 1314/5000, num_steps: 330, total_reward: -223.9\n",
      "Episode: 1315/5000, num_steps: 402, total_reward: 214.7\n",
      "Episode: 1316/5000, num_steps: 454, total_reward: -307.1\n",
      "Episode: 1317/5000, num_steps: 485, total_reward: 159.7\n",
      "Episode: 1318/5000, num_steps: 235, total_reward: -201.3\n",
      "Episode: 1319/5000, num_steps: 628, total_reward: 105.7\n",
      "Episode: 1320/5000, num_steps: 709, total_reward: 274.5\n",
      "\n",
      "======== new gamma: 0.51539976 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -132.018\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1321/5000, num_steps: 329, total_reward: -165.8\n",
      "Episode: 1322/5000, num_steps: 326, total_reward: -240.2\n",
      "Episode: 1323/5000, num_steps: 312, total_reward: -240.1\n",
      "Episode: 1324/5000, num_steps: 169, total_reward: -159.0\n",
      "Episode: 1325/5000, num_steps: 351, total_reward: -105.4\n",
      "Episode: 1326/5000, num_steps: 247, total_reward: 13.18\n",
      "Episode: 1327/5000, num_steps: 246, total_reward: 7.435\n",
      "Episode: 1328/5000, num_steps: 269, total_reward: -44.35\n",
      "Episode: 1329/5000, num_steps: 317, total_reward: -55.9\n",
      "Episode: 1330/5000, num_steps: 225, total_reward: -196.0\n",
      "\n",
      "======== new gamma: 0.52184225 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -71.5767\n",
      "---------------------------\n",
      "Episode: 1331/5000, num_steps: 171, total_reward: -38.14\n",
      "Episode: 1332/5000, num_steps: 289, total_reward: -62.21\n",
      "Episode: 1333/5000, num_steps: 239, total_reward: -29.19\n",
      "Episode: 1334/5000, num_steps: 360, total_reward: -205.6\n",
      "Episode: 1335/5000, num_steps: 287, total_reward: 27.07\n",
      "Episode: 1336/5000, num_steps: 321, total_reward: 12.42\n",
      "Episode: 1337/5000, num_steps: 896, total_reward: 156.1\n",
      "Episode: 1338/5000, num_steps: 249, total_reward: -60.32\n",
      "Episode: 1339/5000, num_steps: 302, total_reward: -210.5\n",
      "Episode: 1340/5000, num_steps: 226, total_reward: -167.4\n",
      "\n",
      "======== new gamma: 0.52836528 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -60.6301\n",
      "---------------------------\n",
      "Episode: 1341/5000, num_steps: 184, total_reward: -38.99\n",
      "Episode: 1342/5000, num_steps: 453, total_reward: -84.78\n",
      "Episode: 1343/5000, num_steps: 598, total_reward: 262.9\n",
      "Episode: 1344/5000, num_steps: 287, total_reward: -22.95\n",
      "Episode: 1345/5000, num_steps: 194, total_reward: 51.14\n",
      "Episode: 1346/5000, num_steps: 259, total_reward: -166.9\n",
      "Episode: 1347/5000, num_steps: 360, total_reward: -232.3\n",
      "Episode: 1348/5000, num_steps: 365, total_reward: -219.4\n",
      "Episode: 1349/5000, num_steps: 1000, total_reward: 107.2\n",
      "Episode: 1350/5000, num_steps: 310, total_reward: -188.7\n",
      "\n",
      "======== new gamma: 0.53496985 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -51.1583\n",
      "---------------------------\n",
      "Episode: 1351/5000, num_steps: 367, total_reward: -230.2\n",
      "Episode: 1352/5000, num_steps: 232, total_reward: -169.1\n",
      "Episode: 1353/5000, num_steps: 247, total_reward: -20.59\n",
      "Episode: 1354/5000, num_steps: 227, total_reward: -198.3\n",
      "Episode: 1355/5000, num_steps: 329, total_reward: -8.93\n",
      "Episode: 1356/5000, num_steps: 234, total_reward: -10.92\n",
      "Episode: 1357/5000, num_steps: 338, total_reward: -32.75\n",
      "Episode: 1358/5000, num_steps: 238, total_reward: -185.8\n",
      "Episode: 1359/5000, num_steps: 263, total_reward: -218.1\n",
      "Episode: 1360/5000, num_steps: 345, total_reward: -215.5\n",
      "\n",
      "======== new gamma: 0.54165697 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -126.338\n",
      "---------------------------\n",
      "Episode: 1361/5000, num_steps: 285, total_reward: -63.06\n",
      "Episode: 1362/5000, num_steps: 180, total_reward: 14.17\n",
      "Episode: 1363/5000, num_steps: 176, total_reward: 24.54\n",
      "Episode: 1364/5000, num_steps: 275, total_reward: -15.61\n",
      "Episode: 1365/5000, num_steps: 280, total_reward: -227.6\n",
      "Episode: 1366/5000, num_steps: 238, total_reward: 16.52\n",
      "Episode: 1367/5000, num_steps: 280, total_reward: -7.965\n",
      "Episode: 1368/5000, num_steps: 281, total_reward: -198.3\n",
      "Episode: 1369/5000, num_steps: 200, total_reward: 28.7\n",
      "Episode: 1370/5000, num_steps: 90, total_reward: 31.14\n",
      "\n",
      "======== new gamma: 0.54842768 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -64.4127\n",
      "---------------------------\n",
      "Episode: 1371/5000, num_steps: 283, total_reward: -17.52\n",
      "Episode: 1372/5000, num_steps: 246, total_reward: -39.04\n",
      "Episode: 1373/5000, num_steps: 532, total_reward: 184.7\n",
      "Episode: 1374/5000, num_steps: 263, total_reward: 9.775\n",
      "Episode: 1375/5000, num_steps: 165, total_reward: 29.47\n",
      "Episode: 1376/5000, num_steps: 353, total_reward: -135.3\n",
      "Episode: 1377/5000, num_steps: 214, total_reward: -163.9\n",
      "Episode: 1378/5000, num_steps: 514, total_reward: 190.2\n",
      "Episode: 1379/5000, num_steps: 212, total_reward: -202.6\n",
      "Episode: 1380/5000, num_steps: 232, total_reward: -320.0\n",
      "\n",
      "======== new gamma: 0.55528303 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -11.2936\n",
      "---------------------------\n",
      "Episode: 1381/5000, num_steps: 101, total_reward: 27.58\n",
      "Episode: 1382/5000, num_steps: 318, total_reward: -76.95\n",
      "Episode: 1383/5000, num_steps: 700, total_reward: 214.5\n",
      "Episode: 1384/5000, num_steps: 360, total_reward: -229.2\n",
      "Episode: 1385/5000, num_steps: 248, total_reward: 13.46\n",
      "Episode: 1386/5000, num_steps: 168, total_reward: -16.12\n",
      "Episode: 1387/5000, num_steps: 715, total_reward: 126.3\n",
      "Episode: 1388/5000, num_steps: 1000, total_reward: 99.62\n",
      "Episode: 1389/5000, num_steps: 696, total_reward: 197.0\n",
      "Episode: 1390/5000, num_steps: 1000, total_reward: 135.1\n",
      "\n",
      "======== new gamma: 0.56222407 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 3.62094\n",
      "---------------------------\n",
      "Episode: 1391/5000, num_steps: 449, total_reward: -263.1\n",
      "Episode: 1392/5000, num_steps: 244, total_reward: -20.6\n",
      "Episode: 1393/5000, num_steps: 233, total_reward: -149.8\n",
      "Episode: 1394/5000, num_steps: 297, total_reward: -43.45\n",
      "Episode: 1395/5000, num_steps: 724, total_reward: 208.9\n",
      "Episode: 1396/5000, num_steps: 189, total_reward: -27.18\n",
      "Episode: 1397/5000, num_steps: 461, total_reward: -114.7\n",
      "Episode: 1398/5000, num_steps: 151, total_reward: -182.3\n",
      "Episode: 1399/5000, num_steps: 430, total_reward: 266.8\n",
      "Episode: 1400/5000, num_steps: 209, total_reward: -181.6\n",
      "\n",
      "======== new gamma: 0.56925187 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -19.0243\n",
      "---------------------------\n",
      "Episode: 1401/5000, num_steps: 243, total_reward: -62.58\n",
      "Episode: 1402/5000, num_steps: 255, total_reward: 297.7\n",
      "Episode: 1403/5000, num_steps: 364, total_reward: -61.95\n",
      "Episode: 1404/5000, num_steps: 249, total_reward: -28.53\n",
      "Episode: 1405/5000, num_steps: 476, total_reward: 184.0\n",
      "Episode: 1406/5000, num_steps: 254, total_reward: -7.775\n",
      "Episode: 1407/5000, num_steps: 558, total_reward: -84.0\n",
      "Episode: 1408/5000, num_steps: 318, total_reward: -87.26\n",
      "Episode: 1409/5000, num_steps: 287, total_reward: -212.0\n",
      "Episode: 1410/5000, num_steps: 345, total_reward: -241.7\n",
      "\n",
      "======== new gamma: 0.57636752 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -24.404\n",
      "---------------------------\n",
      "Episode: 1411/5000, num_steps: 611, total_reward: 212.7\n",
      "Episode: 1412/5000, num_steps: 394, total_reward: -209.7\n",
      "Episode: 1413/5000, num_steps: 219, total_reward: -179.3\n",
      "Episode: 1414/5000, num_steps: 261, total_reward: -30.93\n",
      "Episode: 1415/5000, num_steps: 240, total_reward: -226.7\n",
      "Episode: 1416/5000, num_steps: 247, total_reward: -223.3\n",
      "Episode: 1417/5000, num_steps: 181, total_reward: 46.08\n",
      "Episode: 1418/5000, num_steps: 360, total_reward: -31.63\n",
      "Episode: 1419/5000, num_steps: 227, total_reward: -20.56\n",
      "Episode: 1420/5000, num_steps: 291, total_reward: -24.54\n",
      "\n",
      "======== new gamma: 0.58357211 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -90.5092\n",
      "---------------------------\n",
      "Episode: 1421/5000, num_steps: 418, total_reward: 246.9\n",
      "Episode: 1422/5000, num_steps: 106, total_reward: 18.22\n",
      "Episode: 1423/5000, num_steps: 273, total_reward: -234.4\n",
      "Episode: 1424/5000, num_steps: 217, total_reward: -181.1\n",
      "Episode: 1425/5000, num_steps: 166, total_reward: -11.83\n",
      "Episode: 1426/5000, num_steps: 569, total_reward: 235.7\n",
      "Episode: 1427/5000, num_steps: 352, total_reward: -36.33\n",
      "Episode: 1428/5000, num_steps: 602, total_reward: 255.6\n",
      "Episode: 1429/5000, num_steps: 414, total_reward: 241.6\n",
      "Episode: 1430/5000, num_steps: 160, total_reward: 11.21\n",
      "\n",
      "======== new gamma: 0.59086676 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 50.9752\n",
      "---------------------------\n",
      "Episode: 1431/5000, num_steps: 329, total_reward: -20.65\n",
      "Episode: 1432/5000, num_steps: 240, total_reward: -29.17\n",
      "Episode: 1433/5000, num_steps: 370, total_reward: -30.7\n",
      "Episode: 1434/5000, num_steps: 329, total_reward: 250.8\n",
      "Episode: 1435/5000, num_steps: 577, total_reward: 169.9\n",
      "Episode: 1436/5000, num_steps: 816, total_reward: 227.4\n",
      "Episode: 1437/5000, num_steps: 722, total_reward: 171.9\n",
      "Episode: 1438/5000, num_steps: 221, total_reward: -244.6\n",
      "Episode: 1439/5000, num_steps: 160, total_reward: 39.62\n",
      "Episode: 1440/5000, num_steps: 238, total_reward: -21.28\n",
      "\n",
      "======== new gamma: 0.5982526 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 54.5773\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1441/5000, num_steps: 301, total_reward: -86.2\n",
      "Episode: 1442/5000, num_steps: 395, total_reward: -90.33\n",
      "Episode: 1443/5000, num_steps: 226, total_reward: -177.1\n",
      "Episode: 1444/5000, num_steps: 175, total_reward: -206.9\n",
      "Episode: 1445/5000, num_steps: 259, total_reward: -185.1\n",
      "Episode: 1446/5000, num_steps: 330, total_reward: 256.7\n",
      "Episode: 1447/5000, num_steps: 243, total_reward: -47.62\n",
      "Episode: 1448/5000, num_steps: 282, total_reward: -243.2\n",
      "Episode: 1449/5000, num_steps: 252, total_reward: -30.28\n",
      "Episode: 1450/5000, num_steps: 299, total_reward: -8.068\n",
      "\n",
      "======== new gamma: 0.60573075 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -83.129\n",
      "---------------------------\n",
      "Episode: 1451/5000, num_steps: 239, total_reward: -103.4\n",
      "Episode: 1452/5000, num_steps: 482, total_reward: 249.7\n",
      "Episode: 1453/5000, num_steps: 322, total_reward: -34.99\n",
      "Episode: 1454/5000, num_steps: 278, total_reward: -50.1\n",
      "Episode: 1455/5000, num_steps: 240, total_reward: -28.94\n",
      "Episode: 1456/5000, num_steps: 350, total_reward: 14.95\n",
      "Episode: 1457/5000, num_steps: 321, total_reward: -77.99\n",
      "Episode: 1458/5000, num_steps: 538, total_reward: 215.0\n",
      "Episode: 1459/5000, num_steps: 466, total_reward: 207.0\n",
      "Episode: 1460/5000, num_steps: 449, total_reward: -233.1\n",
      "\n",
      "======== new gamma: 0.61330239 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 38.3051\n",
      "---------------------------\n",
      "Episode: 1461/5000, num_steps: 528, total_reward: 188.2\n",
      "Episode: 1462/5000, num_steps: 262, total_reward: -183.9\n",
      "Episode: 1463/5000, num_steps: 1000, total_reward: 125.5\n",
      "Episode: 1464/5000, num_steps: 543, total_reward: 240.6\n",
      "Episode: 1465/5000, num_steps: 401, total_reward: -294.5\n",
      "Episode: 1466/5000, num_steps: 298, total_reward: -37.23\n",
      "Episode: 1467/5000, num_steps: 614, total_reward: 179.7\n",
      "Episode: 1468/5000, num_steps: 397, total_reward: -100.7\n",
      "Episode: 1469/5000, num_steps: 358, total_reward: -17.04\n",
      "Episode: 1470/5000, num_steps: 328, total_reward: -51.77\n",
      "\n",
      "======== new gamma: 0.62096867 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -13.259\n",
      "---------------------------\n",
      "Episode: 1471/5000, num_steps: 333, total_reward: -31.64\n",
      "Episode: 1472/5000, num_steps: 297, total_reward: -164.6\n",
      "Episode: 1473/5000, num_steps: 348, total_reward: 281.9\n",
      "Episode: 1474/5000, num_steps: 167, total_reward: 26.02\n",
      "Episode: 1475/5000, num_steps: 374, total_reward: -72.53\n",
      "Episode: 1476/5000, num_steps: 435, total_reward: -216.2\n",
      "Episode: 1477/5000, num_steps: 158, total_reward: -130.3\n",
      "Episode: 1478/5000, num_steps: 362, total_reward: 305.0\n",
      "Episode: 1479/5000, num_steps: 218, total_reward: -155.9\n",
      "Episode: 1480/5000, num_steps: 322, total_reward: -11.77\n",
      "\n",
      "======== new gamma: 0.62873078 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -20.9924\n",
      "---------------------------\n",
      "Episode: 1481/5000, num_steps: 247, total_reward: -32.35\n",
      "Episode: 1482/5000, num_steps: 286, total_reward: -71.36\n",
      "Episode: 1483/5000, num_steps: 394, total_reward: -78.48\n",
      "Episode: 1484/5000, num_steps: 332, total_reward: 22.77\n",
      "Episode: 1485/5000, num_steps: 1000, total_reward: 84.14\n",
      "Episode: 1486/5000, num_steps: 229, total_reward: -6.248\n",
      "Episode: 1487/5000, num_steps: 534, total_reward: 283.6\n",
      "Episode: 1488/5000, num_steps: 294, total_reward: -193.5\n",
      "Episode: 1489/5000, num_steps: 280, total_reward: -60.4\n",
      "Episode: 1490/5000, num_steps: 347, total_reward: -202.3\n",
      "\n",
      "======== new gamma: 0.63658991 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -6.35867\n",
      "---------------------------\n",
      "Episode: 1491/5000, num_steps: 268, total_reward: -28.81\n",
      "Episode: 1492/5000, num_steps: 209, total_reward: -209.4\n",
      "Episode: 1493/5000, num_steps: 304, total_reward: -215.8\n",
      "Episode: 1494/5000, num_steps: 347, total_reward: -229.1\n",
      "Episode: 1495/5000, num_steps: 243, total_reward: -12.26\n",
      "Episode: 1496/5000, num_steps: 232, total_reward: -14.33\n",
      "Episode: 1497/5000, num_steps: 364, total_reward: -187.0\n",
      "Episode: 1498/5000, num_steps: 729, total_reward: 295.3\n",
      "Episode: 1499/5000, num_steps: 217, total_reward: -9.294\n",
      "Episode: 1500/5000, num_steps: 226, total_reward: -29.33\n",
      "\n",
      "======== new gamma: 0.64454728 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -81.3043\n",
      "---------------------------\n",
      "Episode: 1501/5000, num_steps: 388, total_reward: 187.3\n",
      "Episode: 1502/5000, num_steps: 284, total_reward: -297.8\n",
      "Episode: 1503/5000, num_steps: 191, total_reward: 48.45\n",
      "Episode: 1504/5000, num_steps: 255, total_reward: -13.95\n",
      "Episode: 1505/5000, num_steps: 150, total_reward: -171.8\n",
      "Episode: 1506/5000, num_steps: 301, total_reward: -1.784\n",
      "Episode: 1507/5000, num_steps: 625, total_reward: 232.1\n",
      "Episode: 1508/5000, num_steps: 345, total_reward: -208.9\n",
      "Episode: 1509/5000, num_steps: 317, total_reward: -11.39\n",
      "Episode: 1510/5000, num_steps: 452, total_reward: 179.1\n",
      "\n",
      "======== new gamma: 0.65260413 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -26.7112\n",
      "---------------------------\n",
      "Episode: 1511/5000, num_steps: 244, total_reward: -32.78\n",
      "Episode: 1512/5000, num_steps: 307, total_reward: -116.3\n",
      "Episode: 1513/5000, num_steps: 376, total_reward: 237.9\n",
      "Episode: 1514/5000, num_steps: 150, total_reward: -115.0\n",
      "Episode: 1515/5000, num_steps: 481, total_reward: -256.9\n",
      "Episode: 1516/5000, num_steps: 370, total_reward: -213.7\n",
      "Episode: 1517/5000, num_steps: 793, total_reward: 267.3\n",
      "Episode: 1518/5000, num_steps: 225, total_reward: -32.34\n",
      "Episode: 1519/5000, num_steps: 251, total_reward: 17.54\n",
      "Episode: 1520/5000, num_steps: 572, total_reward: 230.2\n",
      "\n",
      "======== new gamma: 0.66076168 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -6.51185\n",
      "---------------------------\n",
      "Episode: 1521/5000, num_steps: 1000, total_reward: 72.52\n",
      "Episode: 1522/5000, num_steps: 173, total_reward: -11.19\n",
      "Episode: 1523/5000, num_steps: 239, total_reward: -183.3\n",
      "Episode: 1524/5000, num_steps: 259, total_reward: -2.032\n",
      "Episode: 1525/5000, num_steps: 345, total_reward: 215.7\n",
      "Episode: 1526/5000, num_steps: 236, total_reward: 18.37\n",
      "Episode: 1527/5000, num_steps: 194, total_reward: -333.4\n",
      "Episode: 1528/5000, num_steps: 97, total_reward: -395.7\n",
      "Episode: 1529/5000, num_steps: 145, total_reward: -193.5\n",
      "Episode: 1530/5000, num_steps: 265, total_reward: -224.8\n",
      "\n",
      "======== new gamma: 0.6690212 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -58.2333\n",
      "---------------------------\n",
      "Episode: 1531/5000, num_steps: 261, total_reward: -374.5\n",
      "Episode: 1532/5000, num_steps: 431, total_reward: -104.7\n",
      "Episode: 1533/5000, num_steps: 572, total_reward: 210.8\n",
      "Episode: 1534/5000, num_steps: 355, total_reward: -17.03\n",
      "Episode: 1535/5000, num_steps: 292, total_reward: -41.77\n",
      "Episode: 1536/5000, num_steps: 177, total_reward: -22.63\n",
      "Episode: 1537/5000, num_steps: 195, total_reward: 76.71\n",
      "Episode: 1538/5000, num_steps: 224, total_reward: -60.49\n",
      "Episode: 1539/5000, num_steps: 335, total_reward: -253.1\n",
      "Episode: 1540/5000, num_steps: 351, total_reward: 249.2\n",
      "\n",
      "======== new gamma: 0.67738396 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -81.156\n",
      "---------------------------\n",
      "Episode: 1541/5000, num_steps: 228, total_reward: -11.23\n",
      "Episode: 1542/5000, num_steps: 218, total_reward: -42.57\n",
      "Episode: 1543/5000, num_steps: 295, total_reward: -196.3\n",
      "Episode: 1544/5000, num_steps: 307, total_reward: -62.42\n",
      "Episode: 1545/5000, num_steps: 369, total_reward: -77.16\n",
      "Episode: 1546/5000, num_steps: 366, total_reward: -185.7\n",
      "Episode: 1547/5000, num_steps: 162, total_reward: -19.87\n",
      "Episode: 1548/5000, num_steps: 369, total_reward: 240.4\n",
      "Episode: 1549/5000, num_steps: 288, total_reward: -14.91\n",
      "Episode: 1550/5000, num_steps: 233, total_reward: -16.93\n",
      "\n",
      "======== new gamma: 0.68585126 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -12.0697\n",
      "---------------------------\n",
      "Episode: 1551/5000, num_steps: 311, total_reward: -211.6\n",
      "Episode: 1552/5000, num_steps: 341, total_reward: -203.8\n",
      "Episode: 1553/5000, num_steps: 529, total_reward: -261.9\n",
      "Episode: 1554/5000, num_steps: 255, total_reward: 13.57\n",
      "Episode: 1555/5000, num_steps: 363, total_reward: 256.5\n",
      "Episode: 1556/5000, num_steps: 282, total_reward: -210.4\n",
      "Episode: 1557/5000, num_steps: 465, total_reward: 155.2\n",
      "Episode: 1558/5000, num_steps: 143, total_reward: -160.5\n",
      "Episode: 1559/5000, num_steps: 431, total_reward: -247.2\n",
      "Episode: 1560/5000, num_steps: 170, total_reward: -4.731\n",
      "\n",
      "======== new gamma: 0.6944244 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -88.7029\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1561/5000, num_steps: 355, total_reward: -31.47\n",
      "Episode: 1562/5000, num_steps: 243, total_reward: 17.22\n",
      "Episode: 1563/5000, num_steps: 446, total_reward: 238.9\n",
      "Episode: 1564/5000, num_steps: 230, total_reward: -26.67\n",
      "Episode: 1565/5000, num_steps: 854, total_reward: 245.2\n",
      "Episode: 1566/5000, num_steps: 304, total_reward: -62.23\n",
      "Episode: 1567/5000, num_steps: 414, total_reward: 2.68\n",
      "Episode: 1568/5000, num_steps: 154, total_reward: -160.8\n",
      "Episode: 1569/5000, num_steps: 387, total_reward: -19.87\n",
      "Episode: 1570/5000, num_steps: 231, total_reward: -22.71\n",
      "\n",
      "======== new gamma: 0.70310471 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 19.829\n",
      "---------------------------\n",
      "Episode: 1571/5000, num_steps: 370, total_reward: 296.1\n",
      "Episode: 1572/5000, num_steps: 281, total_reward: -41.5\n",
      "Episode: 1573/5000, num_steps: 665, total_reward: 256.0\n",
      "Episode: 1574/5000, num_steps: 289, total_reward: -232.4\n",
      "Episode: 1575/5000, num_steps: 458, total_reward: 187.6\n",
      "Episode: 1576/5000, num_steps: 133, total_reward: -277.6\n",
      "Episode: 1577/5000, num_steps: 184, total_reward: 45.96\n",
      "Episode: 1578/5000, num_steps: 222, total_reward: -31.25\n",
      "Episode: 1579/5000, num_steps: 183, total_reward: -163.0\n",
      "Episode: 1580/5000, num_steps: 169, total_reward: -56.44\n",
      "\n",
      "======== new gamma: 0.71189352 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 1.73368\n",
      "---------------------------\n",
      "Episode: 1581/5000, num_steps: 156, total_reward: 30.95\n",
      "Episode: 1582/5000, num_steps: 99, total_reward: 32.97\n",
      "Episode: 1583/5000, num_steps: 334, total_reward: -191.0\n",
      "Episode: 1584/5000, num_steps: 236, total_reward: -24.06\n",
      "Episode: 1585/5000, num_steps: 244, total_reward: -46.64\n",
      "Episode: 1586/5000, num_steps: 269, total_reward: -184.9\n",
      "Episode: 1587/5000, num_steps: 238, total_reward: -23.4\n",
      "Episode: 1588/5000, num_steps: 276, total_reward: -216.8\n",
      "Episode: 1589/5000, num_steps: 238, total_reward: -216.1\n",
      "Episode: 1590/5000, num_steps: 400, total_reward: -219.1\n",
      "\n",
      "======== new gamma: 0.72079219 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -89.5443\n",
      "---------------------------\n",
      "Episode: 1591/5000, num_steps: 343, total_reward: -221.2\n",
      "Episode: 1592/5000, num_steps: 162, total_reward: -3.97\n",
      "Episode: 1593/5000, num_steps: 304, total_reward: -181.3\n",
      "Episode: 1594/5000, num_steps: 349, total_reward: 267.7\n",
      "Episode: 1595/5000, num_steps: 387, total_reward: -197.1\n",
      "Episode: 1596/5000, num_steps: 312, total_reward: -9.638\n",
      "Episode: 1597/5000, num_steps: 362, total_reward: 255.7\n",
      "Episode: 1598/5000, num_steps: 284, total_reward: -289.3\n",
      "Episode: 1599/5000, num_steps: 299, total_reward: -36.35\n",
      "Episode: 1600/5000, num_steps: 265, total_reward: -17.56\n",
      "\n",
      "======== new gamma: 0.72980209 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -63.4527\n",
      "---------------------------\n",
      "Episode: 1601/5000, num_steps: 327, total_reward: 8.972\n",
      "Episode: 1602/5000, num_steps: 405, total_reward: -65.77\n",
      "Episode: 1603/5000, num_steps: 790, total_reward: 288.6\n",
      "Episode: 1604/5000, num_steps: 308, total_reward: -16.18\n",
      "Episode: 1605/5000, num_steps: 183, total_reward: -139.6\n",
      "Episode: 1606/5000, num_steps: 341, total_reward: -100.5\n",
      "Episode: 1607/5000, num_steps: 305, total_reward: -22.72\n",
      "Episode: 1608/5000, num_steps: 514, total_reward: 200.7\n",
      "Episode: 1609/5000, num_steps: 264, total_reward: -239.5\n",
      "Episode: 1610/5000, num_steps: 508, total_reward: 240.1\n",
      "\n",
      "======== new gamma: 0.73892461 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -10.3567\n",
      "---------------------------\n",
      "Episode: 1611/5000, num_steps: 166, total_reward: 1.684\n",
      "Episode: 1612/5000, num_steps: 380, total_reward: -252.7\n",
      "Episode: 1613/5000, num_steps: 229, total_reward: -156.7\n",
      "Episode: 1614/5000, num_steps: 210, total_reward: -60.65\n",
      "Episode: 1615/5000, num_steps: 319, total_reward: -213.0\n",
      "Episode: 1616/5000, num_steps: 450, total_reward: -67.05\n",
      "Episode: 1617/5000, num_steps: 212, total_reward: -151.6\n",
      "Episode: 1618/5000, num_steps: 272, total_reward: -204.1\n",
      "Episode: 1619/5000, num_steps: 305, total_reward: -169.8\n",
      "Episode: 1620/5000, num_steps: 292, total_reward: 270.2\n",
      "\n",
      "======== new gamma: 0.74816117 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -103.388\n",
      "---------------------------\n",
      "Episode: 1621/5000, num_steps: 278, total_reward: -264.8\n",
      "Episode: 1622/5000, num_steps: 231, total_reward: -40.29\n",
      "Episode: 1623/5000, num_steps: 386, total_reward: -38.84\n",
      "Episode: 1624/5000, num_steps: 466, total_reward: -81.57\n",
      "Episode: 1625/5000, num_steps: 425, total_reward: 252.5\n",
      "Episode: 1626/5000, num_steps: 359, total_reward: -26.15\n",
      "Episode: 1627/5000, num_steps: 153, total_reward: -123.6\n",
      "Episode: 1628/5000, num_steps: 336, total_reward: -225.0\n",
      "Episode: 1629/5000, num_steps: 379, total_reward: -100.5\n",
      "Episode: 1630/5000, num_steps: 171, total_reward: 0.4874\n",
      "\n",
      "======== new gamma: 0.75751319 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -37.795\n",
      "---------------------------\n",
      "Episode: 1631/5000, num_steps: 548, total_reward: -76.13\n",
      "Episode: 1632/5000, num_steps: 280, total_reward: -22.81\n",
      "Episode: 1633/5000, num_steps: 175, total_reward: 47.61\n",
      "Episode: 1634/5000, num_steps: 498, total_reward: 169.8\n",
      "Episode: 1635/5000, num_steps: 647, total_reward: 245.7\n",
      "Episode: 1636/5000, num_steps: 401, total_reward: 280.3\n",
      "Episode: 1637/5000, num_steps: 326, total_reward: 247.4\n",
      "Episode: 1638/5000, num_steps: 169, total_reward: 6.256\n",
      "Episode: 1639/5000, num_steps: 226, total_reward: -6.446\n",
      "Episode: 1640/5000, num_steps: 335, total_reward: 286.6\n",
      "\n",
      "======== new gamma: 0.7669821 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 89.2131\n",
      "---------------------------\n",
      "Episode: 1641/5000, num_steps: 381, total_reward: 285.4\n",
      "Episode: 1642/5000, num_steps: 157, total_reward: -147.4\n",
      "Episode: 1643/5000, num_steps: 323, total_reward: -25.16\n",
      "Episode: 1644/5000, num_steps: 165, total_reward: 9.575\n",
      "Episode: 1645/5000, num_steps: 387, total_reward: 235.2\n",
      "Episode: 1646/5000, num_steps: 303, total_reward: 1.504\n",
      "Episode: 1647/5000, num_steps: 702, total_reward: 262.2\n",
      "Episode: 1648/5000, num_steps: 227, total_reward: -82.54\n",
      "Episode: 1649/5000, num_steps: 248, total_reward: -25.81\n",
      "Episode: 1650/5000, num_steps: 232, total_reward: -127.7\n",
      "\n",
      "======== new gamma: 0.77656938 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 79.9686\n",
      "---------------------------\n",
      "Episode: 1651/5000, num_steps: 360, total_reward: -232.9\n",
      "Episode: 1652/5000, num_steps: 294, total_reward: -13.06\n",
      "Episode: 1653/5000, num_steps: 527, total_reward: -217.7\n",
      "Episode: 1654/5000, num_steps: 476, total_reward: 224.6\n",
      "Episode: 1655/5000, num_steps: 320, total_reward: -254.4\n",
      "Episode: 1656/5000, num_steps: 232, total_reward: -215.6\n",
      "Episode: 1657/5000, num_steps: 387, total_reward: -224.0\n",
      "Episode: 1658/5000, num_steps: 368, total_reward: 232.2\n",
      "Episode: 1659/5000, num_steps: 421, total_reward: -270.1\n",
      "Episode: 1660/5000, num_steps: 288, total_reward: 232.5\n",
      "\n",
      "======== new gamma: 0.7862765 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -109.866\n",
      "---------------------------\n",
      "Episode: 1661/5000, num_steps: 99, total_reward: 35.17\n",
      "Episode: 1662/5000, num_steps: 286, total_reward: -18.96\n",
      "Episode: 1663/5000, num_steps: 206, total_reward: -212.9\n",
      "Episode: 1664/5000, num_steps: 727, total_reward: -155.2\n",
      "Episode: 1665/5000, num_steps: 301, total_reward: 298.6\n",
      "Episode: 1666/5000, num_steps: 996, total_reward: 248.3\n",
      "Episode: 1667/5000, num_steps: 281, total_reward: -206.3\n",
      "Episode: 1668/5000, num_steps: 292, total_reward: -18.85\n",
      "Episode: 1669/5000, num_steps: 585, total_reward: 208.5\n",
      "Episode: 1670/5000, num_steps: 423, total_reward: 171.2\n",
      "\n",
      "======== new gamma: 0.79610495 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 41.0929\n",
      "---------------------------\n",
      "Episode: 1671/5000, num_steps: 291, total_reward: -66.85\n",
      "Episode: 1672/5000, num_steps: 285, total_reward: 16.33\n",
      "Episode: 1673/5000, num_steps: 303, total_reward: -223.3\n",
      "Episode: 1674/5000, num_steps: 340, total_reward: -56.67\n",
      "Episode: 1675/5000, num_steps: 222, total_reward: 23.93\n",
      "Episode: 1676/5000, num_steps: 304, total_reward: -11.46\n",
      "Episode: 1677/5000, num_steps: 434, total_reward: -92.9\n",
      "Episode: 1678/5000, num_steps: 227, total_reward: -185.1\n",
      "Episode: 1679/5000, num_steps: 464, total_reward: 239.4\n",
      "Episode: 1680/5000, num_steps: 279, total_reward: -27.96\n",
      "\n",
      "======== new gamma: 0.80605626 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -18.5322\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1681/5000, num_steps: 225, total_reward: -150.4\n",
      "Episode: 1682/5000, num_steps: 205, total_reward: -196.6\n",
      "Episode: 1683/5000, num_steps: 227, total_reward: -11.02\n",
      "Episode: 1684/5000, num_steps: 556, total_reward: 172.1\n",
      "Episode: 1685/5000, num_steps: 236, total_reward: -212.0\n",
      "Episode: 1686/5000, num_steps: 540, total_reward: 237.0\n",
      "Episode: 1687/5000, num_steps: 634, total_reward: 243.5\n",
      "Episode: 1688/5000, num_steps: 207, total_reward: -143.9\n",
      "Episode: 1689/5000, num_steps: 476, total_reward: -52.34\n",
      "Episode: 1690/5000, num_steps: 230, total_reward: -11.69\n",
      "\n",
      "======== new gamma: 0.81613197 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -14.1573\n",
      "---------------------------\n",
      "Episode: 1691/5000, num_steps: 1000, total_reward: 58.84\n",
      "Episode: 1692/5000, num_steps: 450, total_reward: 300.2\n",
      "Episode: 1693/5000, num_steps: 292, total_reward: -14.02\n",
      "Episode: 1694/5000, num_steps: 447, total_reward: 245.5\n",
      "Episode: 1695/5000, num_steps: 238, total_reward: 26.17\n",
      "Episode: 1696/5000, num_steps: 225, total_reward: 9.855\n",
      "Episode: 1697/5000, num_steps: 233, total_reward: -31.76\n",
      "Episode: 1698/5000, num_steps: 485, total_reward: -52.63\n",
      "Episode: 1699/5000, num_steps: 104, total_reward: -0.1632\n",
      "Episode: 1700/5000, num_steps: 301, total_reward: -54.66\n",
      "\n",
      "======== new gamma: 0.82633362 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 53.033\n",
      "---------------------------\n",
      "Episode: 1701/5000, num_steps: 840, total_reward: 192.5\n",
      "Episode: 1702/5000, num_steps: 350, total_reward: -219.9\n",
      "Episode: 1703/5000, num_steps: 481, total_reward: 252.5\n",
      "Episode: 1704/5000, num_steps: 796, total_reward: 205.9\n",
      "Episode: 1705/5000, num_steps: 264, total_reward: -233.8\n",
      "Episode: 1706/5000, num_steps: 510, total_reward: 233.6\n",
      "Episode: 1707/5000, num_steps: 224, total_reward: -188.0\n",
      "Episode: 1708/5000, num_steps: 207, total_reward: -165.7\n",
      "Episode: 1709/5000, num_steps: 364, total_reward: 244.7\n",
      "Episode: 1710/5000, num_steps: 113, total_reward: 76.61\n",
      "\n",
      "======== new gamma: 0.83666279 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 26.7231\n",
      "---------------------------\n",
      "Episode: 1711/5000, num_steps: 278, total_reward: -214.7\n",
      "Episode: 1712/5000, num_steps: 229, total_reward: 10.67\n",
      "Episode: 1713/5000, num_steps: 521, total_reward: 192.1\n",
      "Episode: 1714/5000, num_steps: 210, total_reward: 6.59\n",
      "Episode: 1715/5000, num_steps: 404, total_reward: 244.2\n",
      "Episode: 1716/5000, num_steps: 423, total_reward: 236.9\n",
      "Episode: 1717/5000, num_steps: 211, total_reward: -27.2\n",
      "Episode: 1718/5000, num_steps: 164, total_reward: -13.31\n",
      "Episode: 1719/5000, num_steps: 363, total_reward: 243.1\n",
      "Episode: 1720/5000, num_steps: 133, total_reward: -216.4\n",
      "\n",
      "======== new gamma: 0.84712107 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 75.4892\n",
      "---------------------------\n",
      "Episode: 1721/5000, num_steps: 155, total_reward: -179.5\n",
      "Episode: 1722/5000, num_steps: 256, total_reward: 20.87\n",
      "Episode: 1723/5000, num_steps: 205, total_reward: -4.994\n",
      "Episode: 1724/5000, num_steps: 331, total_reward: 274.7\n",
      "Episode: 1725/5000, num_steps: 391, total_reward: 250.5\n",
      "Episode: 1726/5000, num_steps: 559, total_reward: 220.5\n",
      "Episode: 1727/5000, num_steps: 511, total_reward: 248.4\n",
      "Episode: 1728/5000, num_steps: 449, total_reward: 247.9\n",
      "Episode: 1729/5000, num_steps: 162, total_reward: -148.7\n",
      "Episode: 1730/5000, num_steps: 279, total_reward: 2.64\n",
      "\n",
      "======== new gamma: 0.85771008 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 71.3391\n",
      "---------------------------\n",
      "Episode: 1731/5000, num_steps: 510, total_reward: -75.63\n",
      "Episode: 1732/5000, num_steps: 164, total_reward: -185.6\n",
      "Episode: 1733/5000, num_steps: 303, total_reward: -2.537\n",
      "Episode: 1734/5000, num_steps: 194, total_reward: -211.1\n",
      "Episode: 1735/5000, num_steps: 137, total_reward: -31.03\n",
      "Episode: 1736/5000, num_steps: 81, total_reward: -9.932\n",
      "Episode: 1737/5000, num_steps: 338, total_reward: 2.553\n",
      "Episode: 1738/5000, num_steps: 401, total_reward: -36.62\n",
      "Episode: 1739/5000, num_steps: 264, total_reward: -36.68\n",
      "Episode: 1740/5000, num_steps: 352, total_reward: 243.1\n",
      "\n",
      "======== new gamma: 0.86843146 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -58.3932\n",
      "---------------------------\n",
      "Episode: 1741/5000, num_steps: 529, total_reward: 244.4\n",
      "Episode: 1742/5000, num_steps: 420, total_reward: 264.6\n",
      "Episode: 1743/5000, num_steps: 265, total_reward: -65.23\n",
      "Episode: 1744/5000, num_steps: 185, total_reward: -177.6\n",
      "Episode: 1745/5000, num_steps: 170, total_reward: -6.681\n",
      "Episode: 1746/5000, num_steps: 154, total_reward: -134.3\n",
      "Episode: 1747/5000, num_steps: 215, total_reward: 24.76\n",
      "Episode: 1748/5000, num_steps: 147, total_reward: -156.6\n",
      "Episode: 1749/5000, num_steps: 234, total_reward: 19.39\n",
      "Episode: 1750/5000, num_steps: 291, total_reward: -219.1\n",
      "\n",
      "======== new gamma: 0.87928685 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 25.5725\n",
      "---------------------------\n",
      "Episode: 1751/5000, num_steps: 372, total_reward: -193.1\n",
      "Episode: 1752/5000, num_steps: 157, total_reward: -35.0\n",
      "Episode: 1753/5000, num_steps: 430, total_reward: 186.9\n",
      "Episode: 1754/5000, num_steps: 737, total_reward: 173.8\n",
      "Episode: 1755/5000, num_steps: 478, total_reward: -125.7\n",
      "Episode: 1756/5000, num_steps: 262, total_reward: 7.048\n",
      "Episode: 1757/5000, num_steps: 309, total_reward: -27.5\n",
      "Episode: 1758/5000, num_steps: 144, total_reward: -16.04\n",
      "Episode: 1759/5000, num_steps: 167, total_reward: 37.4\n",
      "Episode: 1760/5000, num_steps: 176, total_reward: -47.82\n",
      "\n",
      "======== new gamma: 0.89027794 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -21.1206\n",
      "---------------------------\n",
      "Episode: 1761/5000, num_steps: 246, total_reward: 47.08\n",
      "Episode: 1762/5000, num_steps: 262, total_reward: 21.3\n",
      "Episode: 1763/5000, num_steps: 294, total_reward: 3.02\n",
      "Episode: 1764/5000, num_steps: 215, total_reward: -28.9\n",
      "Episode: 1765/5000, num_steps: 233, total_reward: -26.04\n",
      "Episode: 1766/5000, num_steps: 358, total_reward: -35.47\n",
      "Episode: 1767/5000, num_steps: 581, total_reward: 269.1\n",
      "Episode: 1768/5000, num_steps: 305, total_reward: -61.2\n",
      "Episode: 1769/5000, num_steps: 682, total_reward: 167.8\n",
      "Episode: 1770/5000, num_steps: 307, total_reward: 24.27\n",
      "\n",
      "======== new gamma: 0.90140641 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 30.8835\n",
      "---------------------------\n",
      "Episode: 1771/5000, num_steps: 1000, total_reward: 86.86\n",
      "Episode: 1772/5000, num_steps: 1000, total_reward: -61.7\n",
      "Episode: 1773/5000, num_steps: 267, total_reward: -217.4\n",
      "Episode: 1774/5000, num_steps: 710, total_reward: 156.4\n",
      "Episode: 1775/5000, num_steps: 264, total_reward: -60.68\n",
      "Episode: 1776/5000, num_steps: 353, total_reward: -33.66\n",
      "Episode: 1777/5000, num_steps: 292, total_reward: -41.58\n",
      "Episode: 1778/5000, num_steps: 305, total_reward: -36.91\n",
      "Episode: 1779/5000, num_steps: 797, total_reward: -111.5\n",
      "Episode: 1780/5000, num_steps: 361, total_reward: 18.82\n",
      "\n",
      "======== new gamma: 0.91267399 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -29.5876\n",
      "---------------------------\n",
      "Episode: 1781/5000, num_steps: 442, total_reward: 254.3\n",
      "Episode: 1782/5000, num_steps: 280, total_reward: -10.32\n",
      "Episode: 1783/5000, num_steps: 876, total_reward: 241.3\n",
      "Episode: 1784/5000, num_steps: 298, total_reward: -7.737\n",
      "Episode: 1785/5000, num_steps: 262, total_reward: 8.712\n",
      "Episode: 1786/5000, num_steps: 428, total_reward: 232.1\n",
      "Episode: 1787/5000, num_steps: 572, total_reward: 278.3\n",
      "Episode: 1788/5000, num_steps: 326, total_reward: 248.9\n",
      "Episode: 1789/5000, num_steps: 592, total_reward: 253.5\n",
      "Episode: 1790/5000, num_steps: 142, total_reward: -140.2\n",
      "\n",
      "======== new gamma: 0.92408242 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 151.788\n",
      "---------------------------\n",
      "Episode: 1791/5000, num_steps: 154, total_reward: 84.25\n",
      "Episode: 1792/5000, num_steps: 362, total_reward: -116.0\n",
      "Episode: 1793/5000, num_steps: 304, total_reward: -71.29\n",
      "Episode: 1794/5000, num_steps: 333, total_reward: 12.77\n",
      "Episode: 1795/5000, num_steps: 288, total_reward: -182.7\n",
      "Episode: 1796/5000, num_steps: 210, total_reward: -0.5839\n",
      "Episode: 1797/5000, num_steps: 218, total_reward: 35.24\n",
      "Episode: 1798/5000, num_steps: 156, total_reward: -48.17\n",
      "Episode: 1799/5000, num_steps: 569, total_reward: -81.98\n",
      "Episode: 1800/5000, num_steps: 198, total_reward: -9.802\n",
      "\n",
      "======== new gamma: 0.93563345 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -50.8625\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1801/5000, num_steps: 769, total_reward: 118.7\n",
      "Episode: 1802/5000, num_steps: 151, total_reward: -183.7\n",
      "Episode: 1803/5000, num_steps: 200, total_reward: -77.99\n",
      "Episode: 1804/5000, num_steps: 190, total_reward: 5.641\n",
      "Episode: 1805/5000, num_steps: 220, total_reward: -187.4\n",
      "Episode: 1806/5000, num_steps: 163, total_reward: 16.14\n",
      "Episode: 1807/5000, num_steps: 293, total_reward: 20.95\n",
      "Episode: 1808/5000, num_steps: 263, total_reward: -11.78\n",
      "Episode: 1809/5000, num_steps: 201, total_reward: 26.7\n",
      "Episode: 1810/5000, num_steps: 276, total_reward: 5.108\n",
      "\n",
      "======== new gamma: 0.94732887 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -28.2559\n",
      "---------------------------\n",
      "Episode: 1811/5000, num_steps: 267, total_reward: -7.814\n",
      "Episode: 1812/5000, num_steps: 79, total_reward: -495.5\n",
      "Episode: 1813/5000, num_steps: 490, total_reward: -67.82\n",
      "Episode: 1814/5000, num_steps: 188, total_reward: -48.36\n",
      "Episode: 1815/5000, num_steps: 117, total_reward: -185.1\n",
      "Episode: 1816/5000, num_steps: 381, total_reward: -30.79\n",
      "Episode: 1817/5000, num_steps: 159, total_reward: 27.96\n",
      "Episode: 1818/5000, num_steps: 807, total_reward: 188.3\n",
      "Episode: 1819/5000, num_steps: 860, total_reward: 142.6\n",
      "Episode: 1820/5000, num_steps: 181, total_reward: 32.36\n",
      "\n",
      "======== new gamma: 0.95917048 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -47.1341\n",
      "---------------------------\n",
      "Episode: 1821/5000, num_steps: 538, total_reward: -76.6\n",
      "Episode: 1822/5000, num_steps: 201, total_reward: -4.141\n",
      "Episode: 1823/5000, num_steps: 94, total_reward: -11.86\n",
      "Episode: 1824/5000, num_steps: 588, total_reward: -118.3\n",
      "Episode: 1825/5000, num_steps: 231, total_reward: 27.25\n",
      "Episode: 1826/5000, num_steps: 125, total_reward: -20.96\n",
      "Episode: 1827/5000, num_steps: 985, total_reward: -183.5\n",
      "Episode: 1828/5000, num_steps: 236, total_reward: -174.0\n",
      "Episode: 1829/5000, num_steps: 214, total_reward: -13.92\n",
      "Episode: 1830/5000, num_steps: 194, total_reward: -54.64\n",
      "\n",
      "======== new gamma: 0.97116011 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = -54.3649\n",
      "---------------------------\n",
      "Episode: 1831/5000, num_steps: 1000, total_reward: -79.78\n",
      "Episode: 1832/5000, num_steps: 1000, total_reward: -59.88\n",
      "Episode: 1833/5000, num_steps: 311, total_reward: -79.05\n",
      "Episode: 1834/5000, num_steps: 100, total_reward: 10.8\n",
      "Episode: 1835/5000, num_steps: 212, total_reward: 41.91\n",
      "Episode: 1836/5000, num_steps: 202, total_reward: 19.23\n",
      "Episode: 1837/5000, num_steps: 548, total_reward: 214.4\n",
      "Episode: 1838/5000, num_steps: 1000, total_reward: -86.79\n",
      "Episode: 1839/5000, num_steps: 291, total_reward: 274.3\n",
      "Episode: 1840/5000, num_steps: 836, total_reward: -105.0\n",
      "\n",
      "======== new gamma: 0.98329961 ========\n",
      "---------------------------\n",
      "Last 10 episode avg = 20.0494\n",
      "---------------------------\n",
      "Episode: 1841/5000, num_steps: 925, total_reward: 97.21\n",
      "Episode: 1842/5000, num_steps: 498, total_reward: 207.6\n",
      "Episode: 1843/5000, num_steps: 366, total_reward: 242.0\n",
      "Episode: 1844/5000, num_steps: 739, total_reward: -186.7\n",
      "Episode: 1845/5000, num_steps: 334, total_reward: 227.4\n",
      "Episode: 1846/5000, num_steps: 839, total_reward: -106.5\n",
      "Episode: 1847/5000, num_steps: 136, total_reward: 2.949\n",
      "Episode: 1848/5000, num_steps: 786, total_reward: 124.9\n",
      "Episode: 1849/5000, num_steps: 221, total_reward: -27.19\n",
      "Episode: 1850/5000, num_steps: 714, total_reward: -143.1\n",
      "---------------------------\n",
      "Last 10 episode avg = 47.671\n",
      "---------------------------\n",
      "Episode: 1851/5000, num_steps: 507, total_reward: 217.2\n",
      "Episode: 1852/5000, num_steps: 319, total_reward: -16.87\n",
      "Episode: 1853/5000, num_steps: 735, total_reward: 167.2\n",
      "Episode: 1854/5000, num_steps: 1000, total_reward: -12.05\n",
      "Episode: 1855/5000, num_steps: 1000, total_reward: 128.6\n",
      "Episode: 1856/5000, num_steps: 745, total_reward: 153.8\n",
      "Episode: 1857/5000, num_steps: 1000, total_reward: 49.85\n",
      "Episode: 1858/5000, num_steps: 983, total_reward: 84.4\n",
      "Episode: 1859/5000, num_steps: 677, total_reward: -147.1\n",
      "Episode: 1860/5000, num_steps: 502, total_reward: 210.5\n",
      "---------------------------\n",
      "Last 10 episode avg = 48.2095\n",
      "---------------------------\n",
      "Episode: 1861/5000, num_steps: 192, total_reward: 22.42\n",
      "Episode: 1862/5000, num_steps: 392, total_reward: 241.0\n",
      "Episode: 1863/5000, num_steps: 390, total_reward: 250.9\n",
      "Episode: 1864/5000, num_steps: 536, total_reward: 176.0\n",
      "Episode: 1865/5000, num_steps: 260, total_reward: -8.234\n",
      "Episode: 1866/5000, num_steps: 976, total_reward: 224.1\n",
      "Episode: 1867/5000, num_steps: 137, total_reward: 29.25\n",
      "Episode: 1868/5000, num_steps: 516, total_reward: 187.0\n",
      "Episode: 1869/5000, num_steps: 1000, total_reward: 95.06\n",
      "Episode: 1870/5000, num_steps: 645, total_reward: -252.9\n",
      "---------------------------\n",
      "Last 10 episode avg = 142.791\n",
      "---------------------------\n",
      "Episode: 1871/5000, num_steps: 170, total_reward: -52.12\n",
      "Episode: 1872/5000, num_steps: 448, total_reward: -128.7\n",
      "Episode: 1873/5000, num_steps: 293, total_reward: -13.87\n",
      "Episode: 1874/5000, num_steps: 317, total_reward: 8.759\n",
      "Episode: 1875/5000, num_steps: 470, total_reward: 240.7\n",
      "Episode: 1876/5000, num_steps: 526, total_reward: -38.49\n",
      "Episode: 1877/5000, num_steps: 160, total_reward: 13.29\n",
      "Episode: 1878/5000, num_steps: 232, total_reward: -1.147\n",
      "Episode: 1879/5000, num_steps: 570, total_reward: 193.9\n",
      "Episode: 1880/5000, num_steps: 637, total_reward: 142.6\n",
      "---------------------------\n",
      "Last 10 episode avg = -3.06687\n",
      "---------------------------\n",
      "Episode: 1881/5000, num_steps: 142, total_reward: 27.12\n",
      "Episode: 1882/5000, num_steps: 425, total_reward: -79.29\n",
      "Episode: 1883/5000, num_steps: 315, total_reward: -53.55\n",
      "Episode: 1884/5000, num_steps: 815, total_reward: 150.9\n",
      "Episode: 1885/5000, num_steps: 658, total_reward: 217.2\n",
      "Episode: 1886/5000, num_steps: 660, total_reward: 244.3\n",
      "Episode: 1887/5000, num_steps: 419, total_reward: 226.2\n",
      "Episode: 1888/5000, num_steps: 427, total_reward: 247.3\n",
      "Episode: 1889/5000, num_steps: 263, total_reward: -4.664\n",
      "Episode: 1890/5000, num_steps: 286, total_reward: -90.3\n",
      "---------------------------\n",
      "Last 10 episode avg = 111.822\n",
      "---------------------------\n",
      "Episode: 1891/5000, num_steps: 168, total_reward: 42.54\n",
      "Episode: 1892/5000, num_steps: 860, total_reward: 195.9\n",
      "Episode: 1893/5000, num_steps: 407, total_reward: 276.7\n",
      "Episode: 1894/5000, num_steps: 634, total_reward: 256.2\n",
      "Episode: 1895/5000, num_steps: 371, total_reward: 210.1\n",
      "Episode: 1896/5000, num_steps: 291, total_reward: 272.7\n",
      "Episode: 1897/5000, num_steps: 252, total_reward: 18.95\n",
      "Episode: 1898/5000, num_steps: 365, total_reward: 230.3\n",
      "Episode: 1899/5000, num_steps: 373, total_reward: 167.6\n",
      "Episode: 1900/5000, num_steps: 229, total_reward: -7.642\n",
      "---------------------------\n",
      "Last 10 episode avg = 158.06\n",
      "---------------------------\n",
      "Episode: 1901/5000, num_steps: 233, total_reward: -28.02\n",
      "Episode: 1902/5000, num_steps: 130, total_reward: 59.74\n",
      "Episode: 1903/5000, num_steps: 541, total_reward: 270.3\n",
      "Episode: 1904/5000, num_steps: 455, total_reward: 165.5\n",
      "Episode: 1905/5000, num_steps: 443, total_reward: 186.0\n",
      "Episode: 1906/5000, num_steps: 705, total_reward: 197.8\n",
      "Episode: 1907/5000, num_steps: 196, total_reward: -12.58\n",
      "Episode: 1908/5000, num_steps: 1000, total_reward: 130.8\n",
      "Episode: 1909/5000, num_steps: 350, total_reward: 208.3\n",
      "Episode: 1910/5000, num_steps: 465, total_reward: 256.8\n",
      "---------------------------\n",
      "Last 10 episode avg = 117.007\n",
      "---------------------------\n",
      "Episode: 1911/5000, num_steps: 318, total_reward: 249.0\n",
      "Episode: 1912/5000, num_steps: 329, total_reward: 223.4\n",
      "Episode: 1913/5000, num_steps: 294, total_reward: -42.37\n",
      "Episode: 1914/5000, num_steps: 214, total_reward: 16.69\n",
      "Episode: 1915/5000, num_steps: 271, total_reward: 21.05\n",
      "Episode: 1916/5000, num_steps: 313, total_reward: 251.1\n",
      "Episode: 1917/5000, num_steps: 558, total_reward: 218.4\n",
      "Episode: 1918/5000, num_steps: 399, total_reward: 247.3\n",
      "Episode: 1919/5000, num_steps: 1000, total_reward: 48.05\n",
      "Episode: 1920/5000, num_steps: 109, total_reward: -32.5\n",
      "---------------------------\n",
      "Last 10 episode avg = 148.945\n",
      "---------------------------\n",
      "Episode: 1921/5000, num_steps: 241, total_reward: -4.199\n",
      "Episode: 1922/5000, num_steps: 126, total_reward: -11.91\n",
      "Episode: 1923/5000, num_steps: 425, total_reward: 212.4\n",
      "Episode: 1924/5000, num_steps: 243, total_reward: 23.59\n",
      "Episode: 1925/5000, num_steps: 492, total_reward: 141.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1926/5000, num_steps: 457, total_reward: 188.7\n",
      "Episode: 1927/5000, num_steps: 558, total_reward: 209.6\n",
      "Episode: 1928/5000, num_steps: 432, total_reward: 158.8\n",
      "Episode: 1929/5000, num_steps: 1000, total_reward: 144.7\n",
      "Episode: 1930/5000, num_steps: 288, total_reward: 7.769\n",
      "---------------------------\n",
      "Last 10 episode avg = 103.041\n",
      "---------------------------\n",
      "Episode: 1931/5000, num_steps: 312, total_reward: -26.62\n",
      "Episode: 1932/5000, num_steps: 480, total_reward: 179.4\n",
      "Episode: 1933/5000, num_steps: 128, total_reward: -12.36\n",
      "Episode: 1934/5000, num_steps: 410, total_reward: 194.6\n",
      "Episode: 1935/5000, num_steps: 1000, total_reward: 142.2\n",
      "Episode: 1936/5000, num_steps: 651, total_reward: 180.3\n",
      "Episode: 1937/5000, num_steps: 785, total_reward: 153.3\n",
      "Episode: 1938/5000, num_steps: 1000, total_reward: 97.66\n",
      "Episode: 1939/5000, num_steps: 1000, total_reward: 41.88\n",
      "Episode: 1940/5000, num_steps: 1000, total_reward: 47.0\n",
      "---------------------------\n",
      "Last 10 episode avg = 95.8235\n",
      "---------------------------\n",
      "Episode: 1941/5000, num_steps: 638, total_reward: 162.8\n",
      "Episode: 1942/5000, num_steps: 895, total_reward: 95.63\n",
      "Episode: 1943/5000, num_steps: 631, total_reward: 165.9\n",
      "Episode: 1944/5000, num_steps: 452, total_reward: 156.5\n",
      "Episode: 1945/5000, num_steps: 1000, total_reward: 98.09\n",
      "Episode: 1946/5000, num_steps: 1000, total_reward: 4.16\n",
      "Episode: 1947/5000, num_steps: 526, total_reward: -63.06\n",
      "Episode: 1948/5000, num_steps: 742, total_reward: 247.1\n",
      "Episode: 1949/5000, num_steps: 828, total_reward: 121.5\n",
      "Episode: 1950/5000, num_steps: 220, total_reward: 70.45\n",
      "---------------------------\n",
      "Last 10 episode avg = 103.579\n",
      "---------------------------\n",
      "Episode: 1951/5000, num_steps: 1000, total_reward: 39.88\n",
      "Episode: 1952/5000, num_steps: 962, total_reward: 167.6\n",
      "Episode: 1953/5000, num_steps: 760, total_reward: 107.4\n",
      "Episode: 1954/5000, num_steps: 741, total_reward: 128.1\n",
      "Episode: 1955/5000, num_steps: 585, total_reward: -154.8\n",
      "Episode: 1956/5000, num_steps: 502, total_reward: 242.3\n",
      "Episode: 1957/5000, num_steps: 754, total_reward: 185.0\n",
      "Episode: 1958/5000, num_steps: 1000, total_reward: 65.24\n",
      "Episode: 1959/5000, num_steps: 777, total_reward: 278.8\n",
      "Episode: 1960/5000, num_steps: 1000, total_reward: 17.57\n",
      "---------------------------\n",
      "Last 10 episode avg = 113.003\n",
      "---------------------------\n",
      "Episode: 1961/5000, num_steps: 900, total_reward: 158.3\n",
      "Episode: 1962/5000, num_steps: 431, total_reward: 251.5\n",
      "Episode: 1963/5000, num_steps: 570, total_reward: 179.5\n",
      "Episode: 1964/5000, num_steps: 682, total_reward: 209.5\n",
      "Episode: 1965/5000, num_steps: 593, total_reward: 180.8\n",
      "Episode: 1966/5000, num_steps: 758, total_reward: 179.3\n",
      "Episode: 1967/5000, num_steps: 1000, total_reward: 13.25\n",
      "Episode: 1968/5000, num_steps: 884, total_reward: 151.2\n",
      "Episode: 1969/5000, num_steps: 901, total_reward: 143.9\n",
      "Episode: 1970/5000, num_steps: 800, total_reward: 164.6\n",
      "---------------------------\n",
      "Last 10 episode avg = 148.477\n",
      "---------------------------\n",
      "Episode: 1971/5000, num_steps: 1000, total_reward: 53.43\n",
      "Episode: 1972/5000, num_steps: 466, total_reward: 194.8\n",
      "Episode: 1973/5000, num_steps: 988, total_reward: 143.7\n",
      "Episode: 1974/5000, num_steps: 1000, total_reward: 38.8\n",
      "Episode: 1975/5000, num_steps: 605, total_reward: -67.15\n",
      "Episode: 1976/5000, num_steps: 615, total_reward: 225.6\n",
      "Episode: 1977/5000, num_steps: 804, total_reward: 176.6\n",
      "Episode: 1978/5000, num_steps: 676, total_reward: 149.4\n",
      "Episode: 1979/5000, num_steps: 574, total_reward: -45.61\n",
      "Episode: 1980/5000, num_steps: 1000, total_reward: -9.201\n",
      "---------------------------\n",
      "Last 10 episode avg = 103.413\n",
      "---------------------------\n",
      "Episode: 1981/5000, num_steps: 863, total_reward: -180.9\n",
      "Episode: 1982/5000, num_steps: 525, total_reward: 163.9\n",
      "Episode: 1983/5000, num_steps: 832, total_reward: 130.1\n",
      "Episode: 1984/5000, num_steps: 637, total_reward: -47.73\n",
      "Episode: 1985/5000, num_steps: 1000, total_reward: -38.77\n",
      "Episode: 1986/5000, num_steps: 879, total_reward: 150.7\n",
      "Episode: 1987/5000, num_steps: 866, total_reward: -157.4\n",
      "Episode: 1988/5000, num_steps: 961, total_reward: -187.0\n",
      "Episode: 1989/5000, num_steps: 493, total_reward: -44.68\n",
      "Episode: 1990/5000, num_steps: 1000, total_reward: -70.75\n",
      "---------------------------\n",
      "Last 10 episode avg = -22.1078\n",
      "---------------------------\n",
      "Episode: 1991/5000, num_steps: 1000, total_reward: -17.94\n",
      "Episode: 1992/5000, num_steps: 620, total_reward: 161.3\n",
      "Episode: 1993/5000, num_steps: 1000, total_reward: 25.68\n",
      "Episode: 1994/5000, num_steps: 760, total_reward: 133.7\n",
      "Episode: 1995/5000, num_steps: 904, total_reward: 131.3\n",
      "Episode: 1996/5000, num_steps: 1000, total_reward: -44.23\n",
      "Episode: 1997/5000, num_steps: 1000, total_reward: -4.589\n",
      "Episode: 1998/5000, num_steps: 1000, total_reward: -22.44\n",
      "Episode: 1999/5000, num_steps: 208, total_reward: -43.75\n",
      "Episode: 2000/5000, num_steps: 588, total_reward: 179.7\n",
      "---------------------------\n",
      "Last 10 episode avg = 24.835\n",
      "---------------------------\n",
      "Episode: 2001/5000, num_steps: 641, total_reward: 227.1\n",
      "Episode: 2002/5000, num_steps: 935, total_reward: 138.3\n",
      "Episode: 2003/5000, num_steps: 612, total_reward: -78.5\n",
      "Episode: 2004/5000, num_steps: 621, total_reward: -0.3028\n",
      "Episode: 2005/5000, num_steps: 372, total_reward: 251.6\n",
      "Episode: 2006/5000, num_steps: 357, total_reward: 250.6\n",
      "Episode: 2007/5000, num_steps: 406, total_reward: 207.4\n",
      "Episode: 2008/5000, num_steps: 181, total_reward: 31.32\n",
      "Episode: 2009/5000, num_steps: 332, total_reward: -46.82\n",
      "Episode: 2010/5000, num_steps: 131, total_reward: -4.574\n",
      "---------------------------\n",
      "Last 10 episode avg = 116.039\n",
      "---------------------------\n",
      "Episode: 2011/5000, num_steps: 282, total_reward: 12.67\n",
      "Episode: 2012/5000, num_steps: 252, total_reward: 9.144\n",
      "Episode: 2013/5000, num_steps: 200, total_reward: -4.759\n",
      "Episode: 2014/5000, num_steps: 526, total_reward: 202.4\n",
      "Episode: 2015/5000, num_steps: 540, total_reward: 218.3\n",
      "Episode: 2016/5000, num_steps: 412, total_reward: 235.2\n",
      "Episode: 2017/5000, num_steps: 293, total_reward: -6.034\n",
      "Episode: 2018/5000, num_steps: 1000, total_reward: 142.7\n",
      "Episode: 2019/5000, num_steps: 680, total_reward: 200.3\n",
      "Episode: 2020/5000, num_steps: 275, total_reward: 8.446\n",
      "---------------------------\n",
      "Last 10 episode avg = 100.536\n",
      "---------------------------\n",
      "Episode: 2021/5000, num_steps: 303, total_reward: 257.7\n",
      "Episode: 2022/5000, num_steps: 287, total_reward: -4.396\n",
      "Episode: 2023/5000, num_steps: 1000, total_reward: 103.5\n",
      "Episode: 2024/5000, num_steps: 277, total_reward: 252.0\n",
      "Episode: 2025/5000, num_steps: 235, total_reward: -1.053\n",
      "Episode: 2026/5000, num_steps: 385, total_reward: 275.7\n",
      "Episode: 2027/5000, num_steps: 330, total_reward: -69.11\n",
      "Episode: 2028/5000, num_steps: 324, total_reward: 230.4\n",
      "Episode: 2029/5000, num_steps: 267, total_reward: -46.39\n",
      "Episode: 2030/5000, num_steps: 269, total_reward: -26.95\n",
      "---------------------------\n",
      "Last 10 episode avg = 100.68\n",
      "---------------------------\n",
      "Episode: 2031/5000, num_steps: 305, total_reward: -70.82\n",
      "Episode: 2032/5000, num_steps: 255, total_reward: 3.477\n",
      "Episode: 2033/5000, num_steps: 414, total_reward: 217.8\n",
      "Episode: 2034/5000, num_steps: 288, total_reward: 222.1\n",
      "Episode: 2035/5000, num_steps: 264, total_reward: 21.79\n",
      "Episode: 2036/5000, num_steps: 628, total_reward: 239.4\n",
      "Episode: 2037/5000, num_steps: 478, total_reward: 230.4\n",
      "Episode: 2038/5000, num_steps: 308, total_reward: 9.679\n",
      "Episode: 2039/5000, num_steps: 277, total_reward: -37.39\n",
      "Episode: 2040/5000, num_steps: 236, total_reward: 15.4\n",
      "---------------------------\n",
      "Last 10 episode avg = 80.9449\n",
      "---------------------------\n",
      "Episode: 2041/5000, num_steps: 519, total_reward: 203.0\n",
      "Episode: 2042/5000, num_steps: 138, total_reward: 44.86\n",
      "Episode: 2043/5000, num_steps: 211, total_reward: 26.44\n",
      "Episode: 2044/5000, num_steps: 212, total_reward: -11.35\n",
      "Episode: 2045/5000, num_steps: 371, total_reward: 219.1\n",
      "Episode: 2046/5000, num_steps: 258, total_reward: -25.95\n",
      "Episode: 2047/5000, num_steps: 152, total_reward: -4.221\n",
      "Episode: 2048/5000, num_steps: 244, total_reward: 6.603\n",
      "Episode: 2049/5000, num_steps: 234, total_reward: 18.27\n",
      "Episode: 2050/5000, num_steps: 244, total_reward: -17.41\n",
      "---------------------------\n",
      "Last 10 episode avg = 49.2138\n",
      "---------------------------\n",
      "Episode: 2051/5000, num_steps: 428, total_reward: 225.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2052/5000, num_steps: 187, total_reward: -36.84\n",
      "Episode: 2053/5000, num_steps: 247, total_reward: 51.54\n",
      "Episode: 2054/5000, num_steps: 212, total_reward: -20.47\n",
      "Episode: 2055/5000, num_steps: 262, total_reward: 3.877\n",
      "Episode: 2056/5000, num_steps: 368, total_reward: 216.3\n",
      "Episode: 2057/5000, num_steps: 227, total_reward: 14.96\n",
      "Episode: 2058/5000, num_steps: 267, total_reward: -6.759\n",
      "Episode: 2059/5000, num_steps: 213, total_reward: -3.263\n",
      "Episode: 2060/5000, num_steps: 349, total_reward: 228.5\n",
      "---------------------------\n",
      "Last 10 episode avg = 42.6924\n",
      "---------------------------\n",
      "Episode: 2061/5000, num_steps: 289, total_reward: -54.18\n",
      "Episode: 2062/5000, num_steps: 1000, total_reward: 130.2\n",
      "Episode: 2063/5000, num_steps: 451, total_reward: 191.3\n",
      "Episode: 2064/5000, num_steps: 383, total_reward: 218.5\n",
      "Episode: 2065/5000, num_steps: 314, total_reward: -47.29\n",
      "Episode: 2066/5000, num_steps: 407, total_reward: 203.6\n",
      "Episode: 2067/5000, num_steps: 1000, total_reward: 104.9\n",
      "Episode: 2068/5000, num_steps: 855, total_reward: 211.8\n",
      "Episode: 2069/5000, num_steps: 290, total_reward: 231.1\n",
      "Episode: 2070/5000, num_steps: 516, total_reward: 198.5\n",
      "---------------------------\n",
      "Last 10 episode avg = 141.837\n",
      "---------------------------\n",
      "Episode: 2071/5000, num_steps: 333, total_reward: 256.1\n",
      "Episode: 2072/5000, num_steps: 259, total_reward: 266.5\n",
      "Episode: 2073/5000, num_steps: 134, total_reward: 16.48\n",
      "Episode: 2074/5000, num_steps: 256, total_reward: 16.72\n",
      "Episode: 2075/5000, num_steps: 257, total_reward: -65.08\n",
      "Episode: 2076/5000, num_steps: 514, total_reward: 249.7\n",
      "Episode: 2077/5000, num_steps: 344, total_reward: 235.2\n",
      "Episode: 2078/5000, num_steps: 270, total_reward: -57.98\n",
      "Episode: 2079/5000, num_steps: 579, total_reward: 247.9\n",
      "Episode: 2080/5000, num_steps: 459, total_reward: 201.4\n",
      "---------------------------\n",
      "Last 10 episode avg = 136.414\n",
      "---------------------------\n",
      "Episode: 2081/5000, num_steps: 394, total_reward: 206.5\n",
      "Episode: 2082/5000, num_steps: 509, total_reward: 180.7\n",
      "Episode: 2083/5000, num_steps: 449, total_reward: 206.5\n",
      "Episode: 2084/5000, num_steps: 871, total_reward: 183.5\n",
      "Episode: 2085/5000, num_steps: 303, total_reward: -51.03\n",
      "Episode: 2086/5000, num_steps: 347, total_reward: 202.2\n",
      "Episode: 2087/5000, num_steps: 977, total_reward: 196.0\n",
      "Episode: 2088/5000, num_steps: 744, total_reward: 183.6\n",
      "Episode: 2089/5000, num_steps: 1000, total_reward: 95.16\n",
      "Episode: 2090/5000, num_steps: 711, total_reward: 255.5\n",
      "---------------------------\n",
      "Last 10 episode avg = 160.454\n",
      "---------------------------\n",
      "Episode: 2091/5000, num_steps: 378, total_reward: 262.1\n",
      "Episode: 2092/5000, num_steps: 308, total_reward: -46.27\n",
      "Episode: 2093/5000, num_steps: 402, total_reward: 196.8\n",
      "Episode: 2094/5000, num_steps: 435, total_reward: 265.2\n",
      "Episode: 2095/5000, num_steps: 225, total_reward: -12.24\n",
      "Episode: 2096/5000, num_steps: 604, total_reward: 211.3\n",
      "Episode: 2097/5000, num_steps: 142, total_reward: -3.679\n",
      "Episode: 2098/5000, num_steps: 474, total_reward: 226.7\n",
      "Episode: 2099/5000, num_steps: 337, total_reward: 238.1\n",
      "Episode: 2100/5000, num_steps: 501, total_reward: 196.3\n",
      "---------------------------\n",
      "Last 10 episode avg = 159.354\n",
      "---------------------------\n",
      "Episode: 2101/5000, num_steps: 203, total_reward: 57.36\n",
      "Episode: 2102/5000, num_steps: 712, total_reward: 214.8\n",
      "Episode: 2103/5000, num_steps: 277, total_reward: -9.699\n",
      "Episode: 2104/5000, num_steps: 530, total_reward: 224.0\n",
      "Episode: 2105/5000, num_steps: 282, total_reward: -23.11\n",
      "Episode: 2106/5000, num_steps: 245, total_reward: 4.007\n",
      "Episode: 2107/5000, num_steps: 389, total_reward: 231.1\n",
      "Episode: 2108/5000, num_steps: 530, total_reward: 239.3\n",
      "Episode: 2109/5000, num_steps: 516, total_reward: 230.0\n",
      "Episode: 2110/5000, num_steps: 607, total_reward: 214.7\n",
      "---------------------------\n",
      "Last 10 episode avg = 136.396\n",
      "---------------------------\n",
      "Episode: 2111/5000, num_steps: 1000, total_reward: 36.42\n",
      "Episode: 2112/5000, num_steps: 676, total_reward: 244.7\n",
      "Episode: 2113/5000, num_steps: 1000, total_reward: 94.34\n",
      "Episode: 2114/5000, num_steps: 595, total_reward: -40.82\n",
      "Episode: 2115/5000, num_steps: 566, total_reward: 198.0\n",
      "Episode: 2116/5000, num_steps: 745, total_reward: 216.6\n",
      "Episode: 2117/5000, num_steps: 223, total_reward: -1.182\n",
      "Episode: 2118/5000, num_steps: 165, total_reward: 33.35\n",
      "Episode: 2119/5000, num_steps: 500, total_reward: 212.7\n",
      "Episode: 2120/5000, num_steps: 1000, total_reward: 128.3\n",
      "---------------------------\n",
      "Last 10 episode avg = 120.889\n",
      "---------------------------\n",
      "Episode: 2121/5000, num_steps: 368, total_reward: -7.177\n",
      "Episode: 2122/5000, num_steps: 417, total_reward: 238.6\n",
      "Episode: 2123/5000, num_steps: 362, total_reward: 221.2\n",
      "Episode: 2124/5000, num_steps: 747, total_reward: 218.8\n",
      "Episode: 2125/5000, num_steps: 636, total_reward: 268.7\n",
      "Episode: 2126/5000, num_steps: 385, total_reward: 160.7\n",
      "Episode: 2127/5000, num_steps: 478, total_reward: 166.8\n",
      "Episode: 2128/5000, num_steps: 1000, total_reward: 95.87\n",
      "Episode: 2129/5000, num_steps: 654, total_reward: 181.1\n",
      "Episode: 2130/5000, num_steps: 333, total_reward: -36.1\n",
      "---------------------------\n",
      "Last 10 episode avg = 167.278\n",
      "---------------------------\n",
      "Episode: 2131/5000, num_steps: 518, total_reward: 197.1\n",
      "Episode: 2132/5000, num_steps: 327, total_reward: -58.71\n",
      "Episode: 2133/5000, num_steps: 421, total_reward: 242.7\n",
      "Episode: 2134/5000, num_steps: 363, total_reward: -36.38\n",
      "Episode: 2135/5000, num_steps: 446, total_reward: 232.6\n",
      "Episode: 2136/5000, num_steps: 325, total_reward: -10.51\n",
      "Episode: 2137/5000, num_steps: 631, total_reward: 184.0\n",
      "Episode: 2138/5000, num_steps: 702, total_reward: 214.0\n",
      "Episode: 2139/5000, num_steps: 522, total_reward: -53.06\n",
      "Episode: 2140/5000, num_steps: 478, total_reward: 225.1\n",
      "---------------------------\n",
      "Last 10 episode avg = 87.5774\n",
      "---------------------------\n",
      "Episode: 2141/5000, num_steps: 516, total_reward: 269.6\n",
      "Episode: 2142/5000, num_steps: 288, total_reward: 12.21\n",
      "Episode: 2143/5000, num_steps: 387, total_reward: 233.3\n",
      "Episode: 2144/5000, num_steps: 375, total_reward: -8.407\n",
      "Episode: 2145/5000, num_steps: 381, total_reward: 219.9\n",
      "Episode: 2146/5000, num_steps: 387, total_reward: 241.4\n",
      "Episode: 2147/5000, num_steps: 272, total_reward: 4.482\n",
      "Episode: 2148/5000, num_steps: 416, total_reward: 243.8\n",
      "Episode: 2149/5000, num_steps: 455, total_reward: 171.5\n",
      "Episode: 2150/5000, num_steps: 468, total_reward: 182.6\n",
      "---------------------------\n",
      "Last 10 episode avg = 161.302\n",
      "---------------------------\n",
      "Episode: 2151/5000, num_steps: 344, total_reward: -14.46\n",
      "Episode: 2152/5000, num_steps: 260, total_reward: -46.98\n",
      "Episode: 2153/5000, num_steps: 379, total_reward: 259.0\n",
      "Episode: 2154/5000, num_steps: 746, total_reward: 171.9\n",
      "Episode: 2155/5000, num_steps: 294, total_reward: -47.86\n",
      "Episode: 2156/5000, num_steps: 780, total_reward: 104.0\n",
      "Episode: 2157/5000, num_steps: 561, total_reward: 193.0\n",
      "Episode: 2158/5000, num_steps: 168, total_reward: 3.54\n",
      "Episode: 2159/5000, num_steps: 490, total_reward: 229.2\n",
      "Episode: 2160/5000, num_steps: 552, total_reward: 195.7\n",
      "---------------------------\n",
      "Last 10 episode avg = 103.391\n",
      "---------------------------\n",
      "Episode: 2161/5000, num_steps: 459, total_reward: 235.8\n",
      "Episode: 2162/5000, num_steps: 562, total_reward: 217.2\n",
      "Episode: 2163/5000, num_steps: 504, total_reward: -7.25\n",
      "Episode: 2164/5000, num_steps: 305, total_reward: -66.68\n",
      "Episode: 2165/5000, num_steps: 338, total_reward: 261.2\n",
      "Episode: 2166/5000, num_steps: 270, total_reward: 16.65\n",
      "Episode: 2167/5000, num_steps: 1000, total_reward: 132.2\n",
      "Episode: 2168/5000, num_steps: 285, total_reward: -59.1\n",
      "Episode: 2169/5000, num_steps: 364, total_reward: 11.27\n",
      "Episode: 2170/5000, num_steps: 175, total_reward: 32.61\n",
      "---------------------------\n",
      "Last 10 episode avg = 93.6962\n",
      "---------------------------\n",
      "Episode: 2171/5000, num_steps: 409, total_reward: 262.4\n",
      "Episode: 2172/5000, num_steps: 399, total_reward: 258.0\n",
      "Episode: 2173/5000, num_steps: 388, total_reward: 246.8\n",
      "Episode: 2174/5000, num_steps: 285, total_reward: -15.33\n",
      "Episode: 2175/5000, num_steps: 143, total_reward: 25.05\n",
      "Episode: 2176/5000, num_steps: 430, total_reward: 247.8\n",
      "Episode: 2177/5000, num_steps: 611, total_reward: 237.0\n",
      "Episode: 2178/5000, num_steps: 302, total_reward: -37.67\n",
      "Episode: 2179/5000, num_steps: 160, total_reward: 35.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2180/5000, num_steps: 367, total_reward: 253.7\n",
      "---------------------------\n",
      "Last 10 episode avg = 129.258\n",
      "---------------------------\n",
      "Episode: 2181/5000, num_steps: 728, total_reward: 229.2\n",
      "Episode: 2182/5000, num_steps: 332, total_reward: -4.515\n",
      "Episode: 2183/5000, num_steps: 814, total_reward: 214.2\n",
      "Episode: 2184/5000, num_steps: 563, total_reward: 156.8\n",
      "Episode: 2185/5000, num_steps: 420, total_reward: 233.4\n",
      "Episode: 2186/5000, num_steps: 147, total_reward: -46.17\n",
      "Episode: 2187/5000, num_steps: 675, total_reward: 240.0\n",
      "Episode: 2188/5000, num_steps: 555, total_reward: 167.1\n",
      "Episode: 2189/5000, num_steps: 320, total_reward: -99.76\n",
      "Episode: 2190/5000, num_steps: 329, total_reward: -80.85\n",
      "---------------------------\n",
      "Last 10 episode avg = 134.396\n",
      "---------------------------\n",
      "Episode: 2191/5000, num_steps: 206, total_reward: -19.27\n",
      "Episode: 2192/5000, num_steps: 447, total_reward: 230.4\n",
      "Episode: 2193/5000, num_steps: 486, total_reward: 186.2\n",
      "Episode: 2194/5000, num_steps: 445, total_reward: -30.87\n",
      "Episode: 2195/5000, num_steps: 425, total_reward: 207.7\n",
      "Episode: 2196/5000, num_steps: 570, total_reward: -70.52\n",
      "Episode: 2197/5000, num_steps: 746, total_reward: 241.4\n",
      "Episode: 2198/5000, num_steps: 582, total_reward: -75.5\n",
      "Episode: 2199/5000, num_steps: 857, total_reward: 215.9\n",
      "Episode: 2200/5000, num_steps: 543, total_reward: -95.65\n",
      "---------------------------\n",
      "Last 10 episode avg = 80.4668\n",
      "---------------------------\n",
      "Episode: 2201/5000, num_steps: 447, total_reward: 254.5\n",
      "Episode: 2202/5000, num_steps: 195, total_reward: 31.63\n",
      "Episode: 2203/5000, num_steps: 466, total_reward: 15.56\n",
      "Episode: 2204/5000, num_steps: 510, total_reward: 198.6\n",
      "Episode: 2205/5000, num_steps: 490, total_reward: 217.5\n",
      "Episode: 2206/5000, num_steps: 261, total_reward: 20.1\n",
      "Episode: 2207/5000, num_steps: 641, total_reward: 212.8\n",
      "Episode: 2208/5000, num_steps: 225, total_reward: 38.37\n",
      "Episode: 2209/5000, num_steps: 490, total_reward: 247.2\n",
      "Episode: 2210/5000, num_steps: 371, total_reward: 256.9\n",
      "---------------------------\n",
      "Last 10 episode avg = 114.057\n",
      "---------------------------\n",
      "Episode: 2211/5000, num_steps: 374, total_reward: 259.9\n",
      "Episode: 2212/5000, num_steps: 1000, total_reward: 106.2\n",
      "Episode: 2213/5000, num_steps: 1000, total_reward: 120.4\n",
      "Episode: 2214/5000, num_steps: 467, total_reward: 15.78\n",
      "Episode: 2215/5000, num_steps: 539, total_reward: 203.0\n",
      "Episode: 2216/5000, num_steps: 487, total_reward: 224.1\n",
      "Episode: 2217/5000, num_steps: 712, total_reward: 194.0\n",
      "Episode: 2218/5000, num_steps: 481, total_reward: 233.1\n",
      "Episode: 2219/5000, num_steps: 613, total_reward: 165.7\n",
      "Episode: 2220/5000, num_steps: 816, total_reward: 135.3\n",
      "---------------------------\n",
      "Last 10 episode avg = 177.91\n",
      "---------------------------\n",
      "Episode: 2221/5000, num_steps: 1000, total_reward: 101.6\n",
      "Episode: 2222/5000, num_steps: 514, total_reward: 191.9\n",
      "Episode: 2223/5000, num_steps: 266, total_reward: 3.993\n",
      "Episode: 2224/5000, num_steps: 354, total_reward: 226.5\n",
      "Episode: 2225/5000, num_steps: 378, total_reward: 241.7\n",
      "Episode: 2226/5000, num_steps: 204, total_reward: -4.424\n",
      "Episode: 2227/5000, num_steps: 308, total_reward: 263.8\n",
      "Episode: 2228/5000, num_steps: 390, total_reward: 214.4\n",
      "Episode: 2229/5000, num_steps: 411, total_reward: -146.1\n",
      "Episode: 2230/5000, num_steps: 399, total_reward: 247.9\n",
      "---------------------------\n",
      "Last 10 episode avg = 122.861\n",
      "---------------------------\n",
      "Episode: 2231/5000, num_steps: 568, total_reward: 203.3\n",
      "Episode: 2232/5000, num_steps: 154, total_reward: 39.06\n",
      "Episode: 2233/5000, num_steps: 470, total_reward: 185.6\n",
      "Episode: 2234/5000, num_steps: 351, total_reward: 270.8\n",
      "Episode: 2235/5000, num_steps: 381, total_reward: 227.5\n",
      "Episode: 2236/5000, num_steps: 352, total_reward: -12.86\n",
      "Episode: 2237/5000, num_steps: 425, total_reward: 245.9\n",
      "Episode: 2238/5000, num_steps: 384, total_reward: -33.76\n",
      "Episode: 2239/5000, num_steps: 54, total_reward: -105.4\n",
      "Episode: 2240/5000, num_steps: 214, total_reward: 36.69\n",
      "---------------------------\n",
      "Last 10 episode avg = 126.808\n",
      "---------------------------\n",
      "Episode: 2241/5000, num_steps: 453, total_reward: 256.6\n",
      "Episode: 2242/5000, num_steps: 644, total_reward: 191.4\n",
      "Episode: 2243/5000, num_steps: 1000, total_reward: 104.5\n",
      "Episode: 2244/5000, num_steps: 238, total_reward: 18.05\n",
      "Episode: 2245/5000, num_steps: 704, total_reward: 219.8\n",
      "Episode: 2246/5000, num_steps: 365, total_reward: 218.8\n",
      "Episode: 2247/5000, num_steps: 77, total_reward: -123.2\n",
      "Episode: 2248/5000, num_steps: 313, total_reward: 276.0\n",
      "Episode: 2249/5000, num_steps: 187, total_reward: -17.93\n",
      "Episode: 2250/5000, num_steps: 424, total_reward: 281.4\n",
      "---------------------------\n",
      "Last 10 episode avg = 118.073\n",
      "---------------------------\n",
      "Episode: 2251/5000, num_steps: 187, total_reward: 21.06\n",
      "Episode: 2252/5000, num_steps: 393, total_reward: 245.4\n",
      "Episode: 2253/5000, num_steps: 350, total_reward: 239.0\n",
      "Episode: 2254/5000, num_steps: 113, total_reward: -12.51\n",
      "Episode: 2255/5000, num_steps: 442, total_reward: 223.8\n",
      "Episode: 2256/5000, num_steps: 289, total_reward: 24.18\n",
      "Episode: 2257/5000, num_steps: 379, total_reward: 256.7\n",
      "Episode: 2258/5000, num_steps: 244, total_reward: 56.43\n",
      "Episode: 2259/5000, num_steps: 406, total_reward: 237.0\n",
      "Episode: 2260/5000, num_steps: 314, total_reward: 278.8\n",
      "---------------------------\n",
      "Last 10 episode avg = 157.252\n",
      "---------------------------\n",
      "Episode: 2261/5000, num_steps: 415, total_reward: 288.2\n",
      "Episode: 2262/5000, num_steps: 319, total_reward: 270.9\n",
      "Episode: 2263/5000, num_steps: 136, total_reward: 60.66\n",
      "Episode: 2264/5000, num_steps: 470, total_reward: -87.88\n",
      "Episode: 2265/5000, num_steps: 377, total_reward: 209.8\n",
      "Episode: 2266/5000, num_steps: 293, total_reward: 221.9\n",
      "Episode: 2267/5000, num_steps: 152, total_reward: -16.4\n",
      "Episode: 2268/5000, num_steps: 377, total_reward: 215.2\n",
      "Episode: 2269/5000, num_steps: 433, total_reward: 194.4\n",
      "Episode: 2270/5000, num_steps: 307, total_reward: 215.7\n",
      "---------------------------\n",
      "Last 10 episode avg = 163.55\n",
      "---------------------------\n",
      "Episode: 2271/5000, num_steps: 193, total_reward: -34.98\n",
      "Episode: 2272/5000, num_steps: 217, total_reward: -13.68\n",
      "Episode: 2273/5000, num_steps: 168, total_reward: 27.7\n",
      "Episode: 2274/5000, num_steps: 292, total_reward: 261.2\n",
      "Episode: 2275/5000, num_steps: 355, total_reward: 256.2\n",
      "Episode: 2276/5000, num_steps: 316, total_reward: 269.2\n",
      "Episode: 2277/5000, num_steps: 424, total_reward: 248.5\n",
      "Episode: 2278/5000, num_steps: 106, total_reward: -83.99\n",
      "Episode: 2279/5000, num_steps: 214, total_reward: 3.066\n",
      "Episode: 2280/5000, num_steps: 213, total_reward: 47.97\n",
      "---------------------------\n",
      "Last 10 episode avg = 114.894\n",
      "---------------------------\n",
      "Episode: 2281/5000, num_steps: 474, total_reward: 224.3\n",
      "Episode: 2282/5000, num_steps: 226, total_reward: 6.302\n",
      "Episode: 2283/5000, num_steps: 409, total_reward: 247.3\n",
      "Episode: 2284/5000, num_steps: 352, total_reward: 275.3\n",
      "Episode: 2285/5000, num_steps: 389, total_reward: 246.7\n",
      "Episode: 2286/5000, num_steps: 341, total_reward: 244.3\n",
      "Episode: 2287/5000, num_steps: 236, total_reward: -4.894\n",
      "Episode: 2288/5000, num_steps: 191, total_reward: -38.18\n",
      "Episode: 2289/5000, num_steps: 135, total_reward: 8.671\n",
      "Episode: 2290/5000, num_steps: 232, total_reward: -8.834\n",
      "---------------------------\n",
      "Last 10 episode avg = 125.781\n",
      "---------------------------\n",
      "Episode: 2291/5000, num_steps: 1000, total_reward: 107.2\n",
      "Episode: 2292/5000, num_steps: 691, total_reward: 191.5\n",
      "Episode: 2293/5000, num_steps: 457, total_reward: 210.6\n",
      "Episode: 2294/5000, num_steps: 272, total_reward: -12.98\n",
      "Episode: 2295/5000, num_steps: 500, total_reward: 200.4\n",
      "Episode: 2296/5000, num_steps: 484, total_reward: -82.04\n",
      "Episode: 2297/5000, num_steps: 228, total_reward: 10.25\n",
      "Episode: 2298/5000, num_steps: 355, total_reward: 286.4\n",
      "Episode: 2299/5000, num_steps: 441, total_reward: -216.0\n",
      "Episode: 2300/5000, num_steps: 228, total_reward: -25.44\n",
      "---------------------------\n",
      "Last 10 episode avg = 68.6566\n",
      "---------------------------\n",
      "Episode: 2301/5000, num_steps: 486, total_reward: 203.2\n",
      "Episode: 2302/5000, num_steps: 304, total_reward: 229.4\n",
      "Episode: 2303/5000, num_steps: 473, total_reward: -213.7\n",
      "Episode: 2304/5000, num_steps: 292, total_reward: 229.5\n",
      "Episode: 2305/5000, num_steps: 381, total_reward: 284.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2306/5000, num_steps: 378, total_reward: 164.2\n",
      "Episode: 2307/5000, num_steps: 360, total_reward: 267.8\n",
      "Episode: 2308/5000, num_steps: 462, total_reward: 197.5\n",
      "Episode: 2309/5000, num_steps: 227, total_reward: -25.17\n",
      "Episode: 2310/5000, num_steps: 285, total_reward: -57.83\n",
      "---------------------------\n",
      "Last 10 episode avg = 131.152\n",
      "---------------------------\n",
      "Episode: 2311/5000, num_steps: 293, total_reward: 246.9\n",
      "Episode: 2312/5000, num_steps: 300, total_reward: 251.0\n",
      "Episode: 2313/5000, num_steps: 174, total_reward: -34.23\n",
      "Episode: 2314/5000, num_steps: 226, total_reward: -0.2637\n",
      "Episode: 2315/5000, num_steps: 210, total_reward: -63.57\n",
      "Episode: 2316/5000, num_steps: 424, total_reward: 198.1\n",
      "Episode: 2317/5000, num_steps: 267, total_reward: 2.135\n",
      "Episode: 2318/5000, num_steps: 376, total_reward: 267.9\n",
      "Episode: 2319/5000, num_steps: 209, total_reward: 248.7\n",
      "Episode: 2320/5000, num_steps: 310, total_reward: 282.4\n",
      "---------------------------\n",
      "Last 10 episode avg = 105.87\n",
      "---------------------------\n",
      "Episode: 2321/5000, num_steps: 245, total_reward: -59.46\n",
      "Episode: 2322/5000, num_steps: 243, total_reward: -64.98\n",
      "Episode: 2323/5000, num_steps: 392, total_reward: 183.0\n",
      "Episode: 2324/5000, num_steps: 202, total_reward: -44.62\n",
      "Episode: 2325/5000, num_steps: 236, total_reward: -60.31\n",
      "Episode: 2326/5000, num_steps: 263, total_reward: -93.51\n",
      "Episode: 2327/5000, num_steps: 1000, total_reward: 57.0\n",
      "Episode: 2328/5000, num_steps: 306, total_reward: 212.1\n",
      "Episode: 2329/5000, num_steps: 655, total_reward: 223.3\n",
      "Episode: 2330/5000, num_steps: 364, total_reward: 235.9\n",
      "---------------------------\n",
      "Last 10 episode avg = 63.4962\n",
      "---------------------------\n",
      "Episode: 2331/5000, num_steps: 258, total_reward: 228.1\n",
      "Episode: 2332/5000, num_steps: 217, total_reward: -19.41\n",
      "Episode: 2333/5000, num_steps: 226, total_reward: 21.75\n",
      "Episode: 2334/5000, num_steps: 143, total_reward: 73.58\n",
      "Episode: 2335/5000, num_steps: 258, total_reward: -0.8464\n",
      "Episode: 2336/5000, num_steps: 194, total_reward: -17.31\n",
      "Episode: 2337/5000, num_steps: 580, total_reward: 237.5\n",
      "Episode: 2338/5000, num_steps: 434, total_reward: 273.8\n",
      "Episode: 2339/5000, num_steps: 325, total_reward: 275.4\n",
      "Episode: 2340/5000, num_steps: 189, total_reward: -18.68\n",
      "---------------------------\n",
      "Last 10 episode avg = 130.835\n",
      "---------------------------\n",
      "Episode: 2341/5000, num_steps: 193, total_reward: -47.91\n",
      "Episode: 2342/5000, num_steps: 486, total_reward: 223.7\n",
      "Episode: 2343/5000, num_steps: 672, total_reward: 198.4\n",
      "Episode: 2344/5000, num_steps: 291, total_reward: 4.669\n",
      "Episode: 2345/5000, num_steps: 1000, total_reward: 134.4\n",
      "Episode: 2346/5000, num_steps: 216, total_reward: 4.186\n",
      "Episode: 2347/5000, num_steps: 511, total_reward: 126.0\n",
      "Episode: 2348/5000, num_steps: 720, total_reward: 241.5\n",
      "Episode: 2349/5000, num_steps: 255, total_reward: -51.64\n",
      "Episode: 2350/5000, num_steps: 501, total_reward: 206.9\n",
      "---------------------------\n",
      "Last 10 episode avg = 81.4598\n",
      "---------------------------\n",
      "Episode: 2351/5000, num_steps: 442, total_reward: 200.8\n",
      "Episode: 2352/5000, num_steps: 372, total_reward: 218.2\n",
      "Episode: 2353/5000, num_steps: 335, total_reward: 13.22\n",
      "Episode: 2354/5000, num_steps: 373, total_reward: 174.1\n",
      "Episode: 2355/5000, num_steps: 335, total_reward: 259.7\n",
      "Episode: 2356/5000, num_steps: 335, total_reward: 229.9\n",
      "Episode: 2357/5000, num_steps: 242, total_reward: -66.22\n",
      "Episode: 2358/5000, num_steps: 519, total_reward: 160.0\n",
      "Episode: 2359/5000, num_steps: 364, total_reward: 245.2\n",
      "Episode: 2360/5000, num_steps: 387, total_reward: 251.7\n",
      "---------------------------\n",
      "Last 10 episode avg = 164.195\n",
      "---------------------------\n",
      "Episode: 2361/5000, num_steps: 185, total_reward: -22.68\n",
      "Episode: 2362/5000, num_steps: 279, total_reward: 286.3\n",
      "Episode: 2363/5000, num_steps: 173, total_reward: -39.2\n",
      "Episode: 2364/5000, num_steps: 417, total_reward: 264.5\n",
      "Episode: 2365/5000, num_steps: 225, total_reward: -44.09\n",
      "Episode: 2366/5000, num_steps: 294, total_reward: 240.9\n",
      "Episode: 2367/5000, num_steps: 396, total_reward: 213.1\n",
      "Episode: 2368/5000, num_steps: 390, total_reward: 252.5\n",
      "Episode: 2369/5000, num_steps: 474, total_reward: 211.1\n",
      "Episode: 2370/5000, num_steps: 427, total_reward: 158.8\n",
      "---------------------------\n",
      "Last 10 episode avg = 161.426\n",
      "---------------------------\n",
      "Episode: 2371/5000, num_steps: 236, total_reward: -50.55\n",
      "Episode: 2372/5000, num_steps: 1000, total_reward: 113.9\n",
      "Episode: 2373/5000, num_steps: 523, total_reward: 187.2\n",
      "Episode: 2374/5000, num_steps: 428, total_reward: 230.4\n",
      "Episode: 2375/5000, num_steps: 152, total_reward: 10.09\n",
      "Episode: 2376/5000, num_steps: 626, total_reward: 203.3\n",
      "Episode: 2377/5000, num_steps: 263, total_reward: -5.319\n",
      "Episode: 2378/5000, num_steps: 442, total_reward: 177.8\n",
      "Episode: 2379/5000, num_steps: 486, total_reward: 158.6\n",
      "Episode: 2380/5000, num_steps: 298, total_reward: -25.88\n",
      "---------------------------\n",
      "Last 10 episode avg = 118.437\n",
      "---------------------------\n",
      "Episode: 2381/5000, num_steps: 551, total_reward: 180.0\n",
      "Episode: 2382/5000, num_steps: 295, total_reward: 263.4\n",
      "Episode: 2383/5000, num_steps: 603, total_reward: 269.0\n",
      "Episode: 2384/5000, num_steps: 158, total_reward: 5.784\n",
      "Episode: 2385/5000, num_steps: 323, total_reward: 255.8\n",
      "Episode: 2386/5000, num_steps: 673, total_reward: 175.3\n",
      "Episode: 2387/5000, num_steps: 435, total_reward: 196.5\n",
      "Episode: 2388/5000, num_steps: 364, total_reward: 232.9\n",
      "Episode: 2389/5000, num_steps: 283, total_reward: 11.39\n",
      "Episode: 2390/5000, num_steps: 790, total_reward: 204.1\n",
      "---------------------------\n",
      "Last 10 episode avg = 156.428\n",
      "---------------------------\n",
      "Episode: 2391/5000, num_steps: 349, total_reward: 253.2\n",
      "Episode: 2392/5000, num_steps: 1000, total_reward: 58.0\n",
      "Episode: 2393/5000, num_steps: 397, total_reward: 140.9\n",
      "Episode: 2394/5000, num_steps: 290, total_reward: -16.84\n",
      "Episode: 2395/5000, num_steps: 1000, total_reward: 125.8\n",
      "Episode: 2396/5000, num_steps: 419, total_reward: 265.1\n",
      "Episode: 2397/5000, num_steps: 1000, total_reward: 100.3\n",
      "Episode: 2398/5000, num_steps: 983, total_reward: 166.7\n",
      "Episode: 2399/5000, num_steps: 679, total_reward: 185.1\n",
      "Episode: 2400/5000, num_steps: 1000, total_reward: 191.1\n",
      "---------------------------\n",
      "Last 10 episode avg = 148.232\n",
      "---------------------------\n",
      "Episode: 2401/5000, num_steps: 566, total_reward: 216.7\n",
      "Episode: 2402/5000, num_steps: 427, total_reward: 237.6\n",
      "Episode: 2403/5000, num_steps: 231, total_reward: -30.14\n",
      "Episode: 2404/5000, num_steps: 453, total_reward: 216.0\n",
      "Episode: 2405/5000, num_steps: 643, total_reward: 219.4\n",
      "Episode: 2406/5000, num_steps: 863, total_reward: 165.3\n",
      "Episode: 2407/5000, num_steps: 266, total_reward: -17.54\n",
      "Episode: 2408/5000, num_steps: 853, total_reward: 211.0\n",
      "Episode: 2409/5000, num_steps: 576, total_reward: 210.6\n",
      "Episode: 2410/5000, num_steps: 248, total_reward: -40.54\n",
      "---------------------------\n",
      "Last 10 episode avg = 162.025\n",
      "---------------------------\n",
      "Episode: 2411/5000, num_steps: 1000, total_reward: 150.5\n",
      "Episode: 2412/5000, num_steps: 257, total_reward: -30.38\n",
      "Episode: 2413/5000, num_steps: 320, total_reward: 209.0\n",
      "Episode: 2414/5000, num_steps: 450, total_reward: 214.6\n",
      "Episode: 2415/5000, num_steps: 396, total_reward: 217.2\n",
      "Episode: 2416/5000, num_steps: 160, total_reward: -74.29\n",
      "Episode: 2417/5000, num_steps: 81, total_reward: -108.4\n",
      "Episode: 2418/5000, num_steps: 80, total_reward: -103.5\n",
      "Episode: 2419/5000, num_steps: 220, total_reward: -32.9\n",
      "Episode: 2420/5000, num_steps: 329, total_reward: 238.5\n",
      "---------------------------\n",
      "Last 10 episode avg = 40.1195\n",
      "---------------------------\n",
      "Episode: 2421/5000, num_steps: 483, total_reward: 279.0\n",
      "Episode: 2422/5000, num_steps: 204, total_reward: 16.33\n",
      "Episode: 2423/5000, num_steps: 297, total_reward: 251.7\n",
      "Episode: 2424/5000, num_steps: 153, total_reward: 13.26\n",
      "Episode: 2425/5000, num_steps: 217, total_reward: -32.97\n",
      "Episode: 2426/5000, num_steps: 297, total_reward: -60.16\n",
      "Episode: 2427/5000, num_steps: 398, total_reward: 169.5\n",
      "Episode: 2428/5000, num_steps: 451, total_reward: 215.9\n",
      "Episode: 2429/5000, num_steps: 307, total_reward: 215.5\n",
      "Episode: 2430/5000, num_steps: 414, total_reward: 203.4\n",
      "---------------------------\n",
      "Last 10 episode avg = 130.658\n",
      "---------------------------\n",
      "Episode: 2431/5000, num_steps: 472, total_reward: 147.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2432/5000, num_steps: 148, total_reward: -3.262\n",
      "Episode: 2433/5000, num_steps: 413, total_reward: 155.9\n",
      "Episode: 2434/5000, num_steps: 426, total_reward: 193.3\n",
      "Episode: 2435/5000, num_steps: 716, total_reward: 170.5\n",
      "Episode: 2436/5000, num_steps: 565, total_reward: 211.6\n",
      "Episode: 2437/5000, num_steps: 738, total_reward: 214.1\n",
      "Episode: 2438/5000, num_steps: 587, total_reward: 207.4\n",
      "Episode: 2439/5000, num_steps: 555, total_reward: 202.8\n",
      "Episode: 2440/5000, num_steps: 664, total_reward: 240.0\n",
      "---------------------------\n",
      "Last 10 episode avg = 170.323\n",
      "---------------------------\n",
      "Episode: 2441/5000, num_steps: 131, total_reward: -91.67\n",
      "Episode: 2442/5000, num_steps: 362, total_reward: 212.7\n",
      "Episode: 2443/5000, num_steps: 669, total_reward: 154.7\n",
      "Episode: 2444/5000, num_steps: 141, total_reward: 1.238\n",
      "Episode: 2445/5000, num_steps: 411, total_reward: 193.3\n",
      "Episode: 2446/5000, num_steps: 525, total_reward: 233.3\n",
      "Episode: 2447/5000, num_steps: 343, total_reward: 246.6\n",
      "Episode: 2448/5000, num_steps: 778, total_reward: 221.4\n",
      "Episode: 2449/5000, num_steps: 427, total_reward: 207.1\n",
      "Episode: 2450/5000, num_steps: 1000, total_reward: 128.2\n",
      "---------------------------\n",
      "Last 10 episode avg = 161.858\n",
      "---------------------------\n",
      "Episode: 2451/5000, num_steps: 346, total_reward: 194.6\n",
      "Episode: 2452/5000, num_steps: 233, total_reward: -40.48\n",
      "Episode: 2453/5000, num_steps: 309, total_reward: 238.4\n",
      "Episode: 2454/5000, num_steps: 377, total_reward: 289.7\n",
      "Episode: 2455/5000, num_steps: 202, total_reward: 2.687\n",
      "Episode: 2456/5000, num_steps: 741, total_reward: 168.8\n",
      "Episode: 2457/5000, num_steps: 377, total_reward: 242.6\n",
      "Episode: 2458/5000, num_steps: 1000, total_reward: 143.1\n",
      "Episode: 2459/5000, num_steps: 393, total_reward: 214.6\n",
      "Episode: 2460/5000, num_steps: 540, total_reward: 241.4\n",
      "---------------------------\n",
      "Last 10 episode avg = 158.222\n",
      "---------------------------\n",
      "Episode: 2461/5000, num_steps: 554, total_reward: 261.2\n",
      "Episode: 2462/5000, num_steps: 383, total_reward: 242.9\n",
      "Episode: 2463/5000, num_steps: 493, total_reward: 240.3\n",
      "Episode: 2464/5000, num_steps: 406, total_reward: 210.6\n",
      "Episode: 2465/5000, num_steps: 305, total_reward: 261.6\n",
      "Episode: 2466/5000, num_steps: 948, total_reward: 158.7\n",
      "Episode: 2467/5000, num_steps: 413, total_reward: 191.2\n",
      "Episode: 2468/5000, num_steps: 302, total_reward: 221.9\n",
      "Episode: 2469/5000, num_steps: 1000, total_reward: 135.3\n",
      "Episode: 2470/5000, num_steps: 427, total_reward: 189.5\n",
      "---------------------------\n",
      "Last 10 episode avg = 216.505\n",
      "---------------------------\n",
      "Episode: 2471/5000, num_steps: 383, total_reward: -58.89\n",
      "Episode: 2472/5000, num_steps: 331, total_reward: 209.1\n",
      "Episode: 2473/5000, num_steps: 1000, total_reward: 14.46\n",
      "Episode: 2474/5000, num_steps: 492, total_reward: 214.7\n",
      "Episode: 2475/5000, num_steps: 457, total_reward: 191.2\n",
      "Episode: 2476/5000, num_steps: 583, total_reward: 222.9\n",
      "Episode: 2477/5000, num_steps: 918, total_reward: 142.3\n",
      "Episode: 2478/5000, num_steps: 659, total_reward: 170.8\n",
      "Episode: 2479/5000, num_steps: 200, total_reward: -17.6\n",
      "Episode: 2480/5000, num_steps: 61, total_reward: -88.24\n",
      "---------------------------\n",
      "Last 10 episode avg = 127.839\n",
      "---------------------------\n",
      "Episode: 2481/5000, num_steps: 1000, total_reward: 144.4\n",
      "Episode: 2482/5000, num_steps: 1000, total_reward: 114.2\n",
      "Episode: 2483/5000, num_steps: 485, total_reward: 206.7\n",
      "Episode: 2484/5000, num_steps: 363, total_reward: -11.28\n",
      "Episode: 2485/5000, num_steps: 363, total_reward: 223.7\n",
      "Episode: 2486/5000, num_steps: 439, total_reward: 301.6\n",
      "Episode: 2487/5000, num_steps: 336, total_reward: 202.1\n",
      "Episode: 2488/5000, num_steps: 68, total_reward: -143.0\n",
      "Episode: 2489/5000, num_steps: 64, total_reward: -147.4\n",
      "Episode: 2490/5000, num_steps: 54, total_reward: -89.98\n",
      "---------------------------\n",
      "Last 10 episode avg = 80.2664\n",
      "---------------------------\n",
      "Episode: 2491/5000, num_steps: 439, total_reward: 254.2\n",
      "Episode: 2492/5000, num_steps: 314, total_reward: 248.8\n",
      "Episode: 2493/5000, num_steps: 327, total_reward: 242.4\n",
      "Episode: 2494/5000, num_steps: 351, total_reward: 232.4\n",
      "Episode: 2495/5000, num_steps: 1000, total_reward: 78.07\n",
      "Episode: 2496/5000, num_steps: 1000, total_reward: 115.4\n",
      "Episode: 2497/5000, num_steps: 413, total_reward: 225.6\n",
      "Episode: 2498/5000, num_steps: 236, total_reward: -26.85\n",
      "Episode: 2499/5000, num_steps: 379, total_reward: 242.6\n",
      "Episode: 2500/5000, num_steps: 312, total_reward: 248.9\n",
      "---------------------------\n",
      "Last 10 episode avg = 152.265\n",
      "---------------------------\n",
      "Episode: 2501/5000, num_steps: 360, total_reward: 248.5\n",
      "Episode: 2502/5000, num_steps: 393, total_reward: 236.2\n",
      "Episode: 2503/5000, num_steps: 1000, total_reward: 99.93\n",
      "Episode: 2504/5000, num_steps: 1000, total_reward: 35.41\n",
      "Episode: 2505/5000, num_steps: 456, total_reward: 199.0\n",
      "Episode: 2506/5000, num_steps: 767, total_reward: 185.3\n",
      "Episode: 2507/5000, num_steps: 434, total_reward: 231.0\n",
      "Episode: 2508/5000, num_steps: 396, total_reward: -28.99\n",
      "Episode: 2509/5000, num_steps: 355, total_reward: 207.8\n",
      "Episode: 2510/5000, num_steps: 545, total_reward: 231.9\n",
      "---------------------------\n",
      "Last 10 episode avg = 166.314\n",
      "---------------------------\n",
      "Episode: 2511/5000, num_steps: 292, total_reward: 248.0\n",
      "Episode: 2512/5000, num_steps: 409, total_reward: 255.7\n",
      "Episode: 2513/5000, num_steps: 438, total_reward: 208.4\n",
      "Episode: 2514/5000, num_steps: 381, total_reward: 268.8\n",
      "Episode: 2515/5000, num_steps: 342, total_reward: 240.5\n",
      "Episode: 2516/5000, num_steps: 395, total_reward: 248.7\n",
      "Episode: 2517/5000, num_steps: 57, total_reward: -82.07\n"
     ]
    }
   ],
   "source": [
    "for episode in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    current_step = 0\n",
    "    episode_done = False\n",
    "    while current_step < max_steps_per_episode and not episode_done:\n",
    "        action = agent.choose_action(state)\n",
    "        new_state, reward, episode_done, info = env.step(action)\n",
    "        total_reward[episode] = total_reward[episode] + reward\n",
    "        experience = np.empty([0])\n",
    "        experience = np.append(experience, state)\n",
    "        experience = np.append(experience, [action])\n",
    "        experience = np.append(experience, [reward])\n",
    "        experience = np.append(experience, new_state)\n",
    "        agent.remember(experience)\n",
    "\n",
    "        current_step = current_step + 1\n",
    "        state = new_state\n",
    "        agent.replay()\n",
    "\n",
    "    print(\"Episode: {}/{}, num_steps: {}, total_reward: {:.4}\"\n",
    "          .format(episode, max_episodes, current_step, total_reward[episode]))\n",
    "    \n",
    "    if not episode % 10 and episode and agent.gamma < agent.max_gamma:\n",
    "        agent.increase_gamma()\n",
    "        agent.save('lunar_lander_dqn_backup.h5')\n",
    "    \n",
    "    if not episode % 10 and episode:\n",
    "        print(\"Last 10 episode avg = {:.6}\".format(np.average(total_reward[episode-10:episode])))\n",
    "\n",
    "    if episode and np.average(total_reward[episode-100:episode]) > 150:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраним получившуюся модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('lunar_lander_dqn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим получившуюся модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load('lunar_lander_dqn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 275 timesteps\n",
      " with last reward: 100\n",
      "Episode finished after 464 timesteps\n",
      " with last reward: 100\n",
      "Episode finished after 419 timesteps\n",
      " with last reward: 100\n",
      "Episode finished after 420 timesteps\n",
      " with last reward: 100\n",
      "Episode finished after 304 timesteps\n",
      " with last reward: 100\n",
      "Episode finished after 384 timesteps\n",
      " with last reward: 100\n",
      "Episode finished after 341 timesteps\n",
      " with last reward: 100\n",
      "Episode finished after 307 timesteps\n",
      " with last reward: 100\n",
      "Episode finished after 364 timesteps\n",
      " with last reward: 100\n",
      "Episode finished after 265 timesteps\n",
      " with last reward: 100\n",
      "\n",
      "SUCCESSFULL landing in 10/10 cases\n"
     ]
    }
   ],
   "source": [
    "NUM_TESTS = 10\n",
    "NUM_SUCCESS = 0\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "        \n",
    "        action = agent.choose_action(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            if (reward == 100):\n",
    "                NUM_SUCCESS += 1\n",
    "            print(\"Episode finished after {} timesteps\\n with last reward: {}\".format(t+1, reward))\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "print(\"\\nSUCCESSFULL landing in {}/{} cases\".format(NUM_SUCCESS, NUM_TESTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
